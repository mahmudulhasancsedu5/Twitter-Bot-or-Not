{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef9d3de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6696\n",
      "0\n",
      "Input count: 1339, 1339\n",
      "1339\n",
      "1\n",
      "Input count: 1339, 1339\n",
      "1339\n",
      "2\n",
      "Input count: 1339, 1339\n",
      "1339\n",
      "3\n",
      "Input count: 1339, 1339\n",
      "1339\n",
      "4\n",
      "Input count: 1051, 1051\n",
      "1051\n",
      "[INFO] creating model...\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - acc: 0.8473\n",
      "[INFO] test accuracy: 0.8473\n"
     ]
    }
   ],
   "source": [
    "# required libraries\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.functions import col, udf, to_timestamp, lit\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import when, rand\n",
    "from pyspark.ml.feature import Normalizer, StandardScaler, MinMaxScaler, VectorAssembler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Dense, Input, concatenate, Dropout, Activation\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.losses import MeanSquaredError, CategoricalCrossentropy\n",
    "\n",
    "#dataset path\n",
    "dataset_folder_s3 = 'data/' # 's3://bot-dataset/data/'\n",
    "result_path_s3 = '' # 's3://bot-dataset/result/'\n",
    "\n",
    "## convert into feature vector for ml model\n",
    "feature_columns = ['age', 'has_location', 'is_verified', 'total_tweets', 'total_following', \n",
    "                   'total_followers', 'total_likes', 'has_avatar', 'has_background', \n",
    "                   'is_protected', 'profile_modified']\n",
    "\n",
    "# initialize the number of epochs to train for, batch size, and\n",
    "# initial learning rate\n",
    "EPOCHS = 25\n",
    "BS = 64\n",
    "INIT_LR = 1e-3\n",
    "\n",
    "mse = MeanSquaredError()\n",
    "cce = CategoricalCrossentropy()\n",
    "opt = Adam(learning_rate=INIT_LR)\n",
    "\n",
    "# read dataset from csv\n",
    "def read_dataset(spark):\n",
    "    requiredColumns = ['screen_name', 'created_at', 'updated', 'location', 'verified', 'statuses_count', 'friends_count','followers_count', 'favourites_count', 'default_profile_image', 'profile_use_background_image', 'protected', 'default_profile']\n",
    "\n",
    "    bot_accounts1 = spark.read.csv(dataset_folder_s3 + 'social_spambots_1.csv', header = True, inferSchema = True).select(requiredColumns)\n",
    "    bot_accounts2 = spark.read.csv(dataset_folder_s3 + 'social_spambots_2.csv', header = True, inferSchema = True).select(requiredColumns)\n",
    "    bot_accounts3 = spark.read.csv(dataset_folder_s3 + 'social_spambots_3.csv', header = True, inferSchema = True).select(requiredColumns)\n",
    "\n",
    "    # combine multiple bot_account dataset\n",
    "    bot_accounts = bot_accounts1.union(bot_accounts2.union(bot_accounts3))\n",
    "    clean_accounts = spark.read.csv(dataset_folder_s3 + 'geniune_accounts.csv', header = True, inferSchema = True).select(requiredColumns)\n",
    "    \n",
    "    return bot_accounts, clean_accounts\n",
    "\n",
    "# clean dataset\n",
    "def cleanData(df):\n",
    "    df = df.withColumn('age', lit(0)) # need to calculate from 'updated' -'created_at'\n",
    "    df = df.withColumn('has_location', when((df['location'] != None), 1).otherwise(0))\n",
    "    df = df.withColumn('has_avatar', when((df['default_profile_image'] != None), 1).otherwise(0))\n",
    "    df = df.withColumn('has_background', when((df['profile_use_background_image'] != None), 1).otherwise(0))\n",
    "    df = df.withColumn('is_verified', when((df['verified'] != None), 1).otherwise(0))\n",
    "    df = df.withColumn('is_protected', when((df['protected'] != None), 1).otherwise(0))\n",
    "    df = df.withColumn('profile_modified', when((df['default_profile'] != None), 1).otherwise(0))\n",
    "    df = df.withColumnRenamed(\"screen_name\", \"username\")\n",
    "    df = df.withColumnRenamed(\"statuses_count\", \"total_tweets\")\n",
    "    df = df.withColumnRenamed(\"friends_count\", \"total_following\")\n",
    "    df = df.withColumnRenamed(\"followers_count\", \"total_followers\")\n",
    "    df = df.withColumnRenamed(\"favourites_count\", \"total_likes\")\n",
    "    \n",
    "    return df.select('username', 'age', 'has_location', 'is_verified', 'total_tweets', 'total_following', 'total_followers', 'total_likes', 'has_avatar', 'has_background', 'is_protected', 'profile_modified')\n",
    "\n",
    "\n",
    "\n",
    "def doDataScaling(df, input_column, output_column):\n",
    "    ## Make data standard\n",
    "    # https://spark.apache.org/docs/1.4.1/ml-features.html#standardscaler\n",
    "\n",
    "    scaler = StandardScaler(inputCol=input_column, outputCol=output_column,\n",
    "                            withStd=True, withMean=False)\n",
    "\n",
    "    # Compute summary statistics by fitting the StandardScaler\n",
    "    scalerModel = scaler.fit(df)\n",
    "\n",
    "    # Normalize each feature to have unit standard deviation.\n",
    "    scaled_df = scalerModel.transform(df)\n",
    "    \n",
    "    return scaled_df\n",
    "\n",
    "\n",
    "def datasetSplit(X):\n",
    "    # split data for training ana testing\n",
    "    train_df, test_df = X.randomSplit([0.80, 0.20])\n",
    "\n",
    "    # features --> 'BotOrNot'\n",
    "    X_train = train_df.drop('BotOrNot')\n",
    "    y_train = train_df.select('BotOrNot')\n",
    "    X_test = test_df.drop('BotOrNot')\n",
    "    y_test = test_df.select('BotOrNot')\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "\n",
    "## create model\n",
    "def getDLModel():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(500, input_dim=11))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(200))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# convert DataFrame column into nparray\n",
    "# nparray required for model training, validation\n",
    "\n",
    "def to_nparray_list(df, column_name):\n",
    "    rows = df.select(column_name).collect()\n",
    "    lists = [x[column_name] for x in rows]\n",
    "    nparr = np.array(lists)\n",
    "    \n",
    "    return nparr\n",
    "\n",
    "\n",
    "def splitDataset(n_split, X, Y):\n",
    "    for train_index,test_index in KFold(n_split).split(X):\n",
    "\n",
    "        x_train, x_test=X[train_index],X[test_index]\n",
    "        y_train, y_test=Y[train_index],Y[test_index]\n",
    "        print( \"train: {},{} test: {},{}\".format(len(x_train), len(y_train), len(x_test), len(y_test)))\n",
    "        \n",
    "\n",
    "def distributedTrainingGradients(df, feature_column, target_column, n_splits):\n",
    "    print(df.count())\n",
    "    each_len = df.count() // n_splits\n",
    "    gradients = []\n",
    "    ##split dataset into 'n_splits' part\n",
    "    copy_df = df\n",
    "    for i in range(n_splits):\n",
    "        print(i)\n",
    "        temp_df = copy_df.limit(each_len)\n",
    "        copy_df = copy_df.subtract(temp_df)\n",
    "        \n",
    "        X = temp_df.select(feature_column)\n",
    "        Y = temp_df.select(target_column)\n",
    "        X_np = to_nparray_list(X, feature_column)\n",
    "        Y_np = to_nparray_list(Y, target_column)\n",
    "        \n",
    "        grad = step(X_np, Y_np)\n",
    "        gradients.append(grad)\n",
    "        print(temp_df.count())\n",
    "        \n",
    "    return gradients\n",
    "\n",
    "    \n",
    "\n",
    "def step(X, y):\n",
    "    print(\"Input count: {}, {}\".format(len(X), len(y)))\n",
    "    #keep track of gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        curr_model = getDLModel()\n",
    "        #make a prediction using model\n",
    "        predict = curr_model(X)\n",
    "        #calculate loss\n",
    "        loss = mse(y, predict)\n",
    "    #calculate the gradient\n",
    "    gradient = tape.gradient(loss, curr_model.trainable_variables)\n",
    "    \n",
    "    # return the gradient to train final model\n",
    "    return gradient\n",
    "\n",
    "def stepEPOCH(X, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        curr_model = getDLModel()\n",
    "        for i in range(EPOCHS):\n",
    "            #make a prediction using model\n",
    "            predict = curr_model(X)\n",
    "            #calculate loss\n",
    "            loss = cce(y, predict)\n",
    "            print(\"{}: {}\".format(i, loss))\n",
    "            opt.apply_gradients(zip(grad, curr_model.trainable_variables))\n",
    "            \n",
    "    gradient = tape.gradient(loss, curr_model.trainable_variables)\n",
    "    \n",
    "    return gradient\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "def saveModel(model):\n",
    "    model.save(result_path_s3 + 'my_model.h5')\n",
    "\n",
    "    \n",
    "\n",
    "def app_distributed():\n",
    "    # init spark\n",
    "    spark = SparkSession.builder.appName('ml_account_ base_session').getOrCreate()\n",
    "    \n",
    "    bot_accounts, clean_accounts = read_dataset(spark)\n",
    "    \n",
    "    bot_accounts = cleanData(bot_accounts)\n",
    "    clean_accounts = cleanData(clean_accounts)\n",
    "    \n",
    "    ## add BotOrNot column\n",
    "    bot_accounts = bot_accounts.withColumn('BotOrNot', lit(1))\n",
    "    clean_accounts = clean_accounts.withColumn('BotOrNot', lit(0))\n",
    "    \n",
    "    #combine clean and bot accounts data togather\n",
    "    combined_df = bot_accounts.union(clean_accounts)\n",
    "\n",
    "    # shuffle dataset\n",
    "    new_df = combined_df.orderBy(rand())\n",
    "\n",
    "    #remove 'userrname' columns from dataset\n",
    "    new_df = new_df.drop('username')\n",
    "    \n",
    "    feature_assembler = VectorAssembler(inputCols = feature_columns, outputCol = 'independent_features')\n",
    "    df_updated = feature_assembler.transform(new_df)\n",
    "\n",
    "    # keep only required features/columns\n",
    "    df_updated = df_updated.select('independent_features', 'BotOrNot')\n",
    "    \n",
    "    scaled_df = doDataScaling(df_updated, \"independent_features\", \"scaled_features\")\n",
    "    \n",
    "    \n",
    "    # keep only necessary feature/column for ml model\n",
    "    XY = scaled_df.select('scaled_features', 'BotOrNot')\n",
    "    \n",
    "    train_df, test_df = XY.randomSplit([0.80, 0.20])\n",
    "    \n",
    "    gradients = distributedTrainingGradients(train_df, 'scaled_features', 'BotOrNot', 5)\n",
    "    \n",
    "    print(\"[INFO] creating model...\")\n",
    "    model = getDLModel()\n",
    "    \n",
    "    for grad in gradients:\n",
    "        opt.apply_gradients(zip(grad, model.trainable_variables))\n",
    "        \n",
    "        \n",
    "    # in order to calculate accuracy using Keras' functions we first need\n",
    "    # to compile the model\n",
    "    model.compile(optimizer= opt, loss=cce, metrics=[\"acc\"])\n",
    "    \n",
    "    \n",
    "    # DataFrame(column) --> nparray\n",
    "    X_test = to_nparray_list(test_df, 'scaled_features')\n",
    "    y_test = to_nparray_list(test_df, 'BotOrNot')\n",
    "    \n",
    "    # now that the model is compiled we can compute the accuracy\n",
    "    (loss, acc) = model.evaluate(X_test, y_test)\n",
    "    print(\"[INFO] test accuracy: {:.4f}\".format(acc))\n",
    "    \n",
    "    saveModel(model)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    app_distributed()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb493575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cc2682",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
