{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0969f5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.23.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3\n",
      "[notice] To update, run: C:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sklearn) (1.1.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sklearn) (1.9.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sklearn) (1.23.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3\n",
      "[notice] To update, run: C:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.9.0)\n",
      "Requirement already satisfied: numpy<1.25.0,>=1.18.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scipy) (1.23.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3\n",
      "[notice] To update, run: C:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.9.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imbalanced-learn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imbalanced-learn) (1.23.2)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imbalanced-learn) (1.1.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imbalanced-learn) (1.9.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imbalanced-learn) (1.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3\n",
      "[notice] To update, run: C:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install sklearn\n",
    "!pip install scipy\n",
    "!pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01997a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\USER\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\Lib\\\\site-packages\\\\pyspark'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#libraries / dependencies\n",
    "import glob\n",
    "from preprocessor import api as tweet_preprocessor\n",
    "from storagehelper.HDFSHelper import HDFSHelper\n",
    "from storagehelper.LocalFSHelper import LocalFSHelper\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "from pyspark.sql.functions import col, udf, to_timestamp, lit, to_timestamp, when, rand\n",
    "from pyspark.sql.types import IntegerType, LongType, DoubleType, StringType, ArrayType\n",
    "from pyspark.ml.feature import Normalizer, StandardScaler, MinMaxScaler, VectorAssembler, Normalizer\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.accumulators import AccumulatorParam\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Dense, Input, concatenate, Dropout, Activation, BatchNormalization\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.losses import MeanSquaredError, CategoricalCrossentropy\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import sys\n",
    "import shutil # sutil.rmtree(dir_path)\n",
    "\n",
    "\n",
    "#thread depencency\n",
    "from pyspark import InheritableThread\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from threading import Semaphore\n",
    "import time\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf7635c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize the number of epochs to train for, batch size, and\n",
    "# initial learning rate\n",
    "EPOCHS = 10\n",
    "BS = 64\n",
    "INIT_LR = 1e-3\n",
    "\n",
    "mse = MeanSquaredError()\n",
    "cce = CategoricalCrossentropy()\n",
    "opt = SGD(learning_rate=INIT_LR) #Adam(learning_rate=INIT_LR)\n",
    "\n",
    "\n",
    "LOCAL_USERNAME = 'ubuntu'\n",
    "HADOOP_USERNAME = 'ubuntu'\n",
    "\n",
    "#user info\n",
    "SYSTEM_NAME = {'local':'home','server':'user'}\n",
    "USER_NAME = {'local':'ubuntu','server':'ubuntu'} # {'local':'hadoop','server':'hadoop'}\n",
    "WEIGHT_FOLDER_NAME = {'local':'weights_worker_id','server':'weights'}\n",
    "GRADIENT_FOLDER_NAME = {'local':'gradients_worker_id','server':'gradients'}\n",
    "FILE_NAME_PREF = {'gradients': 'gradient','weights': 'weight'}\n",
    "\n",
    "fs_semaphore = Semaphore()\n",
    "\n",
    "\n",
    "#Dataset location\n",
    "\n",
    "#Local\n",
    "bot_tweets_dataset_path = 'F://TwitterBotDataset//tweet_dataset_full//bot_tweets//' \n",
    "genuine_tweets_dataset_path = 'F://TwitterBotDataset//tweet_dataset_full//genuine_tweets//'\n",
    "\n",
    "#S3\n",
    "#bot_tweets_dataset_path = 's3://tweet-dataset/bot_tweets' #'F://TwitterBotDataset//tweet_dataset_small//bot_tweets//'\n",
    "#genuine_tweets_dataset_path = 's3://tweet-dataset/genuine_tweets' #'F://TwitterBotDataset//tweet_dataset_small//genuine_tweets//'\n",
    "\n",
    "#turn a line of text into d dimentional vector \n",
    "GLOVE_DIR = \"C://Users//USER//projects//\"\n",
    "\n",
    "#all columns\n",
    "BOT_COLUMNS = ['id','text','source','user_id','truncated','in_reply_to_status_id', \n",
    "               'in_reply_to_user_id','in_reply_to_screen_name', 'retweeted_status_id',\n",
    "               'geo','place','contributors','retweet_count', 'reply_count','favorite_count',\n",
    "               'favorited', 'retweeted','possibly_sensitive','num_hashtags','num_urls',\n",
    "               'num_mentions','created_at','timestamp','crawled_at', 'updated']\n",
    "\n",
    "GENUINE_COLUMNS = ['id','text','source','user_id','truncated','in_reply_to_status_id', \n",
    "                   'in_reply_to_user_id','in_reply_to_screen_name', 'retweeted_status_id',\n",
    "                   'geo','place','contributors','retweet_count', 'reply_count','favorite_count',\n",
    "                   'favorited', 'retweeted','possibly_sensitive','num_hashtags','num_urls',\n",
    "                   'num_mentions','REMOVE_IT', 'created_at','timestamp','crawled_at', 'updated',]\n",
    "\n",
    "#feature used for bot detection\n",
    "COLUMN_NAMES = ['text', 'retweet_count', 'reply_count', 'favorite_count',\n",
    "                'num_hashtags', 'num_urls', 'num_mentions']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c1b34f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #configure spark\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[4]\").setAppName(\"ml_account_ base_session\")\n",
    "conf.set(\"spark.executor.instances\", 2)\n",
    "conf.set(\"spark.executor.cores\", 1)\n",
    "conf.set(\"spark.executor.memory\", 871859200)\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# # # for spark-submit\n",
    "# spark = SparkSession.builder.appName('ml_account_ base_session').getOrCreate()\n",
    "# # spark\n",
    "# sc = spark.sparkContext\n",
    "\n",
    "# for local build\n",
    "# spark = SparkSession.builder.appName('ml_account_ base_session').getOrCreate()\n",
    "\n",
    "\n",
    "#for local multi thread\n",
    "\n",
    "# conf = SparkConf()\n",
    "# conf.setMaster(\"local[4]\").setAppName(\"distributed_training_session\")\n",
    "# conf.set(\"spark.scheduler.mode\", \"FAIR\")\n",
    "# sc = SparkContext(conf=conf)\n",
    "# spark = SparkSession.builder.getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9e57e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-SRLISO7:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ml_account_ base_session</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1a11cebbd00>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70e670f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# read dataset from csv\n",
    "def read_dataset():\n",
    "    bot_tweets = spark.read.csv(bot_tweets_dataset_path, header = True, inferSchema = True).limit(100)\n",
    "    genuine_tweets = spark.read.csv(genuine_tweets_dataset_path, header = True, inferSchema = True).limit(100)\n",
    "#     bot_tweets = bot_tweets.persist(StorageLevel.MEMORY_ONLY)\n",
    "#     genuine_tweets = genuine_tweets.persist(StorageLevel.MEMORY_ONLY)\n",
    "    return bot_tweets, genuine_tweets\n",
    "    \n",
    "def read_data_from_csv_files(path, rows):\n",
    "    list_of_csv_files = glob.glob(path + '/*.csv')\n",
    "    df = pd.concat([pd.read_csv(fname, encoding_errors='ignore', on_bad_lines='skip', nrows=rows) for fname in list_of_csv_files]).head(rows)\n",
    "    return df\n",
    "def read_dataset_pandas():\n",
    "    rows = 100\n",
    "    bot_tweets = read_data_from_csv_files(bot_tweets_dataset_path, rows)\n",
    "    genuine_tweets = read_data_from_csv_files(genuine_tweets_dataset_path, rows)\n",
    "#     bot_tweets = bot_tweets.persist(StorageLevel.MEMORY_ONLY)\n",
    "#     genuine_tweets = genuine_tweets.persist(StorageLevel.MEMORY_ONLY)\n",
    "    return bot_tweets, genuine_tweets\n",
    "\n",
    "def set_column_name(df, column_names):\n",
    "    df = df.toDF(*column_names)\n",
    "    return df\n",
    "\n",
    "def remove_column_miss_match(df):\n",
    "    ## dataset have diffrent number of columns\n",
    "    ## column name of dataframe\n",
    "    column_name = [cname for cname, tp in df.dtypes]\n",
    "#     len(df.collect()), len(df.dtypes)\n",
    "    #column_name\n",
    "\n",
    "    #Number of column is diffrent for bot and genuine tweets data\n",
    "\n",
    "    #genuine_tweets_df = genuine_tweets_df.toDF(*column_name)\n",
    "    df = set_column_name(df, GENUINE_COLUMNS)\n",
    "#     print(len(df.collect()))\n",
    "    \n",
    "    df = df.drop('REMOVE_IT') # remove 5th column from end\n",
    "    #update column name according to \n",
    "    df = set_column_name(df, BOT_COLUMNS)\n",
    "#     print(len(df.collect()))\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_type_miss_match(df):\n",
    "    # Same column has diffrent data type. So make data type same for every column\n",
    "    updated_df = df.withColumn(\"id\",col(\"id\").cast(IntegerType())) \\\n",
    "                                    .withColumn(\"favorite_count\",col(\"favorite_count\").cast(LongType())) \\\n",
    "                                    .withColumn(\"favorited\",col(\"favorited\").cast(IntegerType()))\n",
    "    return updated_df\n",
    "\n",
    "\n",
    "def resize_combine_data(bot_tweets_df, genuine_tweets_df):\n",
    "    ## only keep the required column from the dataframe\n",
    "    bot_tweets_df = bot_tweets_df.select(*COLUMN_NAMES)\n",
    "    genuine_tweets_df = genuine_tweets_df.select(*COLUMN_NAMES)\n",
    "    \n",
    "#     print(len(bot_tweets_df.collect()), len(genuine_tweets_df.collect()))\n",
    "\n",
    "    ## add BotOrNot column\n",
    "    bot_tweets_df = bot_tweets_df.withColumn('BotOrNot', lit(1))\n",
    "    genuine_tweets_df = genuine_tweets_df.withColumn('BotOrNot', lit(0))\n",
    "\n",
    "    #combine clean and bot accounts data togather\n",
    "    tweets_df = bot_tweets_df.union(genuine_tweets_df)\n",
    "\n",
    "    # shuffle dataset\n",
    "    tweets_df = tweets_df.orderBy(rand())\n",
    "\n",
    "#     print(len(tweets_df.collect()))\n",
    "    \n",
    "    return tweets_df\n",
    "\n",
    "def removeExtraColumn(df, column_names):\n",
    "    if len(df.columns) == 26:\n",
    "        df = remove_column_miss_match(df)\n",
    "    else:\n",
    "        df = set_column_name(df, column_names)\n",
    "    \n",
    "    return df\n",
    "\n",
    "text_process_udf = udf(lambda x : tweet_preprocessor.tokenize(x), StringType())\n",
    "def preprocess_data(df):\n",
    "    df = df.withColumn('text', text_process_udf(df.text))\n",
    "    print(df.count())\n",
    "    df = df.withColumn(\"retweet_count\",col(\"retweet_count\").cast(DoubleType()))\n",
    "    print(df.count())\n",
    "    df = df.withColumn(\"reply_count\",col(\"reply_count\").cast(DoubleType()))\n",
    "    print(df.count())\n",
    "    df = df.withColumn(\"favorite_count\",col(\"favorite_count\").cast(DoubleType()))\n",
    "    print(df.count())\n",
    "    df = df.withColumn(\"num_hashtags\",col(\"num_hashtags\").cast(DoubleType()))\n",
    "    print(df.count())\n",
    "    df = df.withColumn(\"num_urls\",col(\"num_urls\").cast(DoubleType()))\n",
    "    print(df.count())\n",
    "    df = df.withColumn(\"num_mentions\",col(\"num_mentions\").cast(DoubleType()))\n",
    "    print(df.count())\n",
    "    return df\n",
    "\n",
    "def doDataScaling(df, input_column, output_column):\n",
    "    ## Make data standard\n",
    "    # https://spark.apache.org/docs/1.4.1/ml-features.html#standardscaler\n",
    "\n",
    "    scaler = StandardScaler(inputCol=input_column, outputCol=output_column,\n",
    "                            withStd=True, withMean=False)\n",
    "\n",
    "    # Compute summary statistics by fitting the StandardScaler\n",
    "    scalerModel = scaler.fit(df)\n",
    "\n",
    "    # Normalize each feature to have unit standard deviation.\n",
    "    scaled_df = scalerModel.transform(df)\n",
    "    \n",
    "    return scaled_df\n",
    "\n",
    "def doDataNormalization(df, input_column, output_column):\n",
    "    normalizer = Normalizer(p=2.0, inputCol=input_column, outputCol=output_column)\n",
    "    norm_df = normalizer.transform(df)\n",
    "    return norm_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce3ac92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def makeGloveWordEmbedder(glove_path):\n",
    "    embedding_dict = {}\n",
    "    with open(glove_path, 'r', encoding=\"utf-8\") as glove_file:\n",
    "        for line in glove_file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embedding_dict[word] = vector\n",
    "            \n",
    "    return embedding_dict  \n",
    "\n",
    "\n",
    "# # Test GLoVE result\n",
    "# glove_word2vec_embedder[\"google\"]\n",
    "\n",
    "#Ned to large data chech for performance\n",
    "glove_dict = makeGloveWordEmbedder(GLOVE_DIR + \"glove.twitter.27B.25d.txt\")\n",
    "\n",
    " ##create word embedding GLoVE model dictionary. Use pre trained model\n",
    "text_feature_dimention = 25\n",
    "# glove_word2vec_embedder = makeGloveWordEmbedder(GLOVE_DIR + \"glove.twitter.27B.25d.txt\")\n",
    "\n",
    "\n",
    "#Give a word and get that word representing feature vector of dimention 25 from embedding dictionary\n",
    "def word2vec(word_dict=None, word=None, dim=25):\n",
    "    default_vector = np.zeros(dim)\n",
    "    \n",
    "    if word_dict is None or word is None:\n",
    "        return default_vector\n",
    "    \n",
    "    word_vector = word_dict.get(word)\n",
    "    \n",
    "    if word_vector is None:\n",
    "        return default_vector\n",
    "    return word_vector\n",
    "\n",
    "# # test a word representing feature vector\n",
    "# word_vector = word2vec(glove_word2vec_embedder, \"tweet\", text_feature_dimention)\n",
    "# print(type(word_vector), word_vector)\n",
    "\n",
    "\n",
    "#----------------LSTM Model----------------------\n",
    "\n",
    "#create a 1 layer LSTM model\n",
    "#input dimention: 3d (1,7,25) input_sample = [[[1,...,25]],..,[1,...,25]]\n",
    "#output dimention: 1d (1,32) output_sample [[1,2,3,,,,32]]\n",
    "def lstm_model(output_dim):\n",
    "    model = LSTM(output_dim, return_sequences=False, return_state=False)\n",
    "    return model\n",
    "\n",
    "def reset_lstm_model(model):\n",
    "    model.resate_states() # stateful=True is required for reset states\n",
    "\n",
    "# create LSTM model of output dimention 32. Model output feature vector will be of 32 dimention vector\n",
    "lstm = lstm_model(32) \n",
    "\n",
    "# convver a sentence to a feature vector using LSTM(RNN) model\n",
    "def sent2vec(sent):\n",
    "    words = sent.split(' ')\n",
    "    word_vectors = np.array([])\n",
    "    count = 0;\n",
    "    for word in words:\n",
    "        word_vector = word2vec(glove_dict, word)\n",
    "#         print(\"word dim: {}\".format(len(word_vector)))\n",
    "        if word_vectors.size == 0:\n",
    "            word_vectors = np.array([word_vector])\n",
    "        else:\n",
    "            word_vectors = np.vstack([word_vectors, word_vector])\n",
    "        count = count + 1\n",
    "    \n",
    "#     print(\"Input feature vector shape before reshape(2D): {}\".format(word_vectors.shape))\n",
    "        \n",
    "    input_feature_vectors = np.reshape(word_vectors, (1, count, text_feature_dimention))\n",
    "#     print(\"Input feature vector shape after reshape(3d): {}\".format(input_feature_vectors.shape))\n",
    "#     print(\"LSTM requirs 3d shape inputs [batch, timesteps, feature]\")\n",
    "    output_vector = lstm(input_feature_vectors)\n",
    "#     lstm.reset_states() # stateful = True is required for reset\n",
    "\n",
    "#     print(\"result vector shape: {}\".format(output_vector.shape))\n",
    "#     print(\"Last input was: {}\".format(input_feature_vectors[0][-1]))\n",
    "#     print(\"output result: {}\".format(output_vector))\n",
    "    \n",
    "    # (tensore --> numpy 0bject --> numpy.array --> array/list/ArrayType)\n",
    "    return output_vector.numpy()[0].tolist() \n",
    "    \n",
    "## For Testing sentence to vector convertion\n",
    "# sent = \"Twitter is a large social media network\"\n",
    "# res_vector = sent2vec(sent)\n",
    "# type(res_vector), res_vector\n",
    "\n",
    "\n",
    "# text string --> vector 32 dimention\n",
    "sent_to_vector_udf = udf(lambda x : sent2vec(x), ArrayType(DoubleType()))\n",
    "def processTextColumn(df, column_name, new_column_name):\n",
    "    df = df.withColumn(new_column_name, sent_to_vector_udf(col(column_name)))\n",
    "    return df\n",
    "\n",
    "def sentEmbeddingGLoVE_LSTM(df):\n",
    "    \n",
    "    text_updated_column = 'text_features'\n",
    "    updated_df = processTextColumn(df, \"text\", text_updated_column)\n",
    "\n",
    "#     print(len(updated_df.collect()), type(updated_df), updated_df.printSchema()) \n",
    "    \n",
    "    return updated_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8228a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assembleColumns(tweets_df):\n",
    "    columns = ['retweet_count', 'reply_count', 'favorite_count',\n",
    "               'num_hashtags' ,'num_urls', 'num_mentions', 'BotOrNot']\n",
    "\n",
    "    tweets_df = tweets_df.select(*columns, \n",
    "                          tweets_df.text_features[0], tweets_df.text_features[1], tweets_df.text_features[2],tweets_df.text_features[3], tweets_df.text_features[4],\n",
    "                          tweets_df.text_features[5], tweets_df.text_features[6], tweets_df.text_features[7],tweets_df.text_features[8], tweets_df.text_features[9], \n",
    "                          tweets_df.text_features[10], tweets_df.text_features[11], tweets_df.text_features[12],tweets_df.text_features[13], tweets_df.text_features[14],\n",
    "                          tweets_df.text_features[15], tweets_df.text_features[16], tweets_df.text_features[17],tweets_df.text_features[18], tweets_df.text_features[19],\n",
    "                          tweets_df.text_features[20], tweets_df.text_features[21], tweets_df.text_features[22],tweets_df.text_features[23], tweets_df.text_features[24],\n",
    "                          tweets_df.text_features[25], tweets_df.text_features[26], tweets_df.text_features[27],tweets_df.text_features[28], tweets_df.text_features[29],\n",
    "                          tweets_df.text_features[30], tweets_df.text_features[31])\n",
    "\n",
    "\n",
    "#     print(tweets_df.columns, len(tweets_df.collect()), tweets_df.printSchema())\n",
    "\n",
    "    #remove \n",
    "\n",
    "    feature_columns = ['retweet_count','reply_count','favorite_count','num_hashtags','num_urls','num_mentions',\n",
    "                       'text_features[0]','text_features[1]', 'text_features[2]','text_features[3]','text_features[4]',\n",
    "                       'text_features[5]','text_features[6]','text_features[7]', 'text_features[8]','text_features[9]',\n",
    "                       'text_features[10]','text_features[11]','text_features[12]','text_features[13]','text_features[14]',\n",
    "                       'text_features[15]','text_features[16]','text_features[17]','text_features[18]','text_features[19]',\n",
    "                       'text_features[20]','text_features[21]','text_features[22]', 'text_features[23]', 'text_features[24]',\n",
    "                       'text_features[25]','text_features[26]','text_features[27]', 'text_features[28]', 'text_features[29]',\n",
    "                       'text_features[30]','text_features[31]']\n",
    "\n",
    "\n",
    "    tweets_df = tweets_df.na.fill(value=0.0 ,subset= feature_columns)\n",
    "    feature_assembler = VectorAssembler(inputCols = feature_columns, outputCol = 'independent_features')\n",
    "\n",
    "    tweets_updated_df = feature_assembler.transform(tweets_df)\n",
    "\n",
    "    #check\n",
    "#     num = len(tweets_updated_df.collect())\n",
    "#     print(num, type(tweets_updated_df), tweets_updated_df.printSchema())\n",
    "\n",
    "    #remove unnecessary columns\n",
    "    tweets_updated_df = tweets_updated_df.drop(*feature_columns)\n",
    "    \n",
    "    return tweets_updated_df\n",
    "\n",
    "\n",
    "def to_nparray_list(df, column_name):\n",
    "    rows = df.select(column_name).collect()\n",
    "    lists = [x[column_name] for x in rows]\n",
    "    nparr = np.array(lists)\n",
    "    \n",
    "    return nparr\n",
    "\n",
    "def to_nparray_dataset(df, feature_column, target_column):\n",
    "#     list(df.select('col_name').toPandas()['col_name']) \n",
    "#     feature = list(df.select(feature_column).toPandas()[feature_column])\n",
    "#     target = list(df.select(target_column).toPandas()[target_column])\n",
    "    feature = [row[0] for row in list(df.select(feature_column).toLocalIterator())]\n",
    "    target = [row[0] for row in list(df.select(target_column).toLocalIterator())]\n",
    "        \n",
    "    return np.array(feature), np.array(target)    \n",
    "\n",
    "def partition_dataset(df):\n",
    "    train_df, test_df = df.randomSplit([0.80, 0.20])\n",
    "#     print(len(train_df.collect()), len(test_df.collect()))\n",
    "\n",
    "    # features --> 'BotOrNot'\n",
    "#     X_train = train_df.drop('BotOrNot')\n",
    "#     y_train = train_df.select('BotOrNot')\n",
    "#     X_test = test_df.drop('BotOrNot')\n",
    "#     y_test = test_df.select('BotOrNot')\n",
    "    \n",
    "\n",
    "    #checkpoint\n",
    "#     print(len(X_train.collect()), len(y_train.collect()))\n",
    "#     print(len(X_test.collect()), len(y_test.collect()))\n",
    "\n",
    "#     X_train = to_nparray_list(X_train, 'independent_features')\n",
    "#     y_train = to_nparray_list(y_train, 'BotOrNot')\n",
    "#     X_test = to_nparray_list(X_test, 'independent_features')\n",
    "#     y_test = to_nparray_list(y_test, 'BotOrNot')\n",
    "    train_df = train_df.cache()\n",
    "    test_df = test_df.cache()\n",
    "\n",
    "    train_X,  train_Y = to_nparray_dataset(train_df, 'independent_features', 'BotOrNot')\n",
    "    test_X, test_Y = to_nparray_dataset(test_df, 'independent_features', 'BotOrNot')\n",
    "\n",
    "    \n",
    "    return train_X, train_Y, test_X, test_Y # return type: numpy.array\n",
    "\n",
    "\n",
    "def getTrainTestData(df, seed = 21):\n",
    "    train_X, test_X = df.randomSplit([0.7, 0.3], seed)\n",
    "    return train_X, test_X\n",
    "    \n",
    "def SMOTEENN(X, Y):\n",
    "    s = SMOTE()\n",
    "    s_x, s_y = s.fit_resample(X, Y)\n",
    "\n",
    "    enn = EditedNearestNeighbours()\n",
    "    e_x, e_y = enn.fit_resample(s_x, s_y)\n",
    "\n",
    "    return e_x, e_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b327b00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create model\n",
    "def get2DenseLayeredModel(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(500, input_dim=input_dim))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(200))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def model_evaluation(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train,\n",
    "              batch_size=64,\n",
    "              epochs=10,\n",
    "              validation_data=(X_test, y_test))\n",
    "    score, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "21348228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Application\n",
      "dataset load successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print('Run Application')\n",
    "bot_tweets_df, genuine_tweets_df = read_dataset_pandas()\n",
    "print('dataset load successfully')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "277b4394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bot_tweets_df = bot_tweets_df.convert_dtypes(infer_objects=False)\n",
    "# genuine_tweets_df = genuine_tweets_df.convert_dtypes(infer_objects=False)\n",
    "\n",
    "\n",
    "#renaming colums\n",
    "genuine_tweets_df = genuine_tweets_df.set_axis(GENUINE_COLUMNS, axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8758ac8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(id                           int64\n",
       " text                        object\n",
       " source                      object\n",
       " user_id                      int64\n",
       " truncated                  float64\n",
       " in_reply_to_status_id        int64\n",
       " in_reply_to_user_id          int64\n",
       " in_reply_to_screen_name     object\n",
       " retweeted_status_id          int64\n",
       " geo                        float64\n",
       " place                      float64\n",
       " contributors               float64\n",
       " retweet_count                int64\n",
       " reply_count                  int64\n",
       " favorite_count               int64\n",
       " favorited                  float64\n",
       " retweeted                  float64\n",
       " possibly_sensitive         float64\n",
       " num_hashtags                 int64\n",
       " num_urls                     int64\n",
       " num_mentions                 int64\n",
       " created_at                  object\n",
       " timestamp                   object\n",
       " crawled_at                  object\n",
       " updated                     object\n",
       " dtype: object,\n",
       " id                           int64\n",
       " text                        object\n",
       " source                      object\n",
       " user_id                      int64\n",
       " truncated                  float64\n",
       " in_reply_to_status_id        int64\n",
       " in_reply_to_user_id          int64\n",
       " in_reply_to_screen_name     object\n",
       " retweeted_status_id          int64\n",
       " geo                         object\n",
       " place                       object\n",
       " contributors               float64\n",
       " retweet_count              float64\n",
       " reply_count                  int64\n",
       " favorite_count               int64\n",
       " favorited                    int64\n",
       " retweeted                  float64\n",
       " possibly_sensitive         float64\n",
       " num_hashtags                object\n",
       " num_urls                     int64\n",
       " num_mentions                 int64\n",
       " REMOVE_IT                    int64\n",
       " created_at                  object\n",
       " timestamp                   object\n",
       " crawled_at                  object\n",
       " updated                     object\n",
       " dtype: object)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot_tweets_df.dtypes, genuine_tweets_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "59963c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>user_id</th>\n",
       "      <th>truncated</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <th>retweeted_status_id</th>\n",
       "      <th>geo</th>\n",
       "      <th>...</th>\n",
       "      <th>favorited</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>num_hashtags</th>\n",
       "      <th>num_urls</th>\n",
       "      <th>num_mentions</th>\n",
       "      <th>created_at</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>crawled_at</th>\n",
       "      <th>updated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>532627591686275072</td>\n",
       "      <td>I Pooh - In silenzio 1968 http://t.co/ahvQxUqTws</td>\n",
       "      <td>&lt;a href=\"http://www.facebook.com/twitter\" rel=...</td>\n",
       "      <td>24858289</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Wed Nov 12 20:14:48 +0000 2014</td>\n",
       "      <td>2014-11-12 21:14:48</td>\n",
       "      <td>2014-11-12 21:44:09</td>\n",
       "      <td>2014-11-12 21:44:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>532624255058706432</td>\n",
       "      <td>http://t.co/HyI5EQKz6Q</td>\n",
       "      <td>&lt;a href=\"http://www.facebook.com/twitter\" rel=...</td>\n",
       "      <td>24858289</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Wed Nov 12 20:01:32 +0000 2014</td>\n",
       "      <td>2014-11-12 21:01:32</td>\n",
       "      <td>2014-11-12 21:44:09</td>\n",
       "      <td>2014-11-12 21:44:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>532513524460052480</td>\n",
       "      <td>Tutti a tavola, con il filetto di baccalà. htt...</td>\n",
       "      <td>&lt;a href=\"http://www.facebook.com/twitter\" rel=...</td>\n",
       "      <td>24858289</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Wed Nov 12 12:41:32 +0000 2014</td>\n",
       "      <td>2014-11-12 13:41:32</td>\n",
       "      <td>2014-11-12 21:44:09</td>\n",
       "      <td>2014-11-12 21:44:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                                               text  \\\n",
       "0  532627591686275072   I Pooh - In silenzio 1968 http://t.co/ahvQxUqTws   \n",
       "1  532624255058706432                             http://t.co/HyI5EQKz6Q   \n",
       "2  532513524460052480  Tutti a tavola, con il filetto di baccalà. htt...   \n",
       "\n",
       "                                              source   user_id  truncated  \\\n",
       "0  <a href=\"http://www.facebook.com/twitter\" rel=...  24858289        NaN   \n",
       "1  <a href=\"http://www.facebook.com/twitter\" rel=...  24858289        NaN   \n",
       "2  <a href=\"http://www.facebook.com/twitter\" rel=...  24858289        NaN   \n",
       "\n",
       "   in_reply_to_status_id  in_reply_to_user_id in_reply_to_screen_name  \\\n",
       "0                      0                    0                     NaN   \n",
       "1                      0                    0                     NaN   \n",
       "2                      0                    0                     NaN   \n",
       "\n",
       "   retweeted_status_id  geo  ...  favorited  retweeted  possibly_sensitive  \\\n",
       "0                    0  NaN  ...        NaN        NaN                 NaN   \n",
       "1                    0  NaN  ...        NaN        NaN                 NaN   \n",
       "2                    0  NaN  ...        NaN        NaN                 NaN   \n",
       "\n",
       "   num_hashtags  num_urls  num_mentions                      created_at  \\\n",
       "0             0         1             0  Wed Nov 12 20:14:48 +0000 2014   \n",
       "1             0         1             0  Wed Nov 12 20:01:32 +0000 2014   \n",
       "2             0         1             0  Wed Nov 12 12:41:32 +0000 2014   \n",
       "\n",
       "             timestamp           crawled_at              updated  \n",
       "0  2014-11-12 21:14:48  2014-11-12 21:44:09  2014-11-12 21:44:09  \n",
       "1  2014-11-12 21:01:32  2014-11-12 21:44:09  2014-11-12 21:44:09  \n",
       "2  2014-11-12 13:41:32  2014-11-12 21:44:09  2014-11-12 21:44:09  \n",
       "\n",
       "[3 rows x 25 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot_tweets_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "94a0aed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>user_id</th>\n",
       "      <th>truncated</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <th>retweeted_status_id</th>\n",
       "      <th>geo</th>\n",
       "      <th>...</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>num_hashtags</th>\n",
       "      <th>num_urls</th>\n",
       "      <th>num_mentions</th>\n",
       "      <th>REMOVE_IT</th>\n",
       "      <th>created_at</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>crawled_at</th>\n",
       "      <th>updated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>593895316719423488</td>\n",
       "      <td>This age/face recognition thing..no reason pla...</td>\n",
       "      <td>&lt;a href=\\http://twitter.com\\\" rel=\\\"nofollow\\\"...</td>\n",
       "      <td>678033</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>\\N</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\N</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Thu Apr 30 21:50:52 +0000 2015</td>\n",
       "      <td>2015-04-30 23:50:52</td>\n",
       "      <td>2015-05-01 12:57:19</td>\n",
       "      <td>2015-05-01 12:57:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>593880638069018624</td>\n",
       "      <td>Only upside of the moment I can think of is th...</td>\n",
       "      <td>&lt;a href=\\http://twitter.com\\\" rel=\\\"nofollow\\\"...</td>\n",
       "      <td>678033</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>\\N</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\N</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Thu Apr 30 20:52:32 +0000 2015</td>\n",
       "      <td>2015-04-30 22:52:32</td>\n",
       "      <td>2015-05-01 12:57:19</td>\n",
       "      <td>2015-05-01 12:57:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>593847955536252928</td>\n",
       "      <td>If you're going to think about+create experien...</td>\n",
       "      <td>&lt;a href=\\http://tapbots.com/tweetbot\\\" rel=\\\"n...</td>\n",
       "      <td>678033</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>\\N</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\N</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Thu Apr 30 18:42:40 +0000 2015</td>\n",
       "      <td>2015-04-30 20:42:40</td>\n",
       "      <td>2015-05-01 12:57:19</td>\n",
       "      <td>2015-05-01 12:57:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                                               text  \\\n",
       "0  593895316719423488  This age/face recognition thing..no reason pla...   \n",
       "1  593880638069018624  Only upside of the moment I can think of is th...   \n",
       "2  593847955536252928  If you're going to think about+create experien...   \n",
       "\n",
       "                                              source  user_id  truncated  \\\n",
       "0  <a href=\\http://twitter.com\\\" rel=\\\"nofollow\\\"...   678033        NaN   \n",
       "1  <a href=\\http://twitter.com\\\" rel=\\\"nofollow\\\"...   678033        NaN   \n",
       "2  <a href=\\http://tapbots.com/tweetbot\\\" rel=\\\"n...   678033        NaN   \n",
       "\n",
       "   in_reply_to_status_id  in_reply_to_user_id in_reply_to_screen_name  \\\n",
       "0                      0                    0                     NaN   \n",
       "1                      0                    0                     NaN   \n",
       "2                      0                    0                     NaN   \n",
       "\n",
       "   retweeted_status_id geo  ... retweeted  possibly_sensitive  num_hashtags  \\\n",
       "0                    0  \\N  ...       NaN                 NaN            \\N   \n",
       "1                    0  \\N  ...       NaN                 NaN            \\N   \n",
       "2                    0  \\N  ...       NaN                 NaN            \\N   \n",
       "\n",
       "   num_urls  num_mentions  REMOVE_IT                      created_at  \\\n",
       "0         0             0          0  Thu Apr 30 21:50:52 +0000 2015   \n",
       "1         2             0          0  Thu Apr 30 20:52:32 +0000 2015   \n",
       "2         2             0          0  Thu Apr 30 18:42:40 +0000 2015   \n",
       "\n",
       "             timestamp           crawled_at              updated  \n",
       "0  2015-04-30 23:50:52  2015-05-01 12:57:19  2015-05-01 12:57:19  \n",
       "1  2015-04-30 22:52:32  2015-05-01 12:57:19  2015-05-01 12:57:19  \n",
       "2  2015-04-30 20:42:40  2015-05-01 12:57:19  2015-05-01 12:57:19  \n",
       "\n",
       "[3 rows x 26 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " genuine_tweets_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "223c06fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "field num_hashtags: Can not merge type <class 'pyspark.sql.types.DoubleType'> and <class 'pyspark.sql.types.StringType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [67]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m train_bot_tweet_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(train_bot_tweet_df, verifySchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m test_bot_tweet_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(test_bot_tweet_df, verifySchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 13\u001b[0m train_genuine_tweets_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_genuine_tweets_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m test_genuine_tweets_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(test_genuine_tweets_df, verifySchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\session.py:891\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    888\u001b[0m     has_pandas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m    890\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m--> 891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSparkSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(\n\u001b[0;32m    895\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    896\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:437\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    435\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    436\u001b[0m converted_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_from_pandas(data, schema, timezone)\n\u001b[1;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\session.py:936\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    934\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[0;32m    935\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 936\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    937\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    938\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\session.py:631\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[1;34m(self, data, schema)\u001b[0m\n\u001b[0;32m    628\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m--> 631\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchemaFromList\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[0;32m    633\u001b[0m     tupled_data: Iterable[Tuple] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(converter, data)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\session.py:517\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[1;34m(self, data, names)\u001b[0m\n\u001b[0;32m    515\u001b[0m infer_dict_as_struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf\u001b[38;5;241m.\u001b[39minferDictAsStruct()\n\u001b[0;32m    516\u001b[0m prefer_timestamp_ntz \u001b[38;5;241m=\u001b[39m is_timestamp_ntz_preferred()\n\u001b[1;32m--> 517\u001b[0m schema \u001b[38;5;241m=\u001b[39m \u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_merge_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome of types cannot be determined after inferring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\types.py:1383\u001b[0m, in \u001b[0;36m_merge_type\u001b[1;34m(a, b, name)\u001b[0m\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, StructType):\n\u001b[0;32m   1382\u001b[0m     nfs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((f\u001b[38;5;241m.\u001b[39mname, f\u001b[38;5;241m.\u001b[39mdataType) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m cast(StructType, b)\u001b[38;5;241m.\u001b[39mfields)\n\u001b[1;32m-> 1383\u001b[0m     fields \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1384\u001b[0m         StructField(\n\u001b[0;32m   1385\u001b[0m             f\u001b[38;5;241m.\u001b[39mname, _merge_type(f\u001b[38;5;241m.\u001b[39mdataType, nfs\u001b[38;5;241m.\u001b[39mget(f\u001b[38;5;241m.\u001b[39mname, NullType()), name\u001b[38;5;241m=\u001b[39mnew_name(f\u001b[38;5;241m.\u001b[39mname))\n\u001b[0;32m   1386\u001b[0m         )\n\u001b[0;32m   1387\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m a\u001b[38;5;241m.\u001b[39mfields\n\u001b[0;32m   1388\u001b[0m     ]\n\u001b[0;32m   1389\u001b[0m     names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m([f\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields])\n\u001b[0;32m   1390\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nfs:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\types.py:1385\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, StructType):\n\u001b[0;32m   1382\u001b[0m     nfs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((f\u001b[38;5;241m.\u001b[39mname, f\u001b[38;5;241m.\u001b[39mdataType) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m cast(StructType, b)\u001b[38;5;241m.\u001b[39mfields)\n\u001b[0;32m   1383\u001b[0m     fields \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1384\u001b[0m         StructField(\n\u001b[1;32m-> 1385\u001b[0m             f\u001b[38;5;241m.\u001b[39mname, \u001b[43m_merge_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNullType\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1386\u001b[0m         )\n\u001b[0;32m   1387\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m a\u001b[38;5;241m.\u001b[39mfields\n\u001b[0;32m   1388\u001b[0m     ]\n\u001b[0;32m   1389\u001b[0m     names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m([f\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields])\n\u001b[0;32m   1390\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nfs:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\types.py:1378\u001b[0m, in \u001b[0;36m_merge_type\u001b[1;34m(a, b, name)\u001b[0m\n\u001b[0;32m   1375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m b\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(a) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(b):\n\u001b[0;32m   1377\u001b[0m     \u001b[38;5;66;03m# TODO: type cast (such as int -> long)\u001b[39;00m\n\u001b[1;32m-> 1378\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(new_msg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not merge type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mtype\u001b[39m(a), \u001b[38;5;28mtype\u001b[39m(b))))\n\u001b[0;32m   1380\u001b[0m \u001b[38;5;66;03m# same type\u001b[39;00m\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, StructType):\n",
      "\u001b[1;31mTypeError\u001b[0m: field num_hashtags: Can not merge type <class 'pyspark.sql.types.DoubleType'> and <class 'pyspark.sql.types.StringType'>"
     ]
    }
   ],
   "source": [
    "train_bot_tweet_df = bot_tweets_df.sample(frac=0.8,random_state=200) #random state is a seed value\n",
    "test_bot_tweet_df = bot_tweets_df.drop(train_bot_tweet_df.index)\n",
    "\n",
    "train_genuine_tweets_df = genuine_tweets_df.sample(frac=0.8,random_state=200) #random state is a seed value\n",
    "test_genuine_tweets_df = genuine_tweets_df.drop(train_genuine_tweets_df.index)\n",
    "\n",
    "\n",
    "train_bot_tweet_df.head(3), test_bot_tweet_df.head(3), train_genuine_tweets_df.head(3), test_genuine_tweets_df.head(3)\n",
    "\n",
    "train_bot_tweet_df = spark.createDataFrame(train_bot_tweet_df, verifySchema=False)\n",
    "test_bot_tweet_df = spark.createDataFrame(test_bot_tweet_df, verifySchema=False)\n",
    "\n",
    "train_genuine_tweets_df = spark.createDataFrame(train_genuine_tweets_df, verifySchema=False)\n",
    "test_genuine_tweets_df = spark.createDataFrame(test_genuine_tweets_df, verifySchema=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1a59d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: #bot_tweets: id                         80\n",
      "text                       80\n",
      "source                     80\n",
      "user_id                    80\n",
      "truncated                   0\n",
      "in_reply_to_status_id      80\n",
      "in_reply_to_user_id        80\n",
      "in_reply_to_screen_name     0\n",
      "retweeted_status_id        80\n",
      "geo                         0\n",
      "place                       0\n",
      "contributors                0\n",
      "retweet_count              80\n",
      "reply_count                80\n",
      "favorite_count             80\n",
      "favorited                   0\n",
      "retweeted                   0\n",
      "possibly_sensitive          0\n",
      "num_hashtags               80\n",
      "num_urls                   80\n",
      "num_mentions               80\n",
      "created_at                 80\n",
      "timestamp                  80\n",
      "crawled_at                 80\n",
      "updated                    80\n",
      "dtype: int64 #gen_tweets: 593932392663912449                                                                                                                   80\n",
      "RT @morningJewshow: Speaking about Jews and comedy tonight at Temple Emanu-El in San Francisco. In other words, my High Holidays.    80\n",
      "<a href=\\http://tapbots.com/tweetbot\\\" rel=\\\"nofollow\\\">Tweetbot for iΟS</a>\"                                                        80\n",
      "678033                                                                                                                               80\n",
      "Unnamed: 4                                                                                                                            0\n",
      "0                                                                                                                                    80\n",
      "0.1                                                                                                                                  80\n",
      "Unnamed: 7                                                                                                                           42\n",
      "593932168524533760                                                                                                                   80\n",
      "\\N                                                                                                                                   80\n",
      "\\N.1                                                                                                                                 80\n",
      "Unnamed: 11                                                                                                                           0\n",
      "Unnamed: 12                                                                                                                           0\n",
      "1                                                                                                                                    80\n",
      "0.2                                                                                                                                  80\n",
      "0.3                                                                                                                                  80\n",
      "Unnamed: 16                                                                                                                           0\n",
      "Unnamed: 17                                                                                                                           0\n",
      "\\N.2                                                                                                                                 55\n",
      "0.4                                                                                                                                  80\n",
      "0.5                                                                                                                                  80\n",
      "1.1                                                                                                                                  80\n",
      "Fri May 01 00:18:11 +0000 2015                                                                                                       80\n",
      "2015-05-01 02:18:11                                                                                                                  80\n",
      "2015-05-01 12:57:19                                                                                                                  80\n",
      "2015-05-01 12:57:19.1                                                                                                                80\n",
      "dtype: int64\n",
      "Test: #bot_tweets: id                         20\n",
      "text                       20\n",
      "source                     20\n",
      "user_id                    20\n",
      "truncated                   0\n",
      "in_reply_to_status_id      20\n",
      "in_reply_to_user_id        20\n",
      "in_reply_to_screen_name     0\n",
      "retweeted_status_id        20\n",
      "geo                         0\n",
      "place                       0\n",
      "contributors                0\n",
      "retweet_count              20\n",
      "reply_count                20\n",
      "favorite_count             20\n",
      "favorited                   0\n",
      "retweeted                   0\n",
      "possibly_sensitive          0\n",
      "num_hashtags               20\n",
      "num_urls                   20\n",
      "num_mentions               20\n",
      "created_at                 20\n",
      "timestamp                  20\n",
      "crawled_at                 20\n",
      "updated                    20\n",
      "dtype: int64 #gen_tweets: 593932392663912449                                                                                                                   20\n",
      "RT @morningJewshow: Speaking about Jews and comedy tonight at Temple Emanu-El in San Francisco. In other words, my High Holidays.    20\n",
      "<a href=\\http://tapbots.com/tweetbot\\\" rel=\\\"nofollow\\\">Tweetbot for iΟS</a>\"                                                        20\n",
      "678033                                                                                                                               20\n",
      "Unnamed: 4                                                                                                                            0\n",
      "0                                                                                                                                    20\n",
      "0.1                                                                                                                                  20\n",
      "Unnamed: 7                                                                                                                            6\n",
      "593932168524533760                                                                                                                   20\n",
      "\\N                                                                                                                                   20\n",
      "\\N.1                                                                                                                                 20\n",
      "Unnamed: 11                                                                                                                           0\n",
      "Unnamed: 12                                                                                                                           0\n",
      "1                                                                                                                                    20\n",
      "0.2                                                                                                                                  20\n",
      "0.3                                                                                                                                  20\n",
      "Unnamed: 16                                                                                                                           0\n",
      "Unnamed: 17                                                                                                                           0\n",
      "\\N.2                                                                                                                                  9\n",
      "0.4                                                                                                                                  20\n",
      "0.5                                                                                                                                  20\n",
      "1.1                                                                                                                                  20\n",
      "Fri May 01 00:18:11 +0000 2015                                                                                                       20\n",
      "2015-05-01 02:18:11                                                                                                                  20\n",
      "2015-05-01 12:57:19                                                                                                                  20\n",
      "2015-05-01 12:57:19.1                                                                                                                20\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# #     #cache df\n",
    "# train_bot_tweet_df = train_bot_tweet_df.cache()\n",
    "# train_genuine_tweets_df = train_genuine_tweets_df.cache()\n",
    "\n",
    "# test_bot_tweet_df = test_bot_tweet_df.cache()\n",
    "# test_genuine_tweets_df = test_genuine_tweets_df.cache()\n",
    "\n",
    "print(\"Train: #bot_tweets: {} #gen_tweets: {}\".format(train_bot_tweet_df.count(), train_genuine_tweets_df.count()))\n",
    "print(\"Test: #bot_tweets: {} #gen_tweets: {}\".format(test_bot_tweet_df.count(), test_genuine_tweets_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ccbaf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: #bot_tweets: 86 #gen_tweets: 86\n",
      "Test: #bot_tweets: 14 #gen_tweets: 14\n",
      "+------------------+--------------------+--------------------+--------+---------+---------------------+-------------------+-----------------------+-------------------+----+-----+------------+-------------+-----------+--------------+---------+---------+------------------+------------+--------+------------+--------------------+-------------------+-------------------+-------------------+\n",
      "|                id|                text|              source| user_id|truncated|in_reply_to_status_id|in_reply_to_user_id|in_reply_to_screen_name|retweeted_status_id| geo|place|contributors|retweet_count|reply_count|favorite_count|favorited|retweeted|possibly_sensitive|num_hashtags|num_urls|num_mentions|          created_at|          timestamp|         crawled_at|            updated|\n",
      "+------------------+--------------------+--------------------+--------+---------+---------------------+-------------------+-----------------------+-------------------+----+-----+------------+-------------+-----------+--------------+---------+---------+------------------+------------+--------+------------+--------------------+-------------------+-------------------+-------------------+\n",
      "|522449222457901056|Ho pubblicato una...|\"<a href=\"\"http:/...|24858289|     null|                    0|                  0|                   null|                  0|null| null|        null|            0|          0|             0|     null|     null|              null|           0|       1|           0|Wed Oct 15 18:09:...|2014-10-15 20:09:35|2014-11-12 21:44:09|2014-11-12 21:44:09|\n",
      "|522449462850244608|Andrea Basile ......|\"<a href=\"\"http:/...|24858289|     null|                    0|                  0|                   null|                  0|null| null|        null|            0|          0|             0|     null|     null|              null|           0|       1|           0|Wed Oct 15 18:10:...|2014-10-15 20:10:33|2014-11-12 21:44:09|2014-11-12 21:44:09|\n",
      "|522455698064818176|http://t.co/rc4qZ...|\"<a href=\"\"http:/...|24858289|     null|                    0|                  0|                   null|                  0|null| null|        null|            0|          0|             0|     null|     null|              null|           0|       1|           0|Wed Oct 15 18:35:...|2014-10-15 20:35:19|2014-11-12 21:44:09|2014-11-12 21:44:09|\n",
      "+------------------+--------------------+--------------------+--------+---------+---------------------+-------------------+-----------------------+-------------------+----+-----+------------+-------------+-----------+--------------+---------+---------+------------------+------------+--------+------------+--------------------+-------------------+-------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------------------+--------------------+--------------------+--------+---------+---------------------+-------------------+-----------------------+-------------------+----+-----+------------+-------------+-----------+--------------+---------+---------+------------------+------------+--------+------------+--------------------+-------------------+-------------------+-------------------+\n",
      "|                id|                text|              source| user_id|truncated|in_reply_to_status_id|in_reply_to_user_id|in_reply_to_screen_name|retweeted_status_id| geo|place|contributors|retweet_count|reply_count|favorite_count|favorited|retweeted|possibly_sensitive|num_hashtags|num_urls|num_mentions|          created_at|          timestamp|         crawled_at|            updated|\n",
      "+------------------+--------------------+--------------------+--------+---------+---------------------+-------------------+-----------------------+-------------------+----+-----+------------+-------------+-----------+--------------+---------+---------+------------------+------------+--------+------------+--------------------+-------------------+-------------------+-------------------+\n",
      "|523549396248829952|http://t.co/PUiBY...|\"<a href=\"\"http:/...|24858289|     null|                    0|                  0|                   null|                  0|null| null|        null|            0|          0|             0|     null|     null|              null|           0|       1|           0|Sat Oct 18 19:01:...|2014-10-18 21:01:17|2014-11-12 21:44:09|2014-11-12 21:44:09|\n",
      "|524624746663903232|“Pugnettista” di ...|\"<a href=\"\"http:/...|24858289|     null|                    0|                  0|                   null|                  0|null| null|        null|            0|          0|             0|     null|     null|              null|           0|       1|           0|Tue Oct 21 18:14:...|2014-10-21 20:14:21|2014-11-12 21:44:09|2014-11-12 21:44:09|\n",
      "|525874356702826496|\"Antonello Vendit...|\"<a href=\"\"http:/...|24858289|     null|                    0|                  0|                   null|                  0|null| null|        null|            0|          0|             0|     null|     null|              null|           0|       1|           0|Sat Oct 25 04:59:...|2014-10-25 06:59:51|2014-11-12 21:44:09|2014-11-12 21:44:09|\n",
      "+------------------+--------------------+--------------------+--------+---------+---------------------+-------------------+-----------------------+-------------------+----+-----+------------+-------------+-----------+--------------+---------+---------+------------------+------------+--------+------------+--------------------+-------------------+-------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----+--------------------+------+-------+---------+---------------------+-------------------+-----------------------+-------------------+---+-----+------------+-------------+-----------+--------------+---------+---------+------------------+------------+--------+------------+-------------------+-------------------+-------------------+-------+\n",
      "|  id|                text|source|user_id|truncated|in_reply_to_status_id|in_reply_to_user_id|in_reply_to_screen_name|retweeted_status_id|geo|place|contributors|retweet_count|reply_count|favorite_count|favorited|retweeted|possibly_sensitive|num_hashtags|num_urls|num_mentions|         created_at|          timestamp|         crawled_at|updated|\n",
      "+----+--------------------+------+-------+---------+---------------------+-------------------+-----------------------+-------------------+---+-----+------------+-------------+-----------+--------------+---------+---------+------------------+------------+--------+------------+-------------------+-------------------+-------------------+-------+\n",
      "|#fb\"|<a href=\"http://t...|678033|   null|        0|                    0|               null|                      0|                 \\N| \\N| null|        null|            0|          0|             0|     null|     null|                \\N|           2|       0|           0|2015-04-30 22:52:32|2015-05-01 12:57:19|2015-05-01 12:57:19|   null|\n",
      "|#fb\"|<a href=\"http://t...|678033|   null|        0|                    0|               null|                      0|                 \\N| \\N| null|        null|            0|          0|             0|     null|     null|                \\N|           2|       0|           0|2015-04-29 08:28:53|2015-05-01 12:57:19|2015-05-01 12:57:19|   null|\n",
      "|#fb\"|<a href=\"http://t...|678033|   null|        0|                    0|               null|                      0|                 \\N| \\N| null|        null|            0|          0|             1|     null|     null|              null|           1|       1|           0|2015-04-29 19:35:01|2015-05-01 12:57:19|2015-05-01 12:57:19|   null|\n",
      "+----+--------------------+------+-------+---------+---------------------+-------------------+-----------------------+-------------------+---+-----+------------+-------------+-----------+--------------+---------+---------+------------------+------------+--------+------------+-------------------+-------------------+-------------------+-------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------------------+--------------------+--------------------+-------+---------+---------------------+-------------------+-----------------------+-------------------+---+-----+------------+-------------+-----------+--------------+---------+---------+------------------+------------+--------+------------+--------------------+-------------------+-------------------+-------------------+\n",
      "|                id|                text|              source|user_id|truncated|in_reply_to_status_id|in_reply_to_user_id|in_reply_to_screen_name|retweeted_status_id|geo|place|contributors|retweet_count|reply_count|favorite_count|favorited|retweeted|possibly_sensitive|num_hashtags|num_urls|num_mentions|          created_at|          timestamp|         crawled_at|            updated|\n",
      "+------------------+--------------------+--------------------+-------+---------+---------------------+-------------------+-----------------------+-------------------+---+-----+------------+-------------+-----------+--------------+---------+---------+------------------+------------+--------+------------+--------------------+-------------------+-------------------+-------------------+\n",
      "|592465961350926336|Twitter doesn't h...|<a href=\"http://t...| 678033|     null|                    0|                  0|                   null|                  0| \\N|   \\N|        null|         null|          0|             0|        0|     null|              null|          \\N|       0|           0|Sun Apr 26 23:11:...|2015-04-27 01:11:07|2015-05-01 12:57:19|2015-05-01 12:57:19|\n",
      "|592773515687940097|@kenyatta @jah Ju...|<a href=\"http://t...| 678033|     null|   592766741052293120|             799645|               kenyatta|                  0| \\N|   \\N|        null|         null|          0|             0|        2|     null|              null|          \\N|       0|           0|Mon Apr 27 19:33:...|2015-04-27 21:33:13|2015-05-01 12:57:19|2015-05-01 12:57:19|\n",
      "|593138502956503040|@EllenPapadakis h...|<a href=\"http://t...| 678033|     null|   593130473355776001|           37565480|         EllenPapadakis|                  0| \\N|   \\N|        null|         null|          0|             0|        0|     null|              null|          \\N|       0|           0|Tue Apr 28 19:43:...|2015-04-28 21:43:33|2015-05-01 12:57:19|2015-05-01 12:57:19|\n",
      "+------------------+--------------------+--------------------+-------+---------+---------------------+-------------------+-----------------------+-------------------+---+-----+------------+-------------+-----------+--------------+---------+---------+------------------+------------+--------+------------+--------------------+-------------------+-------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None, None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#solve column number issue\n",
    "train_bot_tweet_df = removeExtraColumn(train_bot_tweet_df, BOT_COLUMNS)\n",
    "train_genuine_tweets_df = removeExtraColumn(train_genuine_tweets_df, BOT_COLUMNS)\n",
    "\n",
    "test_bot_tweet_df = removeExtraColumn(test_bot_tweet_df, BOT_COLUMNS)\n",
    "test_genuine_tweets_df = removeExtraColumn(test_genuine_tweets_df, BOT_COLUMNS)\n",
    "\n",
    "print(\"Train: #bot_tweets: {} #gen_tweets: {}\".format(train_bot_tweet_df.count(), train_genuine_tweets_df.count()))\n",
    "print(\"Test: #bot_tweets: {} #gen_tweets: {}\".format(test_bot_tweet_df.count(), test_genuine_tweets_df.count()))\n",
    "train_bot_tweet_df.show(3), test_bot_tweet_df.show(3), train_genuine_tweets_df.show(3), test_genuine_tweets_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d332d6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bot_columns: ['id', 'text', 'source', 'user_id', 'truncated', 'in_reply_to_status_id', 'in_reply_to_user_id', 'in_reply_to_screen_name', 'retweeted_status_id', 'geo', 'place', 'contributors', 'retweet_count', 'reply_count', 'favorite_count', 'favorited', 'retweeted', 'possibly_sensitive', 'num_hashtags', 'num_urls', 'num_mentions', 'created_at', 'timestamp', 'crawled_at', 'updated']\n",
      "gen_columns: ['id', 'text', 'source', 'user_id', 'truncated', 'in_reply_to_status_id', 'in_reply_to_user_id', 'in_reply_to_screen_name', 'retweeted_status_id', 'geo', 'place', 'contributors', 'retweet_count', 'reply_count', 'favorite_count', 'favorited', 'retweeted', 'possibly_sensitive', 'num_hashtags', 'num_urls', 'num_mentions', 'created_at', 'timestamp', 'crawled_at', 'updated']\n",
      "+-----------+--------------------+--------------------+--------+---------+---------------------+-------------------+-----------------------+-------------------+----+-----+------------+-------------+-----------+--------------+---------+---------+------------------+------------+--------+------------+--------------------+-------------------+-------------------+-------------------+\n",
      "|         id|                text|              source| user_id|truncated|in_reply_to_status_id|in_reply_to_user_id|in_reply_to_screen_name|retweeted_status_id| geo|place|contributors|retweet_count|reply_count|favorite_count|favorited|retweeted|possibly_sensitive|num_hashtags|num_urls|num_mentions|          created_at|          timestamp|         crawled_at|            updated|\n",
      "+-----------+--------------------+--------------------+--------+---------+---------------------+-------------------+-----------------------+-------------------+----+-----+------------+-------------+-----------+--------------+---------+---------+------------------+------------+--------+------------+--------------------+-------------------+-------------------+-------------------+\n",
      "|-1110949888|Ho pubblicato una...|\"<a href=\"\"http:/...|24858289|     null|                    0|                  0|                   null|                  0|null| null|        null|            0|          0|             0|     null|     null|              null|           0|       1|           0|Wed Oct 15 18:09:...|2014-10-15 20:09:35|2014-11-12 21:44:09|2014-11-12 21:44:09|\n",
      "|-1236774912|Andrea Basile ......|\"<a href=\"\"http:/...|24858289|     null|                    0|                  0|                   null|                  0|null| null|        null|            0|          0|             0|     null|     null|              null|           0|       1|           0|Wed Oct 15 18:10:...|2014-10-15 20:10:33|2014-11-12 21:44:09|2014-11-12 21:44:09|\n",
      "| 1980252160|http://t.co/rc4qZ...|\"<a href=\"\"http:/...|24858289|     null|                    0|                  0|                   null|                  0|null| null|        null|            0|          0|             0|     null|     null|              null|           0|       1|           0|Wed Oct 15 18:35:...|2014-10-15 20:35:19|2014-11-12 21:44:09|2014-11-12 21:44:09|\n",
      "+-----------+--------------------+--------------------+--------+---------+---------------------+-------------------+-----------------------+-------------------+----+-----+------------+-------------+-----------+--------------+---------+---------+------------------+------------+--------+------------+--------------------+-------------------+-------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------+--------------------+--------------------+--------+---------+---------------------+-------------------+-----------------------+-------------------+----+-----+------------+-------------+-----------+--------------+---------+---------+------------------+------------+--------+------------+--------------------+-------------------+-------------------+-------------------+\n",
      "|        id|                text|              source| user_id|truncated|in_reply_to_status_id|in_reply_to_user_id|in_reply_to_screen_name|retweeted_status_id| geo|place|contributors|retweet_count|reply_count|favorite_count|favorited|retweeted|possibly_sensitive|num_hashtags|num_urls|num_mentions|          created_at|          timestamp|         crawled_at|            updated|\n",
      "+----------+--------------------+--------------------+--------+---------+---------------------+-------------------+-----------------------+-------------------+----+-----+------------+-------------+-----------+--------------+---------+---------+------------------+------------+--------+------------+--------------------+-------------------+-------------------+-------------------+\n",
      "|-372760576|http://t.co/PUiBY...|\"<a href=\"\"http:/...|24858289|     null|                    0|                  0|                   null|                  0|null| null|        null|            0|          0|             0|     null|     null|              null|           0|       1|           0|Sat Oct 18 19:01:...|2014-10-18 21:01:17|2014-11-12 21:44:09|2014-11-12 21:44:09|\n",
      "|1900544000|“Pugnettista” di ...|\"<a href=\"\"http:/...|24858289|     null|                    0|                  0|                   null|                  0|null| null|        null|            0|          0|             0|     null|     null|              null|           0|       1|           0|Tue Oct 21 18:14:...|2014-10-21 20:14:21|2014-11-12 21:44:09|2014-11-12 21:44:09|\n",
      "|-205369344|\"Antonello Vendit...|\"<a href=\"\"http:/...|24858289|     null|                    0|                  0|                   null|                  0|null| null|        null|            0|          0|             0|     null|     null|              null|           0|       1|           0|Sat Oct 25 04:59:...|2014-10-25 06:59:51|2014-11-12 21:44:09|2014-11-12 21:44:09|\n",
      "+----------+--------------------+--------------------+--------+---------+---------------------+-------------------+-----------------------+-------------------+----+-----+------------+-------------+-----------+--------------+---------+---------+------------------+------------+--------+------------+--------------------+-------------------+-------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----+--------------------+------+-------+---------+---------------------+-------------------+-----------------------+-------------------+---+-----+------------+-------------+-----------+--------------+---------+---------+------------------+------------+--------+------------+-------------------+-------------------+-------------------+-------+\n",
      "|  id|                text|source|user_id|truncated|in_reply_to_status_id|in_reply_to_user_id|in_reply_to_screen_name|retweeted_status_id|geo|place|contributors|retweet_count|reply_count|favorite_count|favorited|retweeted|possibly_sensitive|num_hashtags|num_urls|num_mentions|         created_at|          timestamp|         crawled_at|updated|\n",
      "+----+--------------------+------+-------+---------+---------------------+-------------------+-----------------------+-------------------+---+-----+------------+-------------+-----------+--------------+---------+---------+------------------+------------+--------+------------+-------------------+-------------------+-------------------+-------+\n",
      "|null|<a href=\"http://t...|678033|   null|        0|                    0|               null|                      0|                 \\N| \\N| null|        null|            0|          0|             0|     null|     null|                \\N|           2|       0|           0|2015-04-30 22:52:32|2015-05-01 12:57:19|2015-05-01 12:57:19|   null|\n",
      "|null|<a href=\"http://t...|678033|   null|        0|                    0|               null|                      0|                 \\N| \\N| null|        null|            0|          0|             0|     null|     null|                \\N|           2|       0|           0|2015-04-29 08:28:53|2015-05-01 12:57:19|2015-05-01 12:57:19|   null|\n",
      "|null|<a href=\"http://t...|678033|   null|        0|                    0|               null|                      0|                 \\N| \\N| null|        null|            0|          0|             1|     null|     null|              null|           1|       1|           0|2015-04-29 19:35:01|2015-05-01 12:57:19|2015-05-01 12:57:19|   null|\n",
      "+----+--------------------+------+-------+---------+---------------------+-------------------+-----------------------+-------------------+---+-----+------------+-------------+-----------+--------------+---------+---------+------------------+------------+--------+------------+-------------------+-------------------+-------------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+-------+---------+---------------------+-------------------+-----------------------+-------------------+---+-----+------------+-------------+-----------+--------------+---------+---------+------------------+------------+--------+------------+--------------------+-------------------+-------------------+-------------------+\n",
      "|  id|                text|              source|user_id|truncated|in_reply_to_status_id|in_reply_to_user_id|in_reply_to_screen_name|retweeted_status_id|geo|place|contributors|retweet_count|reply_count|favorite_count|favorited|retweeted|possibly_sensitive|num_hashtags|num_urls|num_mentions|          created_at|          timestamp|         crawled_at|            updated|\n",
      "+----+--------------------+--------------------+-------+---------+---------------------+-------------------+-----------------------+-------------------+---+-----+------------+-------------+-----------+--------------+---------+---------+------------------+------------+--------+------------+--------------------+-------------------+-------------------+-------------------+\n",
      "|null|Twitter doesn't h...|<a href=\"http://t...| 678033|     null|                    0|                  0|                   null|                  0| \\N|   \\N|        null|         null|          0|             0|        0|     null|              null|          \\N|       0|           0|Sun Apr 26 23:11:...|2015-04-27 01:11:07|2015-05-01 12:57:19|2015-05-01 12:57:19|\n",
      "|null|@kenyatta @jah Ju...|<a href=\"http://t...| 678033|     null|   592766741052293120|             799645|               kenyatta|                  0| \\N|   \\N|        null|         null|          0|             0|        2|     null|              null|          \\N|       0|           0|Mon Apr 27 19:33:...|2015-04-27 21:33:13|2015-05-01 12:57:19|2015-05-01 12:57:19|\n",
      "|null|@EllenPapadakis h...|<a href=\"http://t...| 678033|     null|   593130473355776001|           37565480|         EllenPapadakis|                  0| \\N|   \\N|        null|         null|          0|             0|        0|     null|              null|          \\N|       0|           0|Tue Apr 28 19:43:...|2015-04-28 21:43:33|2015-05-01 12:57:19|2015-05-01 12:57:19|\n",
      "+----+--------------------+--------------------+-------+---------+---------------------+-------------------+-----------------------+-------------------+---+-----+------------+-------------+-----------+--------------+---------+---------+------------------+------------+--------+------------+--------------------+-------------------+-------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None, None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bot_tweet_df = remove_type_miss_match(train_bot_tweet_df)\n",
    "train_genuine_tweets_df = remove_type_miss_match(train_genuine_tweets_df)\n",
    "test_bot_tweet_df = remove_type_miss_match(test_bot_tweet_df)\n",
    "test_genuine_tweets_df = remove_type_miss_match(test_genuine_tweets_df)\n",
    "\n",
    "print(\"bot_columns: {}\".format(train_bot_tweet_df.columns))\n",
    "print(\"gen_columns: {}\".format(train_genuine_tweets_df.columns))\n",
    "train_bot_tweet_df.show(3), test_bot_tweet_df.show(3), train_genuine_tweets_df.show(3), test_genuine_tweets_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66e79238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-----------+--------------+------------+--------+------------+--------+\n",
      "|                text|retweet_count|reply_count|favorite_count|num_hashtags|num_urls|num_mentions|BotOrNot|\n",
      "+--------------------+-------------+-----------+--------------+------------+--------+------------+--------+\n",
      "|http://t.co/kOH7c...|            0|          0|             0|           0|       1|           0|       1|\n",
      "|http://t.co/5k2k8...|            0|          0|             0|           0|       1|           0|       1|\n",
      "|Alphaville - Fore...|            0|          0|             0|           0|       1|           0|       1|\n",
      "+--------------------+-------------+-----------+--------------+------------+--------+------------+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "172\n",
      "172\n",
      "172\n",
      "172\n",
      "172\n",
      "172\n",
      "172\n",
      "+--------------------+-------------+-----------+--------------+------------+--------+------------+--------+\n",
      "|                text|retweet_count|reply_count|favorite_count|num_hashtags|num_urls|num_mentions|BotOrNot|\n",
      "+--------------------+-------------+-----------+--------------+------------+--------+------------+--------+\n",
      "|               <URL>|          0.0|        0.0|           0.0|         0.0|     1.0|         0.0|       1|\n",
      "|               <URL>|          0.0|        0.0|           0.0|         0.0|     1.0|         0.0|       1|\n",
      "|Alphaville - Fore...|          0.0|        0.0|           0.0|         0.0|     1.0|         0.0|       1|\n",
      "+--------------------+-------------+-----------+--------------+------------+--------+------------+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------------------+-------------+-----------+--------------+------------+--------+------------+--------+\n",
      "|                text|retweet_count|reply_count|favorite_count|num_hashtags|num_urls|num_mentions|BotOrNot|\n",
      "+--------------------+-------------+-----------+--------------+------------+--------+------------+--------+\n",
      "|Ho pubblicato una...|            0|          0|             0|           0|       1|           0|       1|\n",
      "|One reason my lif...|         null|       null|          null|        null|    null|        null|       0|\n",
      "|                null|         null|       null|          null|        null|    null|        null|       0|\n",
      "+--------------------+-------------+-----------+--------------+------------+--------+------------+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "+--------------------+-------------+-----------+--------------+------------+--------+------------+--------+\n",
      "|                text|retweet_count|reply_count|favorite_count|num_hashtags|num_urls|num_mentions|BotOrNot|\n",
      "+--------------------+-------------+-----------+--------------+------------+--------+------------+--------+\n",
      "|Ho pubblicato una...|          0.0|        0.0|           0.0|         0.0|     1.0|         0.0|       1|\n",
      "|One reason my lif...|         null|       null|          null|        null|    null|        null|       0|\n",
      "|                None|         null|       null|          null|        null|    null|        null|       0|\n",
      "+--------------------+-------------+-----------+--------------+------------+--------+------------+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Train: # 172 Test: # 28\n",
      "train_columns: ['text', 'retweet_count', 'reply_count', 'favorite_count', 'num_hashtags', 'num_urls', 'num_mentions', 'BotOrNot']\n",
      "test_columns: ['text', 'retweet_count', 'reply_count', 'favorite_count', 'num_hashtags', 'num_urls', 'num_mentions', 'BotOrNot']\n",
      "+--------------------+-------------+-----------+--------------+------------+--------+------------+--------+\n",
      "|                text|retweet_count|reply_count|favorite_count|num_hashtags|num_urls|num_mentions|BotOrNot|\n",
      "+--------------------+-------------+-----------+--------------+------------+--------+------------+--------+\n",
      "|               <URL>|          0.0|        0.0|           0.0|         0.0|     1.0|         0.0|       1|\n",
      "|               <URL>|          0.0|        0.0|           0.0|         0.0|     1.0|         0.0|       1|\n",
      "|Alphaville - Fore...|          0.0|        0.0|           0.0|         0.0|     1.0|         0.0|       1|\n",
      "+--------------------+-------------+-----------+--------------+------------+--------+------------+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------------------+-------------+-----------+--------------+------------+--------+------------+--------+\n",
      "|                text|retweet_count|reply_count|favorite_count|num_hashtags|num_urls|num_mentions|BotOrNot|\n",
      "+--------------------+-------------+-----------+--------------+------------+--------+------------+--------+\n",
      "|Ho pubblicato una...|          0.0|        0.0|           0.0|         0.0|     1.0|         0.0|       1|\n",
      "|One reason my lif...|         null|       null|          null|        null|    null|        null|       0|\n",
      "|                None|         null|       null|          null|        null|    null|        null|       0|\n",
      "+--------------------+-------------+-----------+--------------+------------+--------+------------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##preprocess data\n",
    "train_tweets_df = resize_combine_data(train_bot_tweet_df, train_genuine_tweets_df)\n",
    "\n",
    "train_tweets_df.show(3)\n",
    "\n",
    "train_tweets_df = preprocess_data(train_tweets_df)\n",
    "\n",
    "train_tweets_df.show(3)\n",
    "\n",
    "test_tweets_df = resize_combine_data(test_bot_tweet_df, test_genuine_tweets_df)\n",
    "\n",
    "test_tweets_df.show(3)\n",
    "\n",
    "test_tweets_df = preprocess_data(test_tweets_df)\n",
    "test_tweets_df.show(3)\n",
    "\n",
    "print(\"Train: # {} Test: # {}\".format(train_tweets_df.count(), test_tweets_df.count()))\n",
    "print(\"train_columns: {}\".format(train_tweets_df.columns))\n",
    "print(\"test_columns: {}\".format(test_tweets_df.columns))\n",
    "\n",
    "train_tweets_df.show(3), test_tweets_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "151ff774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: # 172 Test: # 28\n",
      "train_columns: ['text', 'retweet_count', 'reply_count', 'favorite_count', 'num_hashtags', 'num_urls', 'num_mentions', 'BotOrNot', 'text_features']\n",
      "test_columns: ['text', 'retweet_count', 'reply_count', 'favorite_count', 'num_hashtags', 'num_urls', 'num_mentions', 'BotOrNot', 'text_features']\n"
     ]
    }
   ],
   "source": [
    "##text embedding using GLoVE & LSTM\n",
    "## Word Embedding\n",
    "train_tweets_df = sentEmbeddingGLoVE_LSTM(train_tweets_df)\n",
    "test_tweets_df = sentEmbeddingGLoVE_LSTM(test_tweets_df)\n",
    "\n",
    "print(\"Train: # {} Test: # {}\".format(train_tweets_df.count(), test_tweets_df.count()))\n",
    "print(\"train_columns: {}\".format(train_tweets_df.columns))\n",
    "print(\"test_columns: {}\".format(test_tweets_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8facdeb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-----------+--------------+------------+--------+------------+--------+--------------------+\n",
      "|                text|retweet_count|reply_count|favorite_count|num_hashtags|num_urls|num_mentions|BotOrNot|       text_features|\n",
      "+--------------------+-------------+-----------+--------------+------------+--------+------------+--------+--------------------+\n",
      "|               <URL>|          0.0|        0.0|           0.0|         0.0|     1.0|         0.0|       1|[0.0, 0.0, 0.0, 0...|\n",
      "|               <URL>|          0.0|        0.0|           0.0|         0.0|     1.0|         0.0|       1|[0.0, 0.0, 0.0, 0...|\n",
      "|Alphaville - Fore...|          0.0|        0.0|           0.0|         0.0|     1.0|         0.0|       1|[0.04599700123071...|\n",
      "+--------------------+-------------+-----------+--------------+------------+--------+------------+--------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------------------+-------------+-----------+--------------+------------+--------+------------+--------+--------------------+\n",
      "|                text|retweet_count|reply_count|favorite_count|num_hashtags|num_urls|num_mentions|BotOrNot|       text_features|\n",
      "+--------------------+-------------+-----------+--------------+------------+--------+------------+--------+--------------------+\n",
      "|Ho pubblicato una...|          0.0|        0.0|           0.0|         0.0|     1.0|         0.0|       1|[0.08286286890506...|\n",
      "|One reason my lif...|         null|       null|          null|        null|    null|        null|       0|[0.05419481918215...|\n",
      "|                None|         null|       null|          null|        null|    null|        null|       0|[0.0, 0.0, 0.0, 0...|\n",
      "+--------------------+-------------+-----------+--------------+------------+--------+------------+--------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tweets_df.show(3), test_tweets_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03139bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|BotOrNot|independent_features|\n",
      "+--------+--------------------+\n",
      "|       1|      (38,[4],[1.0])|\n",
      "|       1|      (38,[4],[1.0])|\n",
      "|       1|[0.0,0.0,0.0,0.0,...|\n",
      "+--------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------+--------------------+\n",
      "|BotOrNot|independent_features|\n",
      "+--------+--------------------+\n",
      "|       1|[0.0,0.0,0.0,0.0,...|\n",
      "|       0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0|          (38,[],[])|\n",
      "+--------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Assable multiple colu,ms to create feature vector\n",
    "train_tweets_df = assembleColumns(train_tweets_df)\n",
    "test_tweets_df = assembleColumns(test_tweets_df)\n",
    "#     print(len(tweets_updated_df.collect()), tweets_updated_df.columns)\n",
    "train_tweets_df.show(3), test_tweets_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f719f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|BotOrNot|independent_features|\n",
      "+--------+--------------------+\n",
      "|       1|      (38,[4],[1.0])|\n",
      "|       1|      (38,[4],[1.0])|\n",
      "|       1|[0.0,0.0,0.0,0.0,...|\n",
      "|       0|[0.0,0.0,0.0,0.0,...|\n",
      "|       1|[0.0,0.0,0.0,0.0,...|\n",
      "|       1|      (38,[4],[1.0])|\n",
      "|       1|      (38,[4],[1.0])|\n",
      "|       1|      (38,[4],[1.0])|\n",
      "|       1|[0.0,0.0,0.0,0.0,...|\n",
      "|       1|[0.0,0.0,0.0,0.0,...|\n",
      "+--------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------+--------------------+\n",
      "|BotOrNot|independent_features|\n",
      "+--------+--------------------+\n",
      "|       1|[0.0,0.0,0.0,0.0,...|\n",
      "|       0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0|          (38,[],[])|\n",
      "|       1|      (38,[4],[1.0])|\n",
      "|       0|[0.0,231.0,0.0,0....|\n",
      "|       1|[0.0,0.0,0.0,0.0,...|\n",
      "|       0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0|[0.0,2.0,0.0,0.0,...|\n",
      "|       0|[0.0,11.0,0.0,0.0...|\n",
      "|       0|[0.0,0.0,0.0,0.0,...|\n",
      "+--------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tweets_df.show(10), test_tweets_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21cfd102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   BotOrNot                               independent_features\n",
       " 0         1  (0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       " 1         1  (0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       " 2         1  [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, -0.004853100981...\n",
       " 3         0  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0534343086183...\n",
       " 4         1  [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, -0.004853100981...\n",
       " 5         1  (0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       " 6         1  (0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       " 7         1  (0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       " 8         1  [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0277484916150...\n",
       " 9         1  [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2018491774797...,\n",
       "    BotOrNot                               independent_features\n",
       " 0         1  [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, -0.050628747791...\n",
       " 1         0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0897102504968...\n",
       " 2         0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       " 3         1  (0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       " 4         0  [0.0, 231.0, 0.0, 0.0, 0.0, 1.0, 0.02456100471...\n",
       " 5         1  [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, -0.050628747791...\n",
       " 6         0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.119067132472...\n",
       " 7         0  [0.0, 2.0, 0.0, 0.0, 2.0, 0.0, -0.108488641679...\n",
       " 8         0  [0.0, 11.0, 0.0, 0.0, 0.0, 1.0, -0.04298163577...\n",
       " 9         0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0283270031213...)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_tweets_df.limit(10).toPandas(), test_tweets_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "784458ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- BotOrNot: integer (nullable = false)\n",
      " |-- independent_features: vector (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>independent_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, -0.021466195583...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                independent_features\n",
       "0  (0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "1  (0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "2  [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, -0.021466195583..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tweets_df.printSchema()\n",
    "train_tweets_df.select('independent_features').limit(3).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fd84818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------------+\n",
      "|BotOrNot|scaled_independent_features|\n",
      "+--------+---------------------------+\n",
      "|       1|       (38,[4],[1.982418...|\n",
      "|       1|       (38,[4],[1.982418...|\n",
      "|       1|       [0.0,0.0,0.0,0.0,...|\n",
      "+--------+---------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------+-------------------------+\n",
      "|BotOrNot|norm_independent_features|\n",
      "+--------+-------------------------+\n",
      "|       1|           (38,[4],[1.0])|\n",
      "|       1|           (38,[4],[1.0])|\n",
      "|       1|     [0.0,0.0,0.0,0.0,...|\n",
      "+--------+-------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------+---------------------------+\n",
      "|BotOrNot|scaled_independent_features|\n",
      "+--------+---------------------------+\n",
      "|       1|       [0.0,0.0,0.0,0.0,...|\n",
      "|       0|       [0.0,0.0,0.0,0.0,...|\n",
      "|       0|                 (38,[],[])|\n",
      "+--------+---------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------+-------------------------+\n",
      "|BotOrNot|norm_independent_features|\n",
      "+--------+-------------------------+\n",
      "|       1|     [0.0,0.0,0.0,0.0,...|\n",
      "|       0|     [0.0,0.0,0.0,0.0,...|\n",
      "|       0|               (38,[],[])|\n",
      "+--------+-------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_tweets_df = doDataScaling(train_tweets_df, \"independent_features\", \"scaled_independent_features\").drop(\"independent_features\")\n",
    "train_tweets_df.show(3)\n",
    "train_tweets_df = doDataNormalization(train_tweets_df, \"scaled_independent_features\", \"norm_independent_features\").drop(\"scaled_independent_features\")\n",
    "train_tweets_df.show(3)\n",
    "\n",
    "test_tweets_df = doDataScaling(test_tweets_df, \"independent_features\", \"scaled_independent_features\").drop(\"independent_features\")\n",
    "test_tweets_df.show(3)\n",
    "test_tweets_df = doDataNormalization(test_tweets_df, \"scaled_independent_features\", \"norm_independent_features\").drop(\"scaled_independent_features\")\n",
    "test_tweets_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eaad3c7c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   BotOrNot                          norm_independent_features\n",
       " 0         1  (0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       " 1         1  (0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       " 2         1  [0.0, 0.0, 0.0, 0.0, 0.8243733216608394, 0.0, ...\n",
       " 3         0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.21711739389638035,...\n",
       " 4         1  [0.0, 0.0, 0.0, 0.0, 0.6736178415049635, 0.0, ...\n",
       " 5         1  (0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       " 6         1  (0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       " 7         1  (0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       " 8         1  [0.0, 0.0, 0.0, 0.0, 0.42456348001042116, 0.0,...\n",
       " 9         1  [0.0, 0.0, 0.0, 0.0, 0.2806755258496218, 0.0, ...,\n",
       "    BotOrNot                          norm_independent_features\n",
       " 0         1  [0.0, 0.0, 0.0, 0.0, 0.1512490132054026, 0.0, ...\n",
       " 1         0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.091804945485...\n",
       " 2         0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       " 3         1  (0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       " 4         0  [0.0, 0.5767901527142147, 0.0, 0.0, 0.0, 0.345...\n",
       " 5         1  [0.0, 0.0, 0.0, 0.0, 0.17919382845996928, 0.0,...\n",
       " 6         0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0371204364454...\n",
       " 7         0  [0.0, 0.0032468506819385134, 0.0, 0.0, 0.24977...\n",
       " 8         0  [0.0, 0.015249802570804154, 0.0, 0.0, 0.0, 0.1...\n",
       " 9         0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0153375745677...)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tweets_df.persist()\n",
    "test_tweets_df.persist()\n",
    "\n",
    "train_tweets_df.limit(10).toPandas(), test_tweets_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "579ec7f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_X, train_Y \u001b[38;5;241m=\u001b[39m \u001b[43mto_nparray_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_tweets_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnorm_independent_features\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBotOrNot\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m test_X, test_Y \u001b[38;5;241m=\u001b[39m to_nparray_dataset(test_tweets_df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnorm_independent_features\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBotOrNot\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m train_X\u001b[38;5;241m.\u001b[39msize, train_Y\u001b[38;5;241m.\u001b[39msize, test_X\u001b[38;5;241m.\u001b[39msize, test_Y\u001b[38;5;241m.\u001b[39msize\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mto_nparray_dataset\u001b[1;34m(df, feature_column, target_column)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_nparray_dataset\u001b[39m(df, feature_column, target_column):\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m#     list(df.select('col_name').toPandas()['col_name']) \u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m#     feature = list(df.select(feature_column).toPandas()[feature_column])\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m#     target = list(df.select(target_column).toPandas()[target_column])\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m     feature \u001b[38;5;241m=\u001b[39m [row[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_column\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoLocalIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m     56\u001b[0m     target \u001b[38;5;241m=\u001b[39m [row[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(df\u001b[38;5;241m.\u001b[39mselect(target_column)\u001b[38;5;241m.\u001b[39mtoLocalIterator())]\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(feature), np\u001b[38;5;241m.\u001b[39marray(target)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\rdd.py:274\u001b[0m, in \u001b[0;36m_local_iterator_from_socket.<locals>.PyLocalIterable.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sockfile\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m    273\u001b[0m \u001b[38;5;66;03m# If response is 1 then there is a partition to read, if 0 then fully consumed\u001b[39;00m\n\u001b[1;32m--> 274\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status \u001b[38;5;241m=\u001b[39m \u001b[43mread_int\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sockfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    276\u001b[0m \n\u001b[0;32m    277\u001b[0m     \u001b[38;5;66;03m# Load the partition data as a stream and read each item\u001b[39;00m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serializer\u001b[38;5;241m.\u001b[39mload_stream(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sockfile)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\serializers.py:593\u001b[0m, in \u001b[0;36mread_int\u001b[1;34m(stream)\u001b[0m\n\u001b[0;32m    592\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_int\u001b[39m(stream):\n\u001b[1;32m--> 593\u001b[0m     length \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m length:\n\u001b[0;32m    595\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_X, train_Y = to_nparray_dataset(train_tweets_df, \"norm_independent_features\", 'BotOrNot')\n",
    "test_X, test_Y = to_nparray_dataset(test_tweets_df, \"norm_independent_features\", 'BotOrNot')\n",
    "\n",
    "train_X.size, train_Y.size, test_X.size, test_Y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b86555",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y = SMOTEENN(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c2e99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3898097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get2DenseLayeredModel(38)\n",
    "\n",
    "# in order to calculate accuracy using Keras' functions we first need\n",
    "# to compile the model\n",
    "model.fit(train_X, train_Y, epochs=20)\n",
    "model.compile(optimizer= opt, loss=cce, metrics=[\"acc\"])\n",
    "\n",
    "\n",
    "# now that the model is compiled we can compute the accuracy\n",
    "(loss, acc) = model.evaluate(test_X, test_Y)\n",
    "print(\"[INFO] test accuracy: {}\".format(acc))\n",
    "print(\"[INFO] test loss: {}\".format(loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f9dd8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
