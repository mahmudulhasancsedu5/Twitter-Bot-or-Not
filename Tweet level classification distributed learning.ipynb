{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e68da5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 400\n",
      "200 400\n",
      "200\n",
      "400\n",
      "400\n",
      "400\n",
      "400\n",
      "200 400\n",
      "200 400\n",
      "600\n",
      "600\n",
      "['text', 'retweet_count', 'reply_count', 'favorite_count', 'num_hashtags', 'num_urls', 'num_mentions', 'BotOrNot']\n",
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- retweet_count: double (nullable = true)\n",
      " |-- reply_count: double (nullable = true)\n",
      " |-- favorite_count: double (nullable = true)\n",
      " |-- num_hashtags: double (nullable = true)\n",
      " |-- num_urls: double (nullable = true)\n",
      " |-- num_mentions: double (nullable = true)\n",
      " |-- BotOrNot: integer (nullable = false)\n",
      " |-- text_features: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      "\n",
      "600 <class 'pyspark.sql.dataframe.DataFrame'> None\n",
      "root\n",
      " |-- retweet_count: double (nullable = true)\n",
      " |-- reply_count: double (nullable = true)\n",
      " |-- favorite_count: double (nullable = true)\n",
      " |-- num_hashtags: double (nullable = true)\n",
      " |-- num_urls: double (nullable = true)\n",
      " |-- num_mentions: double (nullable = true)\n",
      " |-- BotOrNot: integer (nullable = false)\n",
      " |-- text_features[0]: double (nullable = true)\n",
      " |-- text_features[1]: double (nullable = true)\n",
      " |-- text_features[2]: double (nullable = true)\n",
      " |-- text_features[3]: double (nullable = true)\n",
      " |-- text_features[4]: double (nullable = true)\n",
      " |-- text_features[5]: double (nullable = true)\n",
      " |-- text_features[6]: double (nullable = true)\n",
      " |-- text_features[7]: double (nullable = true)\n",
      " |-- text_features[8]: double (nullable = true)\n",
      " |-- text_features[9]: double (nullable = true)\n",
      " |-- text_features[10]: double (nullable = true)\n",
      " |-- text_features[11]: double (nullable = true)\n",
      " |-- text_features[12]: double (nullable = true)\n",
      " |-- text_features[13]: double (nullable = true)\n",
      " |-- text_features[14]: double (nullable = true)\n",
      " |-- text_features[15]: double (nullable = true)\n",
      " |-- text_features[16]: double (nullable = true)\n",
      " |-- text_features[17]: double (nullable = true)\n",
      " |-- text_features[18]: double (nullable = true)\n",
      " |-- text_features[19]: double (nullable = true)\n",
      " |-- text_features[20]: double (nullable = true)\n",
      " |-- text_features[21]: double (nullable = true)\n",
      " |-- text_features[22]: double (nullable = true)\n",
      " |-- text_features[23]: double (nullable = true)\n",
      " |-- text_features[24]: double (nullable = true)\n",
      "\n",
      "['retweet_count', 'reply_count', 'favorite_count', 'num_hashtags', 'num_urls', 'num_mentions', 'BotOrNot', 'text_features[0]', 'text_features[1]', 'text_features[2]', 'text_features[3]', 'text_features[4]', 'text_features[5]', 'text_features[6]', 'text_features[7]', 'text_features[8]', 'text_features[9]', 'text_features[10]', 'text_features[11]', 'text_features[12]', 'text_features[13]', 'text_features[14]', 'text_features[15]', 'text_features[16]', 'text_features[17]', 'text_features[18]', 'text_features[19]', 'text_features[20]', 'text_features[21]', 'text_features[22]', 'text_features[23]', 'text_features[24]'] 600 None\n",
      "root\n",
      " |-- retweet_count: double (nullable = false)\n",
      " |-- reply_count: double (nullable = false)\n",
      " |-- favorite_count: double (nullable = false)\n",
      " |-- num_hashtags: double (nullable = false)\n",
      " |-- num_urls: double (nullable = false)\n",
      " |-- num_mentions: double (nullable = false)\n",
      " |-- BotOrNot: integer (nullable = false)\n",
      " |-- text_features[0]: double (nullable = false)\n",
      " |-- text_features[1]: double (nullable = false)\n",
      " |-- text_features[2]: double (nullable = false)\n",
      " |-- text_features[3]: double (nullable = false)\n",
      " |-- text_features[4]: double (nullable = false)\n",
      " |-- text_features[5]: double (nullable = false)\n",
      " |-- text_features[6]: double (nullable = false)\n",
      " |-- text_features[7]: double (nullable = false)\n",
      " |-- text_features[8]: double (nullable = false)\n",
      " |-- text_features[9]: double (nullable = false)\n",
      " |-- text_features[10]: double (nullable = false)\n",
      " |-- text_features[11]: double (nullable = false)\n",
      " |-- text_features[12]: double (nullable = false)\n",
      " |-- text_features[13]: double (nullable = false)\n",
      " |-- text_features[14]: double (nullable = false)\n",
      " |-- text_features[15]: double (nullable = false)\n",
      " |-- text_features[16]: double (nullable = false)\n",
      " |-- text_features[17]: double (nullable = false)\n",
      " |-- text_features[18]: double (nullable = false)\n",
      " |-- text_features[19]: double (nullable = false)\n",
      " |-- text_features[20]: double (nullable = false)\n",
      " |-- text_features[21]: double (nullable = false)\n",
      " |-- text_features[22]: double (nullable = false)\n",
      " |-- text_features[23]: double (nullable = false)\n",
      " |-- text_features[24]: double (nullable = false)\n",
      " |-- independent_features: vector (nullable = true)\n",
      "\n",
      "600 <class 'pyspark.sql.dataframe.DataFrame'> None\n",
      "+-------------+-----------+--------------+------------+--------+------------+--------+-------------------+-------------------+-------------------+------------------+-----------------+--------------------+-------------------+-------------------+------------------+-------------------+--------------------+------------------+-------------------+--------------------+-----------------+--------------------+------------------+--------------------+------------------+-------------------+--------------------+-------------------+--------------------+-------------------+--------------------+--------------------+\n",
      "|retweet_count|reply_count|favorite_count|num_hashtags|num_urls|num_mentions|BotOrNot|   text_features[0]|   text_features[1]|   text_features[2]|  text_features[3]| text_features[4]|    text_features[5]|   text_features[6]|   text_features[7]|  text_features[8]|   text_features[9]|   text_features[10]| text_features[11]|  text_features[12]|   text_features[13]|text_features[14]|   text_features[15]| text_features[16]|   text_features[17]| text_features[18]|  text_features[19]|   text_features[20]|  text_features[21]|   text_features[22]|  text_features[23]|   text_features[24]|independent_features|\n",
      "+-------------+-----------+--------------+------------+--------+------------+--------+-------------------+-------------------+-------------------+------------------+-----------------+--------------------+-------------------+-------------------+------------------+-------------------+--------------------+------------------+-------------------+--------------------+-----------------+--------------------+------------------+--------------------+------------------+-------------------+--------------------+-------------------+--------------------+-------------------+--------------------+--------------------+\n",
      "|          0.0|        0.0|           0.0|         0.0|     1.0|         0.0|       0|                0.0|                0.0|                0.0|               0.0|              0.0|                 0.0|                0.0|                0.0|               0.0|                0.0|                 0.0|               0.0|                0.0|                 0.0|              0.0|                 0.0|               0.0|                 0.0|               0.0|                0.0|                 0.0|                0.0|                 0.0|                0.0|                 0.0|      (31,[4],[1.0])|\n",
      "|          0.0|        0.0|           0.0|         0.0|     0.0|         0.0|       0|0.10883042961359024|-0.5760724544525146|-0.3426782190799713|0.3327847421169281|0.331408828496933|-0.39295288920402527|-0.2512320876121521|0.47155243158340454|0.3851715624332428|0.01516096480190754|-0.24488376080989838|0.4990305006504059|0.05301550403237343|0.018535461276769638|-0.12926085293293|-0.17131909728050232|-0.622990608215332|-0.13381993770599365|0.4059413969516754|0.24202540516853333|-0.28113266825675964|0.37138795852661133|-0.12830473482608795|0.23567193746566772|-0.43101099133491516|[0.0,0.0,0.0,0.0,...|\n",
      "|          0.0|        0.0|           0.0|         0.0|     1.0|         0.0|       0|                0.0|                0.0|                0.0|               0.0|              0.0|                 0.0|                0.0|                0.0|               0.0|                0.0|                 0.0|               0.0|                0.0|                 0.0|              0.0|                 0.0|               0.0|                 0.0|               0.0|                0.0|                 0.0|                0.0|                 0.0|                0.0|                 0.0|      (31,[4],[1.0])|\n",
      "|          0.0|        0.0|           0.0|         0.0|     1.0|         0.0|       0|                0.0|                0.0|                0.0|               0.0|              0.0|                 0.0|                0.0|                0.0|               0.0|                0.0|                 0.0|               0.0|                0.0|                 0.0|              0.0|                 0.0|               0.0|                 0.0|               0.0|                0.0|                 0.0|                0.0|                 0.0|                0.0|                 0.0|      (31,[4],[1.0])|\n",
      "|          0.0|        0.0|           0.0|         0.0|     1.0|         0.0|       0|                0.0|                0.0|                0.0|               0.0|              0.0|                 0.0|                0.0|                0.0|               0.0|                0.0|                 0.0|               0.0|                0.0|                 0.0|              0.0|                 0.0|               0.0|                 0.0|               0.0|                0.0|                 0.0|                0.0|                 0.0|                0.0|                 0.0|      (31,[4],[1.0])|\n",
      "+-------------+-----------+--------------+------------+--------+------------+--------+-------------------+-------------------+-------------------+------------------+-----------------+--------------------+-------------------+-------------------+------------------+-------------------+--------------------+------------------+-------------------+--------------------+-----------------+--------------------+------------------+--------------------+------------------+-------------------+--------------------+-------------------+--------------------+-------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 ['BotOrNot', 'independent_features']\n",
      "478 122\n",
      "478 478\n",
      "122 122\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 1s 35ms/step - loss: 0.6216 - accuracy: 0.6841 - val_loss: 0.5418 - val_accuracy: 0.7623\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.4992 - accuracy: 0.7782 - val_loss: 0.4657 - val_accuracy: 0.7787\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.4333 - accuracy: 0.7929 - val_loss: 0.4543 - val_accuracy: 0.7787\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.3955 - accuracy: 0.8180 - val_loss: 0.4370 - val_accuracy: 0.7787\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.3792 - accuracy: 0.8285 - val_loss: 0.4310 - val_accuracy: 0.7869\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.3737 - accuracy: 0.8305 - val_loss: 0.4194 - val_accuracy: 0.7951\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.3726 - accuracy: 0.8159 - val_loss: 0.4703 - val_accuracy: 0.7951\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.3530 - accuracy: 0.8431 - val_loss: 0.4288 - val_accuracy: 0.7951\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.3485 - accuracy: 0.8347 - val_loss: 0.4413 - val_accuracy: 0.7951\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.3370 - accuracy: 0.8452 - val_loss: 0.4287 - val_accuracy: 0.8033\n",
      "Test accuracy: 0.8032786846160889\n"
     ]
    }
   ],
   "source": [
    "#libraries / dependencies\n",
    "import glob\n",
    "from preprocessor import api as tweet_preprocessor\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "from pyspark.sql.functions import col, udf, to_timestamp, lit, to_timestamp, when, rand\n",
    "from pyspark.sql.types import IntegerType, LongType, DoubleType, StringType, ArrayType\n",
    "from pyspark.ml.feature import Normalizer, StandardScaler, MinMaxScaler, VectorAssembler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Dense, Input, concatenate, Dropout, Activation, BatchNormalization\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.losses import MeanSquaredError, CategoricalCrossentropy\n",
    "\n",
    "\n",
    "#for windows so disable when run in linux clusted\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "#Dataset location\n",
    "bot_tweets_dataset_path = 'F://TwitterBotDataset//tweet_dataset_small_100_200//bot_tweets//'\n",
    "genuine_tweets_dataset_path = 'F://TwitterBotDataset//tweet_dataset_small_100_200//genuine_tweets//'\n",
    "\n",
    "#turn a line of text into d dimentional vector \n",
    "GLOVE_DIR = \"C://Users//USER//projects//\"\n",
    "\n",
    "#all columns\n",
    "BOT_COLUMNS = ['id','text','source','user_id','truncated','in_reply_to_status_id', \n",
    "               'in_reply_to_user_id','in_reply_to_screen_name', 'retweeted_status_id',\n",
    "               'geo','place','contributors','retweet_count', 'reply_count','favorite_count',\n",
    "               'favorited', 'retweeted','possibly_sensitive','num_hashtags','num_urls',\n",
    "               'num_mentions','created_at','timestamp','crawled_at', 'updated']\n",
    "\n",
    "GENUINE_COLUMNS = ['id','text','source','user_id','truncated','in_reply_to_status_id', \n",
    "                   'in_reply_to_user_id','in_reply_to_screen_name', 'retweeted_status_id',\n",
    "                   'geo','place','contributors','retweet_count', 'reply_count','favorite_count',\n",
    "                   'favorited', 'retweeted','possibly_sensitive','num_hashtags','num_urls',\n",
    "                   'num_mentions','REMOVE_IT', 'created_at','timestamp','crawled_at', 'updated',]\n",
    "\n",
    "#feature used for bot detection\n",
    "COLUMN_NAMES = ['text', 'retweet_count', 'reply_count', 'favorite_count',\n",
    "                'num_hashtags', 'num_urls', 'num_mentions']\n",
    "\n",
    "\n",
    "#configure spark\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[8]\").setAppName(\"ml_account_ base_session\")\n",
    "conf.set(\"spark.executor.instances\", 4)\n",
    "conf.set(\"spark.executor.cores\", 4)\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# init spark, configure spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark\n",
    "\n",
    "# read dataset from csv\n",
    "def read_dataset():\n",
    "    bot_tweets = spark.read.csv(bot_tweets_dataset_path, header = True, inferSchema = True)\n",
    "    genuine_tweets = spark.read.csv(genuine_tweets_dataset_path, header = True, inferSchema = True)\n",
    "    \n",
    "    print(len(bot_tweets.collect()), len(genuine_tweets.collect()))\n",
    "    return bot_tweets, genuine_tweets\n",
    "\n",
    "def set_column_name(df, column_names):\n",
    "    df = df.toDF(*column_names)\n",
    "    print(len(df.collect()))\n",
    "    return df\n",
    "\n",
    "def remove_column_miss_match(df):\n",
    "    ## dataset have diffrent number of columns\n",
    "    ## column name of dataframe\n",
    "    column_name = [cname for cname, tp in df.dtypes]\n",
    "    len(df.collect()), len(df.dtypes)\n",
    "    #column_name\n",
    "\n",
    "    #Number of column is diffrent for bot and genuine tweets data\n",
    "\n",
    "    #genuine_tweets_df = genuine_tweets_df.toDF(*column_name)\n",
    "    df = set_column_name(df, GENUINE_COLUMNS)\n",
    "    print(len(df.collect()))\n",
    "    \n",
    "    df = df.drop('REMOVE_IT') # remove 5th column from end\n",
    "    #update column name according to \n",
    "    df = set_column_name(df, BOT_COLUMNS)\n",
    "    print(len(df.collect()))\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_type_miss_match(df):\n",
    "    # Same column has diffrent data type. So make data type same for every column\n",
    "    genuine_tweets_df = df.withColumn(\"id\",col(\"id\").cast(IntegerType())) \\\n",
    "                                    .withColumn(\"favorite_count\",col(\"favorite_count\").cast(LongType())) \\\n",
    "                                    .withColumn(\"favorited\",col(\"favorited\").cast(IntegerType()))\n",
    "    return df\n",
    "\n",
    "\n",
    "def resize_combine_data(bot_tweets_df, genuine_tweets_df):\n",
    "    ## only keep the required column from the dataframe\n",
    "    bot_tweets_df = bot_tweets_df.select(*COLUMN_NAMES)\n",
    "    genuine_tweets_df = genuine_tweets_df.select(*COLUMN_NAMES)\n",
    "    \n",
    "    print(len(bot_tweets_df.collect()), len(genuine_tweets_df.collect()))\n",
    "\n",
    "    ## add BotOrNot column\n",
    "    bot_tweets_df = bot_tweets_df.withColumn('BotOrNot', lit(1))\n",
    "    genuine_tweets_df = genuine_tweets_df.withColumn('BotOrNot', lit(0))\n",
    "\n",
    "    #combine clean and bot accounts data togather\n",
    "    tweets_df = bot_tweets_df.union(genuine_tweets_df)\n",
    "\n",
    "    # shuffle dataset\n",
    "    tweets_df = tweets_df.orderBy(rand())\n",
    "\n",
    "    print(len(tweets_df.collect()))\n",
    "    \n",
    "    return tweets_df\n",
    "\n",
    "text_process_udf = udf(lambda x : tweet_preprocessor.tokenize(x), StringType())\n",
    "def preprocess_data(df):\n",
    "    df = df.withColumn('text', text_process_udf(df.text))\n",
    "    df = df.withColumn(\"retweet_count\",col(\"retweet_count\").cast(DoubleType()))\n",
    "    df = df.withColumn(\"reply_count\",col(\"reply_count\").cast(DoubleType()))\n",
    "    df = df.withColumn(\"favorite_count\",col(\"favorite_count\").cast(DoubleType()))\n",
    "    df = df.withColumn(\"num_hashtags\",col(\"num_hashtags\").cast(DoubleType()))\n",
    "    df = df.withColumn(\"num_urls\",col(\"num_urls\").cast(DoubleType()))\n",
    "    df = df.withColumn(\"num_mentions\",col(\"num_mentions\").cast(DoubleType()))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def makeGloveWordEmbedder(glove_path):\n",
    "    embedding_dict = {}\n",
    "    with open(glove_path, 'r', encoding=\"utf-8\") as glove_file:\n",
    "        for line in glove_file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embedding_dict[word] = vector\n",
    "            \n",
    "    return embedding_dict  \n",
    "\n",
    "\n",
    "# # Test GLoVE result\n",
    "# glove_word2vec_embedder[\"google\"]\n",
    "\n",
    " ##create word embedding GLoVE model dictionary. Use pre trained model\n",
    "text_feature_dimention = 25\n",
    "glove_word2vec_embedder = makeGloveWordEmbedder(GLOVE_DIR + \"glove.twitter.27B.25d.txt\")\n",
    "\n",
    "\n",
    "#Give a word and get that word representing feature vector of dimention 25 from embedding dictionary\n",
    "def word2vec(word_dict=None, word=None, dim=25):\n",
    "    default_vector = np.zeros(dim)\n",
    "    \n",
    "    if word_dict is None or word is None:\n",
    "        return default_vector\n",
    "    \n",
    "    word_vector = word_dict.get(word)\n",
    "    \n",
    "    if word_vector is None:\n",
    "        return default_vector\n",
    "    return word_vector\n",
    "\n",
    "# # test a word representing feature vector\n",
    "# word_vector = word2vec(glove_word2vec_embedder, \"tweet\", text_feature_dimention)\n",
    "# print(type(word_vector), word_vector)\n",
    "\n",
    "\n",
    "#----------------LSTM Model----------------------\n",
    "\n",
    "#create a 1 layer LSTM model\n",
    "#input dimention: 3d (1,7,25) input_sample = [[[1,...,25]],..,[1,...,25]]\n",
    "#output dimention: 1d (1,32) output_sample [[1,2,3,,,,32]]\n",
    "def lstm_model(output_dim):\n",
    "    model = LSTM(output_dim, return_sequences=False, return_state=False)\n",
    "    return model\n",
    "\n",
    "def reset_lstm_model(model):\n",
    "    model.resate_states() # stateful=True is required for reset states\n",
    "\n",
    "# create LSTM model of output dimention 32. Model output feature vector will be of 32 dimention vector\n",
    "lstm = lstm_model(32) \n",
    "\n",
    "# convver a sentence to a feature vector using LSTM(RNN) model\n",
    "def sent2vec(sent):\n",
    "    words = sent.split(' ')\n",
    "    word_vectors = np.array([])\n",
    "    count = 0;\n",
    "    for word in words:\n",
    "        word_vector = word2vec(glove_word2vec_embedder, word)\n",
    "#         print(\"word dim: {}\".format(len(word_vector)))\n",
    "        if word_vectors.size == 0:\n",
    "            word_vectors = np.array([word_vector])\n",
    "        else:\n",
    "            word_vectors = np.vstack([word_vectors, word_vector])\n",
    "        count = count + 1\n",
    "    \n",
    "#     print(\"Input feature vector shape before reshape(2D): {}\".format(word_vectors.shape))\n",
    "        \n",
    "    input_feature_vectors = np.reshape(word_vectors, (1, count, text_feature_dimention))\n",
    "#     print(\"Input feature vector shape after reshape(3d): {}\".format(input_feature_vectors.shape))\n",
    "#     print(\"LSTM requirs 3d shape inputs [batch, timesteps, feature]\")\n",
    "    output_vector = lstm(input_feature_vectors)\n",
    "#     lstm.reset_states() # stateful = True is required for reset\n",
    "\n",
    "#     print(\"result vector shape: {}\".format(output_vector.shape))\n",
    "#     print(\"Last input was: {}\".format(input_feature_vectors[0][-1]))\n",
    "#     print(\"output result: {}\".format(output_vector))\n",
    "    \n",
    "    # (tensore --> numpy 0bject --> numpy.array --> array/list/ArrayType)\n",
    "    return output_vector.numpy()[0].tolist() \n",
    "    \n",
    "## For Testing sentence to vector convertion\n",
    "# sent = \"Twitter is a large social media network\"\n",
    "# res_vector = sent2vec(sent)\n",
    "# type(res_vector), res_vector\n",
    "\n",
    "\n",
    "# text string --> vector 32 dimention\n",
    "sent_to_vector_udf = udf(lambda x : sent2vec(x), ArrayType(DoubleType()))\n",
    "def processTextColumn(df, column_name, new_column_name):\n",
    "    df = df.withColumn(new_column_name, sent_to_vector_udf(col(column_name)))\n",
    "    return df\n",
    "\n",
    "def sentEmbeddingGLoVE_LSTM(df):\n",
    "    \n",
    "    text_updated_column = 'text_features'\n",
    "    updated_df = processTextColumn(df, \"text\", text_updated_column)\n",
    "\n",
    "    print(len(updated_df.collect()), type(updated_df), updated_df.printSchema()) \n",
    "    \n",
    "    return updated_df\n",
    "\n",
    "\n",
    "\n",
    "def assembleColumns(tweets_df):\n",
    "    columns = ['retweet_count', 'reply_count', 'favorite_count',\n",
    "               'num_hashtags' ,'num_urls', 'num_mentions', 'BotOrNot']\n",
    "\n",
    "    tweets_df = tweets_df.select(*columns, \n",
    "                          tweets_df.text_features[0], tweets_df.text_features[1], tweets_df.text_features[2],tweets_df.text_features[3], tweets_df.text_features[4],\n",
    "                          tweets_df.text_features[5], tweets_df.text_features[6], tweets_df.text_features[7],tweets_df.text_features[8], tweets_df.text_features[9], \n",
    "                          tweets_df.text_features[10], tweets_df.text_features[11], tweets_df.text_features[12],tweets_df.text_features[13], tweets_df.text_features[14],\n",
    "                          tweets_df.text_features[15], tweets_df.text_features[16], tweets_df.text_features[17],tweets_df.text_features[18], tweets_df.text_features[19],\n",
    "                          tweets_df.text_features[20], tweets_df.text_features[21], tweets_df.text_features[22],tweets_df.text_features[23], tweets_df.text_features[24])\n",
    "\n",
    "\n",
    "    print(tweets_df.columns, len(tweets_df.collect()), tweets_df.printSchema())\n",
    "\n",
    "    #remove \n",
    "\n",
    "    feature_columns = ['retweet_count','reply_count','favorite_count','num_hashtags','num_urls','num_mentions',\n",
    "                       'text_features[0]','text_features[1]', 'text_features[2]','text_features[3]','text_features[4]',\n",
    "                       'text_features[5]','text_features[6]','text_features[7]', 'text_features[8]','text_features[9]',\n",
    "                       'text_features[10]','text_features[11]','text_features[12]','text_features[13]','text_features[14]',\n",
    "                       'text_features[15]','text_features[16]','text_features[17]','text_features[18]','text_features[19]',\n",
    "                       'text_features[20]','text_features[21]','text_features[22]', 'text_features[23]', 'text_features[24]']\n",
    "\n",
    "\n",
    "    tweets_df = tweets_df.na.fill(value=0.0 ,subset= feature_columns)\n",
    "    feature_assembler = VectorAssembler(inputCols = feature_columns, outputCol = 'independent_features')\n",
    "\n",
    "    tweets_updated_df = feature_assembler.transform(tweets_df)\n",
    "\n",
    "    #check\n",
    "    num = len(tweets_updated_df.collect())\n",
    "    print(num, type(tweets_updated_df), tweets_updated_df.printSchema())\n",
    "\n",
    "    #remove unnecessary columns\n",
    "    tweets_updated_df = tweets_updated_df.drop(*feature_columns)\n",
    "    \n",
    "    return tweets_updated_df\n",
    "\n",
    "\n",
    "def to_nparray_list(df, column_name):\n",
    "    rows = df.select(column_name).collect()\n",
    "    lists = [x[column_name] for x in rows]\n",
    "    nparr = np.array(lists)\n",
    "    \n",
    "    return nparr\n",
    "\n",
    "def partition_dataset(df):\n",
    "    train_df, test_df = df.randomSplit([0.80, 0.20])\n",
    "    print(len(train_df.collect()), len(test_df.collect()))\n",
    "\n",
    "    # features --> 'BotOrNot'\n",
    "    X_train = train_df.drop('BotOrNot')\n",
    "    y_train = train_df.select('BotOrNot')\n",
    "    X_test = test_df.drop('BotOrNot')\n",
    "    y_test = test_df.select('BotOrNot')\n",
    "\n",
    "    #checkpoint\n",
    "    print(len(X_train.collect()), len(y_train.collect()))\n",
    "    print(len(X_test.collect()), len(y_test.collect()))\n",
    "\n",
    "\n",
    "    X_train = to_nparray_list(X_train, 'independent_features')\n",
    "    y_train = to_nparray_list(y_train, 'BotOrNot')\n",
    "    X_test = to_nparray_list(X_test, 'independent_features')\n",
    "    y_test = to_nparray_list(y_test, 'BotOrNot')\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test # return type: numpy.array\n",
    "\n",
    "## create model\n",
    "def get2DenseLayeredModel(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(500, input_dim=input_dim))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(200))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def model_evaluation(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train,\n",
    "              batch_size=64,\n",
    "              epochs=10,\n",
    "              validation_data=(X_test, y_test))\n",
    "    score, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "\n",
    "def worker_task(bot_tweets_df, genuine_tweets_df):\n",
    "    \n",
    "    ##clean data / remove unwanted column\n",
    "    if len(bot_tweets_df.columns) == 26:\n",
    "        bot_tweets_df = remove_column_miss_match(bot_tweets_df)\n",
    "    else:\n",
    "        bot_tweets_df = set_column_name(bot_tweets_df, BOT_COLUMNS)\n",
    "        \n",
    "    if len(genuine_tweets_df.columns) == 26:\n",
    "        genuine_tweets_df = remove_column_miss_match(genuine_tweets_df)\n",
    "    else:\n",
    "        genuine_tweets_df = set_column_name(genuine_tweets_df, BOT_COLUMNS)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    bot_tweets_df = remove_type_miss_match(bot_tweets_df)\n",
    "    genuine_tweets_df = remove_type_miss_match(genuine_tweets_df)\n",
    "    \n",
    "    print(len(bot_tweets_df.collect()), len(genuine_tweets_df.collect()))\n",
    "    \n",
    "    ##preprocess data\n",
    "    tweets_df = resize_combine_data(bot_tweets_df, genuine_tweets_df)\n",
    "    tweets_df = preprocess_data(tweets_df)\n",
    "    \n",
    "    print(len(tweets_df.collect()))\n",
    "    print(tweets_df.columns)\n",
    "    \n",
    "    ##text embedding using GLoVE & LSTM\n",
    "    ## Word Embedding\n",
    "    tweets_df = sentEmbeddingGLoVE_LSTM(tweets_df)\n",
    "    \n",
    "    ## Assable multiple colu,ms to create feature vector\n",
    "    tweets_updated_df = assembleColumns(tweets_df)\n",
    "    print(len(tweets_updated_df.collect()), tweets_updated_df.columns)\n",
    "    \n",
    "    ##Create Dense Model\n",
    "    \n",
    "    ##device data into test and train parts\n",
    "    X_train, y_train, X_test, y_test = partition_dataset(tweets_updated_df)\n",
    "    \n",
    "    ## ml model train and validation\n",
    "    model = get2DenseLayeredModel(31)\n",
    "    \n",
    "    ##evaluate model\n",
    "    model_evaluation(model, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # save gradient to hdfs/s3\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ##load dara\n",
    "    bot_tweets_df, genuine_tweets_df = read_dataset()\n",
    "    print(bot_tweets_df.count(), genuine_tweets_df.count())\n",
    "    \n",
    "    ## single worker or multiple worker\n",
    "    worker_task(bot_tweets_df, genuine_tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "366fbb07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5.33E+17',\n",
       " 'Am postat o fotografie nouă pe Facebook http://t.co/Pfbffeys8e',\n",
       " '\"<a href=\"\"http://www.facebook.com/twitter\"\" rel=\"\"nofollow\"\">Facebook</a>\"',\n",
       " '16282004',\n",
       " '_c4',\n",
       " '05',\n",
       " '06',\n",
       " '_c7',\n",
       " '08',\n",
       " '_c9',\n",
       " '_c10',\n",
       " '_c11',\n",
       " '012',\n",
       " '013',\n",
       " '014',\n",
       " '_c15',\n",
       " '_c16',\n",
       " '_c17',\n",
       " '018',\n",
       " '1',\n",
       " '020',\n",
       " '_c21',\n",
       " 'Wed Nov 12 20:08:42 +0000 2014',\n",
       " '11/12/2014 21:08',\n",
       " '11/12/2014 21:3324',\n",
       " '11/12/2014 21:3325']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genuine_tweets_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98b6c38c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'text',\n",
       " 'source',\n",
       " 'user_id',\n",
       " 'truncated',\n",
       " 'in_reply_to_status_id',\n",
       " 'in_reply_to_user_id',\n",
       " 'in_reply_to_screen_name',\n",
       " 'retweeted_status_id',\n",
       " 'geo',\n",
       " 'place',\n",
       " 'contributors',\n",
       " 'retweet_count',\n",
       " 'reply_count',\n",
       " 'favorite_count',\n",
       " 'favorited',\n",
       " 'retweeted',\n",
       " 'possibly_sensitive',\n",
       " 'num_hashtags',\n",
       " 'num_urls',\n",
       " 'num_mentions',\n",
       " 'created_at',\n",
       " 'timestamp',\n",
       " 'crawled_at',\n",
       " 'updated']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot_tweets_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22f46d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
