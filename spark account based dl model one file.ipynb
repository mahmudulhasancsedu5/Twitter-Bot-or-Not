{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35f6b180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "105/105 [==============================] - 2s 7ms/step - loss: 0.4168 - accuracy: 0.8631 - val_loss: 0.3017 - val_accuracy: 0.8938\n",
      "Epoch 2/20\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.2529 - accuracy: 0.9175 - val_loss: 0.2583 - val_accuracy: 0.9142\n",
      "Epoch 3/20\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.2025 - accuracy: 0.9341 - val_loss: 0.1846 - val_accuracy: 0.9418\n",
      "Epoch 4/20\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.1760 - accuracy: 0.9427 - val_loss: 0.1660 - val_accuracy: 0.9496\n",
      "Epoch 5/20\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.1571 - accuracy: 0.9498 - val_loss: 0.1611 - val_accuracy: 0.9472\n",
      "Epoch 6/20\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.1432 - accuracy: 0.9521 - val_loss: 0.1488 - val_accuracy: 0.9556\n",
      "Epoch 7/20\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.1368 - accuracy: 0.9555 - val_loss: 0.1359 - val_accuracy: 0.9556\n",
      "Epoch 8/20\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.1318 - accuracy: 0.9583 - val_loss: 0.1300 - val_accuracy: 0.9598\n",
      "Epoch 9/20\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.1265 - accuracy: 0.9586 - val_loss: 0.1221 - val_accuracy: 0.9634\n",
      "Epoch 10/20\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.1236 - accuracy: 0.9607 - val_loss: 0.1216 - val_accuracy: 0.9556\n",
      "Epoch 11/20\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.1235 - accuracy: 0.9620 - val_loss: 0.1572 - val_accuracy: 0.9514\n",
      "Epoch 12/20\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.1155 - accuracy: 0.9649 - val_loss: 0.1103 - val_accuracy: 0.9688\n",
      "Epoch 13/20\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.1157 - accuracy: 0.9634 - val_loss: 0.1232 - val_accuracy: 0.9604\n",
      "Epoch 14/20\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.1165 - accuracy: 0.9649 - val_loss: 0.1218 - val_accuracy: 0.9598\n",
      "Epoch 15/20\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.1186 - accuracy: 0.9628 - val_loss: 0.1175 - val_accuracy: 0.9604\n",
      "Epoch 16/20\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.1072 - accuracy: 0.9683 - val_loss: 0.1035 - val_accuracy: 0.9712\n",
      "Epoch 17/20\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.1088 - accuracy: 0.9662 - val_loss: 0.1197 - val_accuracy: 0.9604\n",
      "Epoch 18/20\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.1073 - accuracy: 0.9687 - val_loss: 0.1039 - val_accuracy: 0.9676\n",
      "Epoch 19/20\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.1065 - accuracy: 0.9695 - val_loss: 0.1041 - val_accuracy: 0.9694\n",
      "Epoch 20/20\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.1004 - accuracy: 0.9687 - val_loss: 0.0973 - val_accuracy: 0.9736\n",
      "Test accuracy: 0.9736052751541138\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'saveModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 179>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    176\u001b[0m     saveModel(model)\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 180\u001b[0m     \u001b[43mapp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mapp\u001b[1;34m()\u001b[0m\n\u001b[0;32m    173\u001b[0m score, acc \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest accuracy:\u001b[39m\u001b[38;5;124m'\u001b[39m, acc)\n\u001b[1;32m--> 176\u001b[0m \u001b[43msaveModel\u001b[49m(model)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'saveModel' is not defined"
     ]
    }
   ],
   "source": [
    "# required libraries\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.functions import col, udf, to_timestamp, lit\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import when, rand\n",
    "from pyspark.ml.feature import Normalizer, StandardScaler, MinMaxScaler, VectorAssembler\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Dense, Input, concatenate, Dropout, Activation\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#dataset path\n",
    "dataset_folder_s3 = 's3://bot-dataset/data/'# 'data/' # 's3://bot-dataset/data/'\n",
    "result_path_s3 = 's3://bot-dataset/result/' # '' # 's3://bot-dataset/result/'\n",
    "\n",
    "## convert into feature vector for ml model\n",
    "feature_columns = ['age', 'has_location', 'is_verified', 'total_tweets', 'total_following', \n",
    "                   'total_followers', 'total_likes', 'has_avatar', 'has_background', \n",
    "                   'is_protected', 'profile_modified']\n",
    "\n",
    "# read dataset from csv\n",
    "def read_dataset(spark):\n",
    "    requiredColumns = ['screen_name', 'created_at', 'updated', 'location', 'verified', 'statuses_count', 'friends_count','followers_count', 'favourites_count', 'default_profile_image', 'profile_use_background_image', 'protected', 'default_profile']\n",
    "\n",
    "    bot_accounts1 = spark.read.csv(dataset_folder_s3 + 'social_spambots_1.csv', header = True, inferSchema = True).select(requiredColumns)\n",
    "    bot_accounts2 = spark.read.csv(dataset_folder_s3 + 'social_spambots_2.csv', header = True, inferSchema = True).select(requiredColumns)\n",
    "    bot_accounts3 = spark.read.csv(dataset_folder_s3 + 'social_spambots_3.csv', header = True, inferSchema = True).select(requiredColumns)\n",
    "\n",
    "    # combine multiple bot_account dataset\n",
    "    bot_accounts = bot_accounts1.union(bot_accounts2.union(bot_accounts3))\n",
    "    clean_accounts = spark.read.csv(dataset_folder_s3 + 'geniune_accounts.csv', header = True, inferSchema = True).select(requiredColumns)\n",
    "    \n",
    "    return bot_accounts, clean_accounts\n",
    "\n",
    "# clean dataset\n",
    "def cleanData(df):\n",
    "    df = df.withColumn('age', lit(0)) # need to calculate from 'updated' -'created_at'\n",
    "    df = df.withColumn('has_location', when((df['location'] != None), 1).otherwise(0))\n",
    "    df = df.withColumn('has_avatar', when((df['default_profile_image'] != None), 1).otherwise(0))\n",
    "    df = df.withColumn('has_background', when((df['profile_use_background_image'] != None), 1).otherwise(0))\n",
    "    df = df.withColumn('is_verified', when((df['verified'] != None), 1).otherwise(0))\n",
    "    df = df.withColumn('is_protected', when((df['protected'] != None), 1).otherwise(0))\n",
    "    df = df.withColumn('profile_modified', when((df['default_profile'] != None), 1).otherwise(0))\n",
    "    df = df.withColumnRenamed(\"screen_name\", \"username\")\n",
    "    df = df.withColumnRenamed(\"statuses_count\", \"total_tweets\")\n",
    "    df = df.withColumnRenamed(\"friends_count\", \"total_following\")\n",
    "    df = df.withColumnRenamed(\"followers_count\", \"total_followers\")\n",
    "    df = df.withColumnRenamed(\"favourites_count\", \"total_likes\")\n",
    "    \n",
    "    return df.select('username', 'age', 'has_location', 'is_verified', 'total_tweets', 'total_following', 'total_followers', 'total_likes', 'has_avatar', 'has_background', 'is_protected', 'profile_modified')\n",
    "\n",
    "\n",
    "\n",
    "def doDataScaling(df, input_column, output_column):\n",
    "    ## Make data standard\n",
    "    # https://spark.apache.org/docs/1.4.1/ml-features.html#standardscaler\n",
    "\n",
    "    scaler = StandardScaler(inputCol=input_column, outputCol=output_column,\n",
    "                            withStd=True, withMean=False)\n",
    "\n",
    "    # Compute summary statistics by fitting the StandardScaler\n",
    "    scalerModel = scaler.fit(df)\n",
    "\n",
    "    # Normalize each feature to have unit standard deviation.\n",
    "    scaled_df = scalerModel.transform(df)\n",
    "    \n",
    "    return scaled_df\n",
    "\n",
    "\n",
    "def datasetSplit(X):\n",
    "    # split data for training ana testing\n",
    "    train_df, test_df = X.randomSplit([0.80, 0.20])\n",
    "\n",
    "    # features --> 'BotOrNot'\n",
    "    X_train = train_df.drop('BotOrNot')\n",
    "    y_train = train_df.select('BotOrNot')\n",
    "    X_test = test_df.drop('BotOrNot')\n",
    "    y_test = test_df.select('BotOrNot')\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "\n",
    "## create model\n",
    "def getDLModel():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(500, input_dim=11))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(200))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# convert DataFrame column into nparray\n",
    "# nparray required for model training, validation\n",
    "\n",
    "def to_nparray_list(df, column_name):\n",
    "    rows = df.select(column_name).collect()\n",
    "    lists = [x[column_name] for x in rows]\n",
    "    nparr = np.array(lists)\n",
    "    \n",
    "    return nparr\n",
    "\n",
    "# ml model train and validation\n",
    "\n",
    "def saveModel(model):\n",
    "    model.save(result_path_s3 + 'my_model.h5')\n",
    "\n",
    "def app():\n",
    "    # init spark\n",
    "    spark = SparkSession.builder.appName('ml_account_base_session').getOrCreate()\n",
    "    \n",
    "    bot_accounts, clean_accounts = read_dataset(spark)\n",
    "    \n",
    "    bot_accounts = cleanData(bot_accounts)\n",
    "    clean_accounts = cleanData(clean_accounts)\n",
    "    \n",
    "    ## add BotOrNot column\n",
    "    bot_accounts = bot_accounts.withColumn('BotOrNot', lit(1))\n",
    "    clean_accounts = clean_accounts.withColumn('BotOrNot', lit(0))\n",
    "    \n",
    "    #combine clean and bot accounts data togather\n",
    "    combined_df = bot_accounts.union(clean_accounts)\n",
    "\n",
    "    # shuffle dataset\n",
    "    new_df = combined_df.orderBy(rand())\n",
    "\n",
    "    #remove 'userrname' columns from dataset\n",
    "    new_df = new_df.drop('username')\n",
    "    \n",
    "    feature_assembler = VectorAssembler(inputCols = feature_columns, outputCol = 'independent_features')\n",
    "    df_updated = feature_assembler.transform(new_df)\n",
    "\n",
    "    # keep only required features/columns\n",
    "    df_updated = df_updated.select('independent_features', 'BotOrNot')\n",
    "    \n",
    "    scaled_df = doDataScaling(df_updated, \"independent_features\", \"scaled_features\")\n",
    "    \n",
    "    # keep only necessary feature/column for ml model\n",
    "    XY = scaled_df.select('scaled_features', 'BotOrNot')\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = datasetSplit(XY)\n",
    "    \n",
    "    # DataFrame(column) --> nparray\n",
    "    X_train = to_nparray_list(X_train, 'scaled_features')\n",
    "    y_train = to_nparray_list(y_train, 'BotOrNot')\n",
    "    X_test = to_nparray_list(X_test, 'scaled_features')\n",
    "    y_test = to_nparray_list(y_test, 'BotOrNot')\n",
    "    \n",
    "    model = getDLModel()\n",
    "    \n",
    "    model.fit(X_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=20,\n",
    "          validation_data=(X_test, y_test))\n",
    "    \n",
    "    \n",
    "    score, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    \n",
    "    saveModel(model)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    app()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35870ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
