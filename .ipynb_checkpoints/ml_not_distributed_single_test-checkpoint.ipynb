{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08242a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Application\n",
      "dataset load successfully\n",
      "38\n",
      "38\n",
      "38\n",
      "38\n",
      "38\n",
      "38\n",
      "38\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Column 'norm_independent_features' does not exist. Did you mean one of the following? [independent_features, BotOrNot];\n'Project ['norm_independent_features]\n+- Project [BotOrNot#818, independent_features#2120]\n   +- Project [retweet_count#2042, reply_count#2043, favorite_count#2044, num_hashtags#2045, num_urls#2046, num_mentions#2047, BotOrNot#818, text_features[0]#2048, text_features[1]#2049, text_features[2]#2050, text_features[3]#2051, text_features[4]#2052, text_features[5]#2053, text_features[6]#2054, text_features[7]#2055, text_features[8]#2056, text_features[9]#2057, text_features[10]#2058, text_features[11]#2059, text_features[12]#2060, text_features[13]#2061, text_features[14]#2062, text_features[15]#2063, text_features[16]#2064, ... 16 more fields]\n      +- Project [coalesce(nanvl(retweet_count#919, cast(null as double)), cast(0.0 as double)) AS retweet_count#2042, coalesce(nanvl(reply_count#992, cast(null as double)), cast(0.0 as double)) AS reply_count#2043, coalesce(nanvl(favorite_count#1065, cast(null as double)), cast(0.0 as double)) AS favorite_count#2044, coalesce(nanvl(num_hashtags#1138, cast(null as double)), cast(0.0 as double)) AS num_hashtags#2045, coalesce(nanvl(num_urls#1211, cast(null as double)), cast(0.0 as double)) AS num_urls#2046, coalesce(nanvl(num_mentions#1284, cast(null as double)), cast(0.0 as double)) AS num_mentions#2047, BotOrNot#818, coalesce(nanvl(text_features[0]#1932, cast(null as double)), cast(0.0 as double)) AS text_features[0]#2048, coalesce(nanvl(text_features[1]#1933, cast(null as double)), cast(0.0 as double)) AS text_features[1]#2049, coalesce(nanvl(text_features[2]#1934, cast(null as double)), cast(0.0 as double)) AS text_features[2]#2050, coalesce(nanvl(text_features[3]#1935, cast(null as double)), cast(0.0 as double)) AS text_features[3]#2051, coalesce(nanvl(text_features[4]#1936, cast(null as double)), cast(0.0 as double)) AS text_features[4]#2052, coalesce(nanvl(text_features[5]#1937, cast(null as double)), cast(0.0 as double)) AS text_features[5]#2053, coalesce(nanvl(text_features[6]#1938, cast(null as double)), cast(0.0 as double)) AS text_features[6]#2054, coalesce(nanvl(text_features[7]#1939, cast(null as double)), cast(0.0 as double)) AS text_features[7]#2055, coalesce(nanvl(text_features[8]#1940, cast(null as double)), cast(0.0 as double)) AS text_features[8]#2056, coalesce(nanvl(text_features[9]#1941, cast(null as double)), cast(0.0 as double)) AS text_features[9]#2057, coalesce(nanvl(text_features[10]#1942, cast(null as double)), cast(0.0 as double)) AS text_features[10]#2058, coalesce(nanvl(text_features[11]#1943, cast(null as double)), cast(0.0 as double)) AS text_features[11]#2059, coalesce(nanvl(text_features[12]#1944, cast(null as double)), cast(0.0 as double)) AS text_features[12]#2060, coalesce(nanvl(text_features[13]#1945, cast(null as double)), cast(0.0 as double)) AS text_features[13]#2061, coalesce(nanvl(text_features[14]#1946, cast(null as double)), cast(0.0 as double)) AS text_features[14]#2062, coalesce(nanvl(text_features[15]#1947, cast(null as double)), cast(0.0 as double)) AS text_features[15]#2063, coalesce(nanvl(text_features[16]#1948, cast(null as double)), cast(0.0 as double)) AS text_features[16]#2064, ... 15 more fields]\n         +- Project [retweet_count#919, reply_count#992, favorite_count#1065, num_hashtags#1138, num_urls#1211, num_mentions#1284, BotOrNot#818, text_features#1911[0] AS text_features[0]#1932, text_features#1911[1] AS text_features[1]#1933, text_features#1911[2] AS text_features[2]#1934, text_features#1911[3] AS text_features[3]#1935, text_features#1911[4] AS text_features[4]#1936, text_features#1911[5] AS text_features[5]#1937, text_features#1911[6] AS text_features[6]#1938, text_features#1911[7] AS text_features[7]#1939, text_features#1911[8] AS text_features[8]#1940, text_features#1911[9] AS text_features[9]#1941, text_features#1911[10] AS text_features[10]#1942, text_features#1911[11] AS text_features[11]#1943, text_features#1911[12] AS text_features[12]#1944, text_features#1911[13] AS text_features[13]#1945, text_features#1911[14] AS text_features[14]#1946, text_features#1911[15] AS text_features[15]#1947, text_features#1911[16] AS text_features[16]#1948, ... 15 more fields]\n            +- Project [text#846, retweet_count#919, reply_count#992, favorite_count#1065, num_hashtags#1138, num_urls#1211, num_mentions#1284, BotOrNot#818, <lambda>(text#846)#1910 AS text_features#1911]\n               +- Project [text#846, retweet_count#919, reply_count#992, favorite_count#1065, num_hashtags#1138, num_urls#1211, cast(num_mentions#158 as double) AS num_mentions#1284, BotOrNot#818]\n                  +- Project [text#846, retweet_count#919, reply_count#992, favorite_count#1065, num_hashtags#1138, cast(num_urls#157 as double) AS num_urls#1211, num_mentions#158, BotOrNot#818]\n                     +- Project [text#846, retweet_count#919, reply_count#992, favorite_count#1065, cast(num_hashtags#156 as double) AS num_hashtags#1138, num_urls#157, num_mentions#158, BotOrNot#818]\n                        +- Project [text#846, retweet_count#919, reply_count#992, cast(favorite_count#518L as double) AS favorite_count#1065, num_hashtags#156, num_urls#157, num_mentions#158, BotOrNot#818]\n                           +- Project [text#846, retweet_count#919, cast(reply_count#151 as double) AS reply_count#992, favorite_count#518L, num_hashtags#156, num_urls#157, num_mentions#158, BotOrNot#818]\n                              +- Project [text#846, cast(retweet_count#150 as double) AS retweet_count#919, reply_count#151, favorite_count#518L, num_hashtags#156, num_urls#157, num_mentions#158, BotOrNot#818]\n                                 +- Project [<lambda>(text#139)#845 AS text#846, retweet_count#150, reply_count#151, favorite_count#518L, num_hashtags#156, num_urls#157, num_mentions#158, BotOrNot#818]\n                                    +- Project [text#139, retweet_count#150, reply_count#151, favorite_count#518L, num_hashtags#156, num_urls#157, num_mentions#158, BotOrNot#818]\n                                       +- Sort [_nondeterministic#844 ASC NULLS FIRST], true\n                                          +- Project [text#139, retweet_count#150, reply_count#151, favorite_count#518L, num_hashtags#156, num_urls#157, num_mentions#158, BotOrNot#818, rand(8033883464056718618) AS _nondeterministic#844]\n                                             +- Union false, false\n                                                :- Project [text#18 AS text#139, retweet_count#29 AS retweet_count#150, reply_count#30 AS reply_count#151, cast(favorite_count#31L as bigint) AS favorite_count#518L, num_hashtags#35 AS num_hashtags#156, num_urls#36 AS num_urls#157, num_mentions#37 AS num_mentions#158, 1 AS BotOrNot#818]\n                                                :  +- Sample 0.0, 0.8, false, 21\n                                                :     +- Sort [id#17L ASC NULLS FIRST, text#18 ASC NULLS FIRST, source#19 ASC NULLS FIRST, user_id#20 ASC NULLS FIRST, truncated#21 ASC NULLS FIRST, in_reply_to_status_id#22 ASC NULLS FIRST, in_reply_to_user_id#23 ASC NULLS FIRST, in_reply_to_screen_name#24 ASC NULLS FIRST, retweeted_status_id#25 ASC NULLS FIRST, geo#26 ASC NULLS FIRST, place#27 ASC NULLS FIRST, contributors#28 ASC NULLS FIRST, retweet_count#29 ASC NULLS FIRST, reply_count#30 ASC NULLS FIRST, favorite_count#31L ASC NULLS FIRST, favorited#32 ASC NULLS FIRST, retweeted#33 ASC NULLS FIRST, possibly_sensitive#34 ASC NULLS FIRST, num_hashtags#35 ASC NULLS FIRST, num_urls#36 ASC NULLS FIRST, num_mentions#37 ASC NULLS FIRST, created_at#38 ASC NULLS FIRST, timestamp#39 ASC NULLS FIRST, crawled_at#40 ASC NULLS FIRST, updated#41 ASC NULLS FIRST], false\n                                                :        +- GlobalLimit 20\n                                                :           +- LocalLimit 20\n                                                :              +- Relation [id#17L,text#18,source#19,user_id#20,truncated#21,in_reply_to_status_id#22,in_reply_to_user_id#23,in_reply_to_screen_name#24,retweeted_status_id#25,geo#26,place#27,contributors#28,retweet_count#29,reply_count#30,favorite_count#31L,favorited#32,retweeted#33,possibly_sensitive#34,num_hashtags#35,num_urls#36,num_mentions#37,created_at#38,timestamp#39,crawled_at#40,updated#41] csv\n                                                +- Project [RT @morningJewshow: Speaking about Jews and comedy tonight at Temple Emanu-El in San Francisco. In other words, my High Holidays.#85 AS text#266, _c12#96 AS retweet_count#277, 113#97 AS reply_count#278, cast(014#98 as bigint) AS favorite_count#596L, \\N18#102 AS num_hashtags#283, 019#103 AS num_urls#284, 020#104 AS num_mentions#285, 0 AS BotOrNot#827]\n                                                   +- Sample 0.0, 0.8, false, 21\n                                                      +- Sort [593932392663912449#84 ASC NULLS FIRST, RT @morningJewshow: Speaking about Jews and comedy tonight at Temple Emanu-El in San Francisco. In other words, my High Holidays.#85 ASC NULLS FIRST, <a href=\"http://tapbots.com/tweetbot\" rel=\"nofollow\">Tweetbot for iΟS</a>#86 ASC NULLS FIRST, 678033#87 ASC NULLS FIRST, _c4#88 ASC NULLS FIRST, 05#89 ASC NULLS FIRST, 06#90 ASC NULLS FIRST, _c7#91 ASC NULLS FIRST, 593932168524533760#92 ASC NULLS FIRST, \\N9#93 ASC NULLS FIRST, \\N10#94 ASC NULLS FIRST, _c11#95 ASC NULLS FIRST, _c12#96 ASC NULLS FIRST, 113#97 ASC NULLS FIRST, 014#98 ASC NULLS FIRST, 015#99 ASC NULLS FIRST, _c16#100 ASC NULLS FIRST, _c17#101 ASC NULLS FIRST, \\N18#102 ASC NULLS FIRST, 019#103 ASC NULLS FIRST, 020#104 ASC NULLS FIRST, 121#105 ASC NULLS FIRST, Fri May 01 00:18:11 +0000 2015#106 ASC NULLS FIRST, 2015-05-01 02:18:11#107 ASC NULLS FIRST, ... 2 more fields], false\n                                                         +- GlobalLimit 20\n                                                            +- LocalLimit 20\n                                                               +- Relation [593932392663912449#84,RT @morningJewshow: Speaking about Jews and comedy tonight at Temple Emanu-El in San Francisco. In other words, my High Holidays.#85,<a href=\"http://tapbots.com/tweetbot\" rel=\"nofollow\">Tweetbot for iΟS</a>#86,678033#87,_c4#88,05#89,06#90,_c7#91,593932168524533760#92,\\N9#93,\\N10#94,_c11#95,_c12#96,113#97,014#98,015#99,_c16#100,_c17#101,\\N18#102,019#103,020#104,121#105,Fri May 01 00:18:11 +0000 2015#106,2015-05-01 02:18:11#107,... 2 more fields] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 549>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    547\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] test loss: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(loss))\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 550\u001b[0m     \u001b[43mApplicationJob\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    551\u001b[0m     spark\u001b[38;5;241m.\u001b[39mstop()\n",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36mApplicationJob\u001b[1;34m()\u001b[0m\n\u001b[0;32m    528\u001b[0m train_tweets_df\u001b[38;5;241m.\u001b[39mpersist()\n\u001b[0;32m    529\u001b[0m test_tweets_df\u001b[38;5;241m.\u001b[39mpersist()\n\u001b[1;32m--> 531\u001b[0m train_X, train_Y \u001b[38;5;241m=\u001b[39m \u001b[43mto_nparray_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_tweets_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnorm_independent_features\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBotOrNot\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    532\u001b[0m test_X, test_Y \u001b[38;5;241m=\u001b[39m to_nparray_dataset(test_tweets_df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnorm_independent_features\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBotOrNot\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    534\u001b[0m train_X, train_Y \u001b[38;5;241m=\u001b[39m SMOTEENN(train_X, train_Y)\n",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36mto_nparray_dataset\u001b[1;34m(df, feature_column, target_column)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_nparray_dataset\u001b[39m(df, feature_column, target_column):\n\u001b[0;32m    412\u001b[0m \u001b[38;5;66;03m#     list(df.select('col_name').toPandas()['col_name']) \u001b[39;00m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;66;03m#     feature = list(df.select(feature_column).toPandas()[feature_column])\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;66;03m#     target = list(df.select(target_column).toPandas()[target_column])\u001b[39;00m\n\u001b[1;32m--> 415\u001b[0m     feature \u001b[38;5;241m=\u001b[39m [row[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_column\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtoLocalIterator())]\n\u001b[0;32m    416\u001b[0m     target \u001b[38;5;241m=\u001b[39m [row[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(df\u001b[38;5;241m.\u001b[39mselect(target_column)\u001b[38;5;241m.\u001b[39mtoLocalIterator())]\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(feature), np\u001b[38;5;241m.\u001b[39marray(target)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:2023\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m     \u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \n\u001b[0;32m   2005\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2021\u001b[0m \u001b[38;5;124;03m    [Row(name='Alice', age=12), Row(name='Bob', age=15)]\u001b[39;00m\n\u001b[0;32m   2022\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2023\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2024\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Column 'norm_independent_features' does not exist. Did you mean one of the following? [independent_features, BotOrNot];\n'Project ['norm_independent_features]\n+- Project [BotOrNot#818, independent_features#2120]\n   +- Project [retweet_count#2042, reply_count#2043, favorite_count#2044, num_hashtags#2045, num_urls#2046, num_mentions#2047, BotOrNot#818, text_features[0]#2048, text_features[1]#2049, text_features[2]#2050, text_features[3]#2051, text_features[4]#2052, text_features[5]#2053, text_features[6]#2054, text_features[7]#2055, text_features[8]#2056, text_features[9]#2057, text_features[10]#2058, text_features[11]#2059, text_features[12]#2060, text_features[13]#2061, text_features[14]#2062, text_features[15]#2063, text_features[16]#2064, ... 16 more fields]\n      +- Project [coalesce(nanvl(retweet_count#919, cast(null as double)), cast(0.0 as double)) AS retweet_count#2042, coalesce(nanvl(reply_count#992, cast(null as double)), cast(0.0 as double)) AS reply_count#2043, coalesce(nanvl(favorite_count#1065, cast(null as double)), cast(0.0 as double)) AS favorite_count#2044, coalesce(nanvl(num_hashtags#1138, cast(null as double)), cast(0.0 as double)) AS num_hashtags#2045, coalesce(nanvl(num_urls#1211, cast(null as double)), cast(0.0 as double)) AS num_urls#2046, coalesce(nanvl(num_mentions#1284, cast(null as double)), cast(0.0 as double)) AS num_mentions#2047, BotOrNot#818, coalesce(nanvl(text_features[0]#1932, cast(null as double)), cast(0.0 as double)) AS text_features[0]#2048, coalesce(nanvl(text_features[1]#1933, cast(null as double)), cast(0.0 as double)) AS text_features[1]#2049, coalesce(nanvl(text_features[2]#1934, cast(null as double)), cast(0.0 as double)) AS text_features[2]#2050, coalesce(nanvl(text_features[3]#1935, cast(null as double)), cast(0.0 as double)) AS text_features[3]#2051, coalesce(nanvl(text_features[4]#1936, cast(null as double)), cast(0.0 as double)) AS text_features[4]#2052, coalesce(nanvl(text_features[5]#1937, cast(null as double)), cast(0.0 as double)) AS text_features[5]#2053, coalesce(nanvl(text_features[6]#1938, cast(null as double)), cast(0.0 as double)) AS text_features[6]#2054, coalesce(nanvl(text_features[7]#1939, cast(null as double)), cast(0.0 as double)) AS text_features[7]#2055, coalesce(nanvl(text_features[8]#1940, cast(null as double)), cast(0.0 as double)) AS text_features[8]#2056, coalesce(nanvl(text_features[9]#1941, cast(null as double)), cast(0.0 as double)) AS text_features[9]#2057, coalesce(nanvl(text_features[10]#1942, cast(null as double)), cast(0.0 as double)) AS text_features[10]#2058, coalesce(nanvl(text_features[11]#1943, cast(null as double)), cast(0.0 as double)) AS text_features[11]#2059, coalesce(nanvl(text_features[12]#1944, cast(null as double)), cast(0.0 as double)) AS text_features[12]#2060, coalesce(nanvl(text_features[13]#1945, cast(null as double)), cast(0.0 as double)) AS text_features[13]#2061, coalesce(nanvl(text_features[14]#1946, cast(null as double)), cast(0.0 as double)) AS text_features[14]#2062, coalesce(nanvl(text_features[15]#1947, cast(null as double)), cast(0.0 as double)) AS text_features[15]#2063, coalesce(nanvl(text_features[16]#1948, cast(null as double)), cast(0.0 as double)) AS text_features[16]#2064, ... 15 more fields]\n         +- Project [retweet_count#919, reply_count#992, favorite_count#1065, num_hashtags#1138, num_urls#1211, num_mentions#1284, BotOrNot#818, text_features#1911[0] AS text_features[0]#1932, text_features#1911[1] AS text_features[1]#1933, text_features#1911[2] AS text_features[2]#1934, text_features#1911[3] AS text_features[3]#1935, text_features#1911[4] AS text_features[4]#1936, text_features#1911[5] AS text_features[5]#1937, text_features#1911[6] AS text_features[6]#1938, text_features#1911[7] AS text_features[7]#1939, text_features#1911[8] AS text_features[8]#1940, text_features#1911[9] AS text_features[9]#1941, text_features#1911[10] AS text_features[10]#1942, text_features#1911[11] AS text_features[11]#1943, text_features#1911[12] AS text_features[12]#1944, text_features#1911[13] AS text_features[13]#1945, text_features#1911[14] AS text_features[14]#1946, text_features#1911[15] AS text_features[15]#1947, text_features#1911[16] AS text_features[16]#1948, ... 15 more fields]\n            +- Project [text#846, retweet_count#919, reply_count#992, favorite_count#1065, num_hashtags#1138, num_urls#1211, num_mentions#1284, BotOrNot#818, <lambda>(text#846)#1910 AS text_features#1911]\n               +- Project [text#846, retweet_count#919, reply_count#992, favorite_count#1065, num_hashtags#1138, num_urls#1211, cast(num_mentions#158 as double) AS num_mentions#1284, BotOrNot#818]\n                  +- Project [text#846, retweet_count#919, reply_count#992, favorite_count#1065, num_hashtags#1138, cast(num_urls#157 as double) AS num_urls#1211, num_mentions#158, BotOrNot#818]\n                     +- Project [text#846, retweet_count#919, reply_count#992, favorite_count#1065, cast(num_hashtags#156 as double) AS num_hashtags#1138, num_urls#157, num_mentions#158, BotOrNot#818]\n                        +- Project [text#846, retweet_count#919, reply_count#992, cast(favorite_count#518L as double) AS favorite_count#1065, num_hashtags#156, num_urls#157, num_mentions#158, BotOrNot#818]\n                           +- Project [text#846, retweet_count#919, cast(reply_count#151 as double) AS reply_count#992, favorite_count#518L, num_hashtags#156, num_urls#157, num_mentions#158, BotOrNot#818]\n                              +- Project [text#846, cast(retweet_count#150 as double) AS retweet_count#919, reply_count#151, favorite_count#518L, num_hashtags#156, num_urls#157, num_mentions#158, BotOrNot#818]\n                                 +- Project [<lambda>(text#139)#845 AS text#846, retweet_count#150, reply_count#151, favorite_count#518L, num_hashtags#156, num_urls#157, num_mentions#158, BotOrNot#818]\n                                    +- Project [text#139, retweet_count#150, reply_count#151, favorite_count#518L, num_hashtags#156, num_urls#157, num_mentions#158, BotOrNot#818]\n                                       +- Sort [_nondeterministic#844 ASC NULLS FIRST], true\n                                          +- Project [text#139, retweet_count#150, reply_count#151, favorite_count#518L, num_hashtags#156, num_urls#157, num_mentions#158, BotOrNot#818, rand(8033883464056718618) AS _nondeterministic#844]\n                                             +- Union false, false\n                                                :- Project [text#18 AS text#139, retweet_count#29 AS retweet_count#150, reply_count#30 AS reply_count#151, cast(favorite_count#31L as bigint) AS favorite_count#518L, num_hashtags#35 AS num_hashtags#156, num_urls#36 AS num_urls#157, num_mentions#37 AS num_mentions#158, 1 AS BotOrNot#818]\n                                                :  +- Sample 0.0, 0.8, false, 21\n                                                :     +- Sort [id#17L ASC NULLS FIRST, text#18 ASC NULLS FIRST, source#19 ASC NULLS FIRST, user_id#20 ASC NULLS FIRST, truncated#21 ASC NULLS FIRST, in_reply_to_status_id#22 ASC NULLS FIRST, in_reply_to_user_id#23 ASC NULLS FIRST, in_reply_to_screen_name#24 ASC NULLS FIRST, retweeted_status_id#25 ASC NULLS FIRST, geo#26 ASC NULLS FIRST, place#27 ASC NULLS FIRST, contributors#28 ASC NULLS FIRST, retweet_count#29 ASC NULLS FIRST, reply_count#30 ASC NULLS FIRST, favorite_count#31L ASC NULLS FIRST, favorited#32 ASC NULLS FIRST, retweeted#33 ASC NULLS FIRST, possibly_sensitive#34 ASC NULLS FIRST, num_hashtags#35 ASC NULLS FIRST, num_urls#36 ASC NULLS FIRST, num_mentions#37 ASC NULLS FIRST, created_at#38 ASC NULLS FIRST, timestamp#39 ASC NULLS FIRST, crawled_at#40 ASC NULLS FIRST, updated#41 ASC NULLS FIRST], false\n                                                :        +- GlobalLimit 20\n                                                :           +- LocalLimit 20\n                                                :              +- Relation [id#17L,text#18,source#19,user_id#20,truncated#21,in_reply_to_status_id#22,in_reply_to_user_id#23,in_reply_to_screen_name#24,retweeted_status_id#25,geo#26,place#27,contributors#28,retweet_count#29,reply_count#30,favorite_count#31L,favorited#32,retweeted#33,possibly_sensitive#34,num_hashtags#35,num_urls#36,num_mentions#37,created_at#38,timestamp#39,crawled_at#40,updated#41] csv\n                                                +- Project [RT @morningJewshow: Speaking about Jews and comedy tonight at Temple Emanu-El in San Francisco. In other words, my High Holidays.#85 AS text#266, _c12#96 AS retweet_count#277, 113#97 AS reply_count#278, cast(014#98 as bigint) AS favorite_count#596L, \\N18#102 AS num_hashtags#283, 019#103 AS num_urls#284, 020#104 AS num_mentions#285, 0 AS BotOrNot#827]\n                                                   +- Sample 0.0, 0.8, false, 21\n                                                      +- Sort [593932392663912449#84 ASC NULLS FIRST, RT @morningJewshow: Speaking about Jews and comedy tonight at Temple Emanu-El in San Francisco. In other words, my High Holidays.#85 ASC NULLS FIRST, <a href=\"http://tapbots.com/tweetbot\" rel=\"nofollow\">Tweetbot for iΟS</a>#86 ASC NULLS FIRST, 678033#87 ASC NULLS FIRST, _c4#88 ASC NULLS FIRST, 05#89 ASC NULLS FIRST, 06#90 ASC NULLS FIRST, _c7#91 ASC NULLS FIRST, 593932168524533760#92 ASC NULLS FIRST, \\N9#93 ASC NULLS FIRST, \\N10#94 ASC NULLS FIRST, _c11#95 ASC NULLS FIRST, _c12#96 ASC NULLS FIRST, 113#97 ASC NULLS FIRST, 014#98 ASC NULLS FIRST, 015#99 ASC NULLS FIRST, _c16#100 ASC NULLS FIRST, _c17#101 ASC NULLS FIRST, \\N18#102 ASC NULLS FIRST, 019#103 ASC NULLS FIRST, 020#104 ASC NULLS FIRST, 121#105 ASC NULLS FIRST, Fri May 01 00:18:11 +0000 2015#106 ASC NULLS FIRST, 2015-05-01 02:18:11#107 ASC NULLS FIRST, ... 2 more fields], false\n                                                         +- GlobalLimit 20\n                                                            +- LocalLimit 20\n                                                               +- Relation [593932392663912449#84,RT @morningJewshow: Speaking about Jews and comedy tonight at Temple Emanu-El in San Francisco. In other words, my High Holidays.#85,<a href=\"http://tapbots.com/tweetbot\" rel=\"nofollow\">Tweetbot for iΟS</a>#86,678033#87,_c4#88,05#89,06#90,_c7#91,593932168524533760#92,\\N9#93,\\N10#94,_c11#95,_c12#96,113#97,014#98,015#99,_c16#100,_c17#101,\\N18#102,019#103,020#104,121#105,Fri May 01 00:18:11 +0000 2015#106,2015-05-01 02:18:11#107,... 2 more fields] csv\n"
     ]
    }
   ],
   "source": [
    "#libraries / dependencies\n",
    "import glob\n",
    "from preprocessor import api as tweet_preprocessor\n",
    "from storagehelper.HDFSHelper import HDFSHelper\n",
    "from storagehelper.LocalFSHelper import LocalFSHelper\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "from pyspark.sql.functions import col, udf, to_timestamp, lit, to_timestamp, when, rand\n",
    "from pyspark.sql.types import IntegerType, LongType, DoubleType, StringType, ArrayType\n",
    "from pyspark.ml.feature import Normalizer, StandardScaler, MinMaxScaler, VectorAssembler, Normalizer\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.accumulators import AccumulatorParam\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Dense, Input, concatenate, Dropout, Activation, BatchNormalization\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.losses import MeanSquaredError, CategoricalCrossentropy\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import sys\n",
    "import shutil # sutil.rmtree(dir_path)\n",
    "\n",
    "\n",
    "#thread depencency\n",
    "from pyspark import InheritableThread\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from threading import Semaphore\n",
    "import time\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# initialize the number of epochs to train for, batch size, and\n",
    "# initial learning rate\n",
    "EPOCHS = 10\n",
    "BS = 64\n",
    "INIT_LR = 1e-3\n",
    "\n",
    "mse = MeanSquaredError()\n",
    "cce = CategoricalCrossentropy()\n",
    "opt = SGD(learning_rate=INIT_LR) #Adam(learning_rate=INIT_LR)\n",
    "\n",
    "\n",
    "LOCAL_USERNAME = 'ubuntu'\n",
    "HADOOP_USERNAME = 'ubuntu'\n",
    "\n",
    "#user info\n",
    "SYSTEM_NAME = {'local':'home','server':'user'}\n",
    "USER_NAME = {'local':'ubuntu','server':'ubuntu'} # {'local':'hadoop','server':'hadoop'}\n",
    "WEIGHT_FOLDER_NAME = {'local':'weights_worker_id','server':'weights'}\n",
    "GRADIENT_FOLDER_NAME = {'local':'gradients_worker_id','server':'gradients'}\n",
    "FILE_NAME_PREF = {'gradients': 'gradient','weights': 'weight'}\n",
    "\n",
    "fs_semaphore = Semaphore()\n",
    "\n",
    "\n",
    "#Dataset location\n",
    "\n",
    "#Local\n",
    "bot_tweets_dataset_path = 'F://TwitterBotDataset//tweet_dataset_full//bot_tweets//' \n",
    "genuine_tweets_dataset_path = 'F://TwitterBotDataset//tweet_dataset_full//genuine_tweets//'\n",
    "\n",
    "#S3\n",
    "#bot_tweets_dataset_path = 's3://tweet-dataset/bot_tweets' #'F://TwitterBotDataset//tweet_dataset_small//bot_tweets//'\n",
    "#genuine_tweets_dataset_path = 's3://tweet-dataset/genuine_tweets' #'F://TwitterBotDataset//tweet_dataset_small//genuine_tweets//'\n",
    "\n",
    "#turn a line of text into d dimentional vector \n",
    "GLOVE_DIR = \"C://Users//USER//projects//\"\n",
    "\n",
    "#all columns\n",
    "BOT_COLUMNS = ['id','text','source','user_id','truncated','in_reply_to_status_id', \n",
    "               'in_reply_to_user_id','in_reply_to_screen_name', 'retweeted_status_id',\n",
    "               'geo','place','contributors','retweet_count', 'reply_count','favorite_count',\n",
    "               'favorited', 'retweeted','possibly_sensitive','num_hashtags','num_urls',\n",
    "               'num_mentions','created_at','timestamp','crawled_at', 'updated']\n",
    "\n",
    "GENUINE_COLUMNS = ['id','text','source','user_id','truncated','in_reply_to_status_id', \n",
    "                   'in_reply_to_user_id','in_reply_to_screen_name', 'retweeted_status_id',\n",
    "                   'geo','place','contributors','retweet_count', 'reply_count','favorite_count',\n",
    "                   'favorited', 'retweeted','possibly_sensitive','num_hashtags','num_urls',\n",
    "                   'num_mentions','REMOVE_IT', 'created_at','timestamp','crawled_at', 'updated',]\n",
    "\n",
    "#feature used for bot detection\n",
    "COLUMN_NAMES = ['text', 'retweet_count', 'reply_count', 'favorite_count',\n",
    "                'num_hashtags', 'num_urls', 'num_mentions']\n",
    "\n",
    "\n",
    "# #configure spark\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[4]\").setAppName(\"ml_account_ base_session\")\n",
    "conf.set(\"spark.executor.instances\", 2)\n",
    "conf.set(\"spark.executor.cores\", 1)\n",
    "conf.set(\"spark.executor.memory\", 871859200)\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# # # for spark-submit\n",
    "# spark = SparkSession.builder.appName('ml_account_ base_session').getOrCreate()\n",
    "# # spark\n",
    "# sc = spark.sparkContext\n",
    "\n",
    "# for local build\n",
    "# spark = SparkSession.builder.appName('ml_account_ base_session').getOrCreate()\n",
    "\n",
    "\n",
    "#for local multi thread\n",
    "\n",
    "# conf = SparkConf()\n",
    "# conf.setMaster(\"local[4]\").setAppName(\"distributed_training_session\")\n",
    "# conf.set(\"spark.scheduler.mode\", \"FAIR\")\n",
    "# sc = SparkContext(conf=conf)\n",
    "# spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "\n",
    "# read dataset from csv\n",
    "def read_dataset():\n",
    "    bot_tweets = spark.read.csv(bot_tweets_dataset_path, header = True, inferSchema = True).limit(20)\n",
    "    genuine_tweets = spark.read.csv(genuine_tweets_dataset_path, header = True, inferSchema = True).limit(20)\n",
    "    \n",
    "#     bot_tweets = bot_tweets.persist(StorageLevel.MEMORY_ONLY)\n",
    "#     genuine_tweets = genuine_tweets.persist(StorageLevel.MEMORY_ONLY)\n",
    "    return bot_tweets, genuine_tweets\n",
    "\n",
    "def set_column_name(df, column_names):\n",
    "    df = df.toDF(*column_names)\n",
    "    return df\n",
    "\n",
    "def remove_column_miss_match(df):\n",
    "    ## dataset have diffrent number of columns\n",
    "    ## column name of dataframe\n",
    "    column_name = [cname for cname, tp in df.dtypes]\n",
    "#     len(df.collect()), len(df.dtypes)\n",
    "    #column_name\n",
    "\n",
    "    #Number of column is diffrent for bot and genuine tweets data\n",
    "\n",
    "    #genuine_tweets_df = genuine_tweets_df.toDF(*column_name)\n",
    "    df = set_column_name(df, GENUINE_COLUMNS)\n",
    "#     print(len(df.collect()))\n",
    "    \n",
    "    df = df.drop('REMOVE_IT') # remove 5th column from end\n",
    "    #update column name according to \n",
    "    df = set_column_name(df, BOT_COLUMNS)\n",
    "#     print(len(df.collect()))\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_type_miss_match(df):\n",
    "    # Same column has diffrent data type. So make data type same for every column\n",
    "    updated_df = df.withColumn(\"id\",col(\"id\").cast(IntegerType())) \\\n",
    "                                    .withColumn(\"favorite_count\",col(\"favorite_count\").cast(LongType())) \\\n",
    "                                    .withColumn(\"favorited\",col(\"favorited\").cast(IntegerType()))\n",
    "    return updated_df\n",
    "\n",
    "\n",
    "def resize_combine_data(bot_tweets_df, genuine_tweets_df):\n",
    "    ## only keep the required column from the dataframe\n",
    "    bot_tweets_df = bot_tweets_df.select(*COLUMN_NAMES)\n",
    "    genuine_tweets_df = genuine_tweets_df.select(*COLUMN_NAMES)\n",
    "    \n",
    "#     print(len(bot_tweets_df.collect()), len(genuine_tweets_df.collect()))\n",
    "\n",
    "    ## add BotOrNot column\n",
    "    bot_tweets_df = bot_tweets_df.withColumn('BotOrNot', lit(1))\n",
    "    genuine_tweets_df = genuine_tweets_df.withColumn('BotOrNot', lit(0))\n",
    "\n",
    "    #combine clean and bot accounts data togather\n",
    "    tweets_df = bot_tweets_df.union(genuine_tweets_df)\n",
    "\n",
    "    # shuffle dataset\n",
    "    tweets_df = tweets_df.orderBy(rand())\n",
    "\n",
    "#     print(len(tweets_df.collect()))\n",
    "    \n",
    "    return tweets_df\n",
    "\n",
    "def removeExtraColumn(df, column_names):\n",
    "    if len(df.columns) == 26:\n",
    "        df = remove_column_miss_match(df)\n",
    "    else:\n",
    "        df = set_column_name(df, column_names)\n",
    "    \n",
    "    return df\n",
    "\n",
    "text_process_udf = udf(lambda x : tweet_preprocessor.tokenize(x), StringType())\n",
    "def preprocess_data(df):\n",
    "    df = df.withColumn('text', text_process_udf(df.text))\n",
    "    print(df.count())\n",
    "    df = df.withColumn(\"retweet_count\",col(\"retweet_count\").cast(DoubleType()))\n",
    "    print(df.count())\n",
    "    df = df.withColumn(\"reply_count\",col(\"reply_count\").cast(DoubleType()))\n",
    "    print(df.count())\n",
    "    df = df.withColumn(\"favorite_count\",col(\"favorite_count\").cast(DoubleType()))\n",
    "    print(df.count())\n",
    "    df = df.withColumn(\"num_hashtags\",col(\"num_hashtags\").cast(DoubleType()))\n",
    "    print(df.count())\n",
    "    df = df.withColumn(\"num_urls\",col(\"num_urls\").cast(DoubleType()))\n",
    "    print(df.count())\n",
    "    df = df.withColumn(\"num_mentions\",col(\"num_mentions\").cast(DoubleType()))\n",
    "    print(df.count())\n",
    "    return df\n",
    "\n",
    "def doDataScaling(df, input_column, output_column):\n",
    "    ## Make data standard\n",
    "    # https://spark.apache.org/docs/1.4.1/ml-features.html#standardscaler\n",
    "\n",
    "    scaler = StandardScaler(inputCol=input_column, outputCol=output_column,\n",
    "                            withStd=True, withMean=False)\n",
    "\n",
    "    # Compute summary statistics by fitting the StandardScaler\n",
    "    scalerModel = scaler.fit(df)\n",
    "\n",
    "    # Normalize each feature to have unit standard deviation.\n",
    "    scaled_df = scalerModel.transform(df)\n",
    "    \n",
    "    return scaled_df\n",
    "\n",
    "def doDataNormalization(df, input_column, output_column):\n",
    "    normalizer = Normalizer(p=2.0, inputCol=input_column, outputCol=output_column)\n",
    "    norm_df = normalizer.transform(df)\n",
    "    return norm_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def makeGloveWordEmbedder(glove_path):\n",
    "    embedding_dict = {}\n",
    "    with open(glove_path, 'r', encoding=\"utf-8\") as glove_file:\n",
    "        for line in glove_file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embedding_dict[word] = vector\n",
    "            \n",
    "    return embedding_dict  \n",
    "\n",
    "\n",
    "# # Test GLoVE result\n",
    "# glove_word2vec_embedder[\"google\"]\n",
    "\n",
    "#Ned to large data chech for performance\n",
    "glove_dict = makeGloveWordEmbedder(GLOVE_DIR + \"glove.twitter.27B.25d.txt\")\n",
    "\n",
    " ##create word embedding GLoVE model dictionary. Use pre trained model\n",
    "text_feature_dimention = 25\n",
    "# glove_word2vec_embedder = makeGloveWordEmbedder(GLOVE_DIR + \"glove.twitter.27B.25d.txt\")\n",
    "\n",
    "\n",
    "#Give a word and get that word representing feature vector of dimention 25 from embedding dictionary\n",
    "def word2vec(word_dict=None, word=None, dim=25):\n",
    "    default_vector = np.zeros(dim)\n",
    "    \n",
    "    if word_dict is None or word is None:\n",
    "        return default_vector\n",
    "    \n",
    "    word_vector = word_dict.get(word)\n",
    "    \n",
    "    if word_vector is None:\n",
    "        return default_vector\n",
    "    return word_vector\n",
    "\n",
    "# # test a word representing feature vector\n",
    "# word_vector = word2vec(glove_word2vec_embedder, \"tweet\", text_feature_dimention)\n",
    "# print(type(word_vector), word_vector)\n",
    "\n",
    "\n",
    "#----------------LSTM Model----------------------\n",
    "\n",
    "#create a 1 layer LSTM model\n",
    "#input dimention: 3d (1,7,25) input_sample = [[[1,...,25]],..,[1,...,25]]\n",
    "#output dimention: 1d (1,32) output_sample [[1,2,3,,,,32]]\n",
    "def lstm_model(output_dim):\n",
    "    model = LSTM(output_dim, return_sequences=False, return_state=False)\n",
    "    return model\n",
    "\n",
    "def reset_lstm_model(model):\n",
    "    model.resate_states() # stateful=True is required for reset states\n",
    "\n",
    "# create LSTM model of output dimention 32. Model output feature vector will be of 32 dimention vector\n",
    "lstm = lstm_model(32) \n",
    "\n",
    "# convver a sentence to a feature vector using LSTM(RNN) model\n",
    "def sent2vec(sent):\n",
    "    words = sent.split(' ')\n",
    "    word_vectors = np.array([])\n",
    "    count = 0;\n",
    "    for word in words:\n",
    "        word_vector = word2vec(glove_dict, word)\n",
    "#         print(\"word dim: {}\".format(len(word_vector)))\n",
    "        if word_vectors.size == 0:\n",
    "            word_vectors = np.array([word_vector])\n",
    "        else:\n",
    "            word_vectors = np.vstack([word_vectors, word_vector])\n",
    "        count = count + 1\n",
    "    \n",
    "#     print(\"Input feature vector shape before reshape(2D): {}\".format(word_vectors.shape))\n",
    "        \n",
    "    input_feature_vectors = np.reshape(word_vectors, (1, count, text_feature_dimention))\n",
    "#     print(\"Input feature vector shape after reshape(3d): {}\".format(input_feature_vectors.shape))\n",
    "#     print(\"LSTM requirs 3d shape inputs [batch, timesteps, feature]\")\n",
    "    output_vector = lstm(input_feature_vectors)\n",
    "#     lstm.reset_states() # stateful = True is required for reset\n",
    "\n",
    "#     print(\"result vector shape: {}\".format(output_vector.shape))\n",
    "#     print(\"Last input was: {}\".format(input_feature_vectors[0][-1]))\n",
    "#     print(\"output result: {}\".format(output_vector))\n",
    "    \n",
    "    # (tensore --> numpy 0bject --> numpy.array --> array/list/ArrayType)\n",
    "    return output_vector.numpy()[0].tolist() \n",
    "    \n",
    "## For Testing sentence to vector convertion\n",
    "# sent = \"Twitter is a large social media network\"\n",
    "# res_vector = sent2vec(sent)\n",
    "# type(res_vector), res_vector\n",
    "\n",
    "\n",
    "# text string --> vector 32 dimention\n",
    "sent_to_vector_udf = udf(lambda x : sent2vec(x), ArrayType(DoubleType()))\n",
    "def processTextColumn(df, column_name, new_column_name):\n",
    "    df = df.withColumn(new_column_name, sent_to_vector_udf(col(column_name)))\n",
    "    return df\n",
    "\n",
    "def sentEmbeddingGLoVE_LSTM(df):\n",
    "    \n",
    "    text_updated_column = 'text_features'\n",
    "    updated_df = processTextColumn(df, \"text\", text_updated_column)\n",
    "\n",
    "#     print(len(updated_df.collect()), type(updated_df), updated_df.printSchema()) \n",
    "    \n",
    "    return updated_df\n",
    "\n",
    "\n",
    "\n",
    "def assembleColumns(tweets_df):\n",
    "    columns = ['retweet_count', 'reply_count', 'favorite_count',\n",
    "               'num_hashtags' ,'num_urls', 'num_mentions', 'BotOrNot']\n",
    "\n",
    "    tweets_df = tweets_df.select(*columns, \n",
    "                          tweets_df.text_features[0], tweets_df.text_features[1], tweets_df.text_features[2],tweets_df.text_features[3], tweets_df.text_features[4],\n",
    "                          tweets_df.text_features[5], tweets_df.text_features[6], tweets_df.text_features[7],tweets_df.text_features[8], tweets_df.text_features[9], \n",
    "                          tweets_df.text_features[10], tweets_df.text_features[11], tweets_df.text_features[12],tweets_df.text_features[13], tweets_df.text_features[14],\n",
    "                          tweets_df.text_features[15], tweets_df.text_features[16], tweets_df.text_features[17],tweets_df.text_features[18], tweets_df.text_features[19],\n",
    "                          tweets_df.text_features[20], tweets_df.text_features[21], tweets_df.text_features[22],tweets_df.text_features[23], tweets_df.text_features[24],\n",
    "                          tweets_df.text_features[25], tweets_df.text_features[26], tweets_df.text_features[27],tweets_df.text_features[28], tweets_df.text_features[29],\n",
    "                          tweets_df.text_features[30], tweets_df.text_features[31])\n",
    "\n",
    "\n",
    "#     print(tweets_df.columns, len(tweets_df.collect()), tweets_df.printSchema())\n",
    "\n",
    "    #remove \n",
    "\n",
    "    feature_columns = ['retweet_count','reply_count','favorite_count','num_hashtags','num_urls','num_mentions',\n",
    "                       'text_features[0]','text_features[1]', 'text_features[2]','text_features[3]','text_features[4]',\n",
    "                       'text_features[5]','text_features[6]','text_features[7]', 'text_features[8]','text_features[9]',\n",
    "                       'text_features[10]','text_features[11]','text_features[12]','text_features[13]','text_features[14]',\n",
    "                       'text_features[15]','text_features[16]','text_features[17]','text_features[18]','text_features[19]',\n",
    "                       'text_features[20]','text_features[21]','text_features[22]', 'text_features[23]', 'text_features[24]',\n",
    "                       'text_features[25]','text_features[26]','text_features[27]', 'text_features[28]', 'text_features[29]',\n",
    "                       'text_features[30]','text_features[31]']\n",
    "\n",
    "\n",
    "    tweets_df = tweets_df.na.fill(value=0.0 ,subset= feature_columns)\n",
    "    feature_assembler = VectorAssembler(inputCols = feature_columns, outputCol = 'independent_features')\n",
    "\n",
    "    tweets_updated_df = feature_assembler.transform(tweets_df)\n",
    "\n",
    "    #check\n",
    "#     num = len(tweets_updated_df.collect())\n",
    "#     print(num, type(tweets_updated_df), tweets_updated_df.printSchema())\n",
    "\n",
    "    #remove unnecessary columns\n",
    "    tweets_updated_df = tweets_updated_df.drop(*feature_columns)\n",
    "    \n",
    "    return tweets_updated_df\n",
    "\n",
    "\n",
    "def to_nparray_list(df, column_name):\n",
    "    rows = df.select(column_name).collect()\n",
    "    lists = [x[column_name] for x in rows]\n",
    "    nparr = np.array(lists)\n",
    "    \n",
    "    return nparr\n",
    "\n",
    "def to_nparray_dataset(df, feature_column, target_column):\n",
    "#     list(df.select('col_name').toPandas()['col_name']) \n",
    "#     feature = list(df.select(feature_column).toPandas()[feature_column])\n",
    "#     target = list(df.select(target_column).toPandas()[target_column])\n",
    "    feature = [row[0] for row in list(df.select(feature_column).toLocalIterator())]\n",
    "    target = [row[0] for row in list(df.select(target_column).toLocalIterator())]\n",
    "        \n",
    "    return np.array(feature), np.array(target)    \n",
    "\n",
    "def partition_dataset(df):\n",
    "    train_df, test_df = df.randomSplit([0.80, 0.20])\n",
    "#     print(len(train_df.collect()), len(test_df.collect()))\n",
    "\n",
    "    # features --> 'BotOrNot'\n",
    "#     X_train = train_df.drop('BotOrNot')\n",
    "#     y_train = train_df.select('BotOrNot')\n",
    "#     X_test = test_df.drop('BotOrNot')\n",
    "#     y_test = test_df.select('BotOrNot')\n",
    "    \n",
    "\n",
    "    #checkpoint\n",
    "#     print(len(X_train.collect()), len(y_train.collect()))\n",
    "#     print(len(X_test.collect()), len(y_test.collect()))\n",
    "\n",
    "#     X_train = to_nparray_list(X_train, 'independent_features')\n",
    "#     y_train = to_nparray_list(y_train, 'BotOrNot')\n",
    "#     X_test = to_nparray_list(X_test, 'independent_features')\n",
    "#     y_test = to_nparray_list(y_test, 'BotOrNot')\n",
    "    train_df = train_df.cache()\n",
    "    test_df = test_df.cache()\n",
    "\n",
    "    train_X,  train_Y = to_nparray_dataset(train_df, 'independent_features', 'BotOrNot')\n",
    "    test_X, test_Y = to_nparray_dataset(test_df, 'independent_features', 'BotOrNot')\n",
    "\n",
    "    \n",
    "    return train_X, train_Y, test_X, test_Y # return type: numpy.array\n",
    "\n",
    "\n",
    "def getTrainTestData(df, seed = 21):\n",
    "    train_X, test_X = df.randomSplit([0.7, 0.3], seed)\n",
    "    return train_X, test_X\n",
    "    \n",
    "def SMOTEENN(X, Y):\n",
    "    s = SMOTE()\n",
    "    s_x, s_y = s.fit_resample(X, Y)\n",
    "\n",
    "    enn = EditedNearestNeighbours()\n",
    "    e_x, e_y = enn.fit_resample(s_x, s_y)\n",
    "\n",
    "    return e_x, e_y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## create model\n",
    "def get2DenseLayeredModel(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(500, input_dim=input_dim))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(200))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def model_evaluation(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train,\n",
    "              batch_size=64,\n",
    "              epochs=10,\n",
    "              validation_data=(X_test, y_test))\n",
    "    score, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    \n",
    "\n",
    "    \n",
    "def ApplicationJob():\n",
    "\n",
    "    print('Run Application')\n",
    "    bot_tweets_df, genuine_tweets_df = read_dataset()\n",
    "    print('dataset load successfully')\n",
    "\n",
    "\n",
    "    train_bot_tweet_df, test_bot_tweet_df = bot_tweets_df.randomSplit([0.8, 0.2], seed = 21)\n",
    "    train_genuine_tweets_df, test_genuine_tweets_df = genuine_tweets_df.randomSplit([0.8, 0.2], seed = 21)\n",
    "\n",
    "    #solve column number issue\n",
    "    train_bot_tweet_df = removeExtraColumn(train_bot_tweet_df, BOT_COLUMNS)\n",
    "    train_genuine_tweets_df = removeExtraColumn(train_genuine_tweets_df, BOT_COLUMNS)\n",
    "\n",
    "    test_bot_tweet_df = removeExtraColumn(test_bot_tweet_df, BOT_COLUMNS)\n",
    "    test_genuine_tweets_df = removeExtraColumn(test_genuine_tweets_df, BOT_COLUMNS)\n",
    "\n",
    "    train_bot_tweet_df = remove_type_miss_match(train_bot_tweet_df)\n",
    "    train_genuine_tweets_df = remove_type_miss_match(train_genuine_tweets_df)\n",
    "    test_bot_tweet_df = remove_type_miss_match(test_bot_tweet_df)\n",
    "    test_genuine_tweets_df = remove_type_miss_match(test_genuine_tweets_df)\n",
    "\n",
    "    train_tweets_df = resize_combine_data(train_bot_tweet_df, train_genuine_tweets_df)\n",
    "    train_tweets_df = preprocess_data(train_tweets_df)\n",
    "\n",
    "    test_tweets_df = resize_combine_data(test_bot_tweet_df, test_genuine_tweets_df)\n",
    "    test_tweets_df = preprocess_data(test_tweets_df)\n",
    "\n",
    "    ##text embedding using GLoVE & LSTM\n",
    "    ## Word Embedding\n",
    "    train_tweets_df = sentEmbeddingGLoVE_LSTM(train_tweets_df)\n",
    "    test_tweets_df = sentEmbeddingGLoVE_LSTM(test_tweets_df)\n",
    "\n",
    "    ## Assable multiple colu,ms to create feature vector\n",
    "    train_tweets_df = assembleColumns(train_tweets_df)\n",
    "    test_tweets_df = assembleColumns(test_tweets_df)\n",
    "\n",
    "    train_tweets_df.persist()\n",
    "    test_tweets_df.persist()\n",
    "\n",
    "    train_tweets_df = doDataScaling(train_tweets_df, \"independent_features\", \"scaled_independent_features\").drop(\"independent_features\")\n",
    "    train_tweets_df = doDataNormalization(train_tweets_df, \"scaled_independent_features\", \"norm_independent_features\").drop(\"scaled_independent_features\")\n",
    "\n",
    "    test_tweets_df = doDataScaling(test_tweets_df, \"independent_features\", \"scaled_independent_features\").drop(\"independent_features\")\n",
    "    test_tweets_df = doDataNormalization(test_tweets_df, \"scaled_independent_features\", \"norm_independent_features\").drop(\"scaled_independent_features\")\n",
    "\n",
    "    train_X, train_Y = to_nparray_dataset(train_tweets_df, \"norm_independent_features\", 'BotOrNot')\n",
    "    test_X, test_Y = to_nparray_dataset(test_tweets_df, \"norm_independent_features\", 'BotOrNot')\n",
    "\n",
    "    train_X, train_Y = SMOTEENN(train_X, train_Y)\n",
    "\n",
    "    model = get2DenseLayeredModel(38)\n",
    "\n",
    "    # in order to calculate accuracy using Keras' functions we first need\n",
    "    # to compile the model\n",
    "    model.fit(train_X, train_Y, epochs=20)\n",
    "    model.compile(optimizer= opt, loss=cce, metrics=[\"acc\"])\n",
    "\n",
    "\n",
    "    # now that the model is compiled we can compute the accuracy\n",
    "    (loss, acc) = model.evaluate(test_X, test_Y)\n",
    "    print(\"[INFO] test accuracy: {}\".format(acc))\n",
    "    print(\"[INFO] test loss: {}\".format(loss))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ApplicationJob()\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6c324f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
