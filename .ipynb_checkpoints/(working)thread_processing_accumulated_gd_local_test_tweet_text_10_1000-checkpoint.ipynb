{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef9e63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#bot_tweets: 105 #gen_tweets: 105\n",
      "Input count: 210, 210\n",
      "Curr gd: [<tf.Tensor: shape=(38, 500), dtype=float32, numpy=\n",
      "array([[ 0.0000000e+00, -1.1687484e-06,  0.0000000e+00, ...,\n",
      "        -3.6305224e-07,  0.0000000e+00,  1.6548127e-07],\n",
      "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
      "       [ 0.0000000e+00, -1.1311359e-06,  0.0000000e+00, ...,\n",
      "        -9.6262352e-07,  8.0354221e-07,  0.0000000e+00],\n",
      "       ...,\n",
      "       [-1.9126048e-07,  4.3795867e-06, -7.4880290e-08, ...,\n",
      "         1.0497497e-06, -5.8225637e-06, -1.1717495e-06],\n",
      "       [-1.9072915e-07,  5.9109561e-07, -4.3454683e-08, ...,\n",
      "         2.2985587e-06, -3.3951014e-06,  1.0577437e-06],\n",
      "       [-2.2686422e-09, -2.6719797e-06, -4.4141423e-08, ...,\n",
      "         5.9597397e-07, -1.6506146e-07, -1.8497883e-07]], dtype=float32)>, <tf.Tensor: shape=(500,), dtype=float32, numpy=\n",
      "array([ 3.10155974e-06, -1.16721128e-06,  2.59644025e-07,  1.94834479e-06,\n",
      "        1.24073762e-04, -1.67848426e-04, -8.97357168e-05, -1.03051434e-05,\n",
      "        1.73207889e-06,  2.19610661e-06,  7.99362169e-06, -1.71809447e-08,\n",
      "       -5.69083059e-05, -5.50737204e-05,  1.09224049e-04, -6.28077541e-05,\n",
      "        2.38965949e-05,  2.76405126e-06,  8.80723928e-06, -4.58537727e-07,\n",
      "        1.88008971e-06,  1.37031398e-04, -1.44237220e-06, -9.03631098e-06,\n",
      "       -2.07413314e-06,  3.36187122e-06,  6.19649791e-06,  3.79672929e-06,\n",
      "        3.38246628e-06,  9.44805288e-06, -2.45418573e-06, -2.21878981e-05,\n",
      "        1.50535561e-04, -3.73707985e-06,  5.47362242e-06,  3.46286856e-06,\n",
      "        2.57761159e-04, -1.29819282e-05,  6.34203661e-06,  1.85261215e-05,\n",
      "       -1.07500400e-05, -3.42708736e-05,  1.97471265e-04,  3.55183613e-04,\n",
      "        0.00000000e+00,  2.65275361e-04, -1.68106621e-04,  4.59768416e-05,\n",
      "       -2.36858327e-07,  0.00000000e+00,  4.45759833e-05, -8.91639429e-05,\n",
      "       -8.59783977e-05, -2.18497225e-04, -9.32375860e-06,  1.39637896e-05,\n",
      "        5.33122475e-05,  4.61741543e-08,  1.94100110e-04, -2.43479415e-04,\n",
      "        4.35075490e-04, -8.09235571e-05,  2.11090752e-04, -2.31305057e-06,\n",
      "       -4.95898894e-06, -8.31939760e-05,  6.02280579e-06,  1.65987920e-04,\n",
      "        1.45665195e-04,  1.11506861e-05, -1.25804843e-04,  5.15262082e-06,\n",
      "       -5.22050468e-05,  1.58096314e-04,  1.19214826e-06,  1.74657615e-07,\n",
      "        8.26872856e-05, -8.98673679e-05, -1.36622810e-04,  1.09768844e-05,\n",
      "        2.84740127e-05,  2.85653630e-04,  3.17051629e-09, -7.56763711e-07,\n",
      "        6.29129790e-05, -4.42050441e-06,  3.29264253e-06,  9.10257586e-05,\n",
      "        1.08238837e-05, -4.45945225e-05, -1.90962062e-04, -1.63230688e-05,\n",
      "        1.20544496e-06, -8.00714497e-06, -2.72264879e-04,  2.43591392e-04,\n",
      "       -6.19887896e-06, -3.80424353e-07,  2.40468507e-05, -7.71284758e-05,\n",
      "       -2.25595577e-04, -5.27529664e-06,  0.00000000e+00,  8.05958771e-05,\n",
      "       -8.03677831e-05,  2.35843936e-06,  2.67004245e-04, -8.99273873e-05,\n",
      "       -1.38433279e-05,  0.00000000e+00,  3.95652169e-04,  3.03288175e-06,\n",
      "       -3.19579470e-07,  3.30842198e-07, -3.86918373e-06,  7.65514983e-07,\n",
      "        1.51355838e-04,  9.76807587e-06, -4.22767835e-06,  0.00000000e+00,\n",
      "        5.89819647e-05,  2.05524043e-06,  1.92482927e-04,  1.31701177e-04,\n",
      "       -1.77217207e-05, -3.19765547e-08,  2.89128238e-05,  8.77579225e-07,\n",
      "        1.74864169e-07,  2.31034337e-05, -5.89185220e-06, -1.65690799e-05,\n",
      "        6.06697722e-05, -3.52622628e-05,  1.86065563e-07,  0.00000000e+00,\n",
      "        2.69171342e-05,  5.23556673e-06, -9.99099357e-05,  3.06887864e-06,\n",
      "       -7.63126081e-05,  1.75154986e-04, -5.41313557e-06,  2.24798896e-05,\n",
      "        8.50762376e-07, -2.54689917e-06, -2.76722949e-06, -1.83664757e-04,\n",
      "        9.75448756e-07, -1.71595631e-04, -1.31219656e-06, -1.14064278e-06,\n",
      "        4.21660407e-05,  1.68375089e-04,  2.28703502e-04,  9.64747983e-07,\n",
      "       -1.05562831e-04, -5.61338720e-06,  1.56392125e-04,  4.09059203e-06,\n",
      "        4.55593181e-07,  9.26173834e-06,  0.00000000e+00,  3.32310265e-06,\n",
      "       -5.77086837e-07,  1.70710831e-04, -8.42913068e-05,  1.43432510e-06,\n",
      "        4.41830343e-05,  1.56795446e-04, -2.27421115e-05,  1.59172896e-06,\n",
      "        8.20089190e-06, -1.77077090e-05, -9.01935127e-05,  2.75605680e-06,\n",
      "        0.00000000e+00, -1.06884145e-04, -1.41923988e-06, -1.84218698e-06,\n",
      "        2.21406197e-04,  4.86616227e-05,  1.49139441e-05, -1.01484249e-04,\n",
      "       -5.05755906e-06, -1.82911535e-05,  8.30572972e-05,  1.33132489e-04,\n",
      "        1.62917378e-04,  6.30401973e-06,  0.00000000e+00,  1.17355448e-05,\n",
      "        3.52799892e-04,  1.54562113e-05,  0.00000000e+00, -9.09313094e-05,\n",
      "       -2.37249278e-05, -7.21080369e-06,  8.31713464e-07,  1.00786801e-05,\n",
      "       -5.98748920e-05, -5.84880763e-05, -1.21031298e-04,  1.11494801e-05,\n",
      "        4.42378223e-08,  2.53123344e-05, -1.86533964e-06, -1.46056618e-06,\n",
      "        1.35972179e-04,  1.04631745e-05,  2.03595414e-06,  1.27627922e-04,\n",
      "       -4.52928434e-05,  2.98144732e-04, -1.91516938e-06, -2.31220711e-05,\n",
      "        2.65809853e-04, -9.13270287e-06,  9.25722532e-07, -1.65929407e-04,\n",
      "        6.14328064e-06,  7.77364403e-06,  2.34160761e-04,  1.47119420e-06,\n",
      "       -9.22756044e-07, -1.62248932e-06, -3.45652370e-06,  0.00000000e+00,\n",
      "       -1.34844886e-05, -1.99450486e-04,  7.49631272e-06, -2.01961549e-04,\n",
      "        2.38171546e-04, -1.08094146e-05, -3.23780853e-07, -1.16671479e-04,\n",
      "        1.26829684e-06,  6.51603768e-05, -1.04197978e-04, -1.32364206e-04,\n",
      "        1.59972231e-04,  3.13838200e-07, -1.03405955e-05, -9.70625406e-05,\n",
      "        1.00957048e-04,  2.56581377e-04,  3.94129813e-07,  2.00758914e-05,\n",
      "        9.10749441e-06, -2.97500555e-05,  4.93705966e-06, -4.32084562e-07,\n",
      "       -1.39004042e-04,  2.16503831e-04,  1.74294451e-06, -1.54788024e-04,\n",
      "        2.37646327e-05, -4.60006413e-06,  6.49063850e-06,  0.00000000e+00,\n",
      "        6.29058632e-05, -7.44876743e-05,  4.76127207e-05, -3.37557981e-06,\n",
      "        2.57756255e-05,  1.17895515e-05,  1.75926747e-04,  5.81181848e-05,\n",
      "       -1.26540908e-05, -8.64131653e-05, -3.98935072e-06,  9.27801102e-06,\n",
      "        2.84618613e-06, -7.85918674e-05,  2.49279819e-05, -5.28805322e-06,\n",
      "        0.00000000e+00,  4.66448967e-07,  0.00000000e+00,  1.19641811e-07,\n",
      "       -3.08366398e-05,  0.00000000e+00,  4.20676059e-07,  1.05826992e-04,\n",
      "        6.28363996e-05,  5.61219053e-07, -2.12654044e-07, -1.68957893e-04,\n",
      "        8.90964031e-08,  3.63325421e-06, -2.19764615e-06,  2.21037189e-04,\n",
      "        4.84775637e-06, -7.97368216e-09, -9.70621113e-05, -2.42697046e-04,\n",
      "       -3.93644939e-07,  2.14961547e-04, -1.73538829e-05,  7.46766489e-07,\n",
      "        2.38959779e-04, -3.25684523e-05, -2.13743042e-04,  1.35444670e-05,\n",
      "        0.00000000e+00,  2.58530963e-06, -6.89654335e-05,  1.09802015e-04,\n",
      "       -8.08445407e-07,  1.52923494e-05,  8.24000017e-05, -9.79942779e-05,\n",
      "        8.01444694e-05,  6.01914280e-06,  2.92988443e-05,  7.04999366e-07,\n",
      "       -3.48227331e-05, -1.88353035e-04, -4.45874844e-04,  1.23461898e-04,\n",
      "       -2.52046244e-04,  3.10458563e-05, -1.65637687e-06,  1.87608407e-06,\n",
      "        4.65737094e-06, -2.20048678e-04, -2.65994868e-06,  1.08202803e-05,\n",
      "        0.00000000e+00, -4.87598800e-06,  8.16039028e-06, -1.22852780e-05,\n",
      "        0.00000000e+00, -5.20944886e-05, -1.84824194e-06, -1.28921438e-05,\n",
      "        6.22305492e-07,  2.03355648e-06, -3.17700142e-06,  3.38324594e-06,\n",
      "       -1.21980862e-04, -9.00941959e-05, -1.63452336e-04,  1.76952756e-06,\n",
      "       -2.92943560e-06, -1.94549109e-04, -3.29589375e-06,  0.00000000e+00,\n",
      "       -2.72722136e-05, -4.57116694e-05,  1.72424029e-06, -2.34705294e-04,\n",
      "       -2.85531496e-06,  7.54894572e-05,  1.59948615e-06,  4.66262209e-05,\n",
      "        1.54593090e-05,  3.30538605e-05, -2.12554660e-05,  2.01082585e-05,\n",
      "        8.16108004e-05, -7.08902462e-07,  3.12579664e-06,  1.07045504e-04,\n",
      "        1.53442932e-04,  1.33297317e-05,  3.37260106e-04,  2.33817400e-06,\n",
      "        8.62285560e-06,  3.94194473e-07, -1.60617292e-05,  1.59043717e-04,\n",
      "       -1.36225553e-05,  3.49743846e-06,  6.49716385e-05,  7.19580839e-06,\n",
      "       -8.94666664e-05, -1.91364761e-05, -6.77805110e-06,  7.33110064e-05,\n",
      "        2.50084704e-04, -8.92449589e-06,  1.20128561e-05,  2.48395986e-06,\n",
      "        7.07623985e-06, -1.00601821e-04, -2.83459030e-05,  6.27338522e-05,\n",
      "        1.59394796e-04,  6.28468706e-06,  5.39635323e-07,  1.99457475e-07,\n",
      "        1.91919593e-04,  1.61507214e-05,  1.60114644e-06,  2.91451420e-06,\n",
      "        2.53378630e-07, -4.63601573e-05, -3.63293861e-06, -5.79018115e-06,\n",
      "        1.50324222e-05, -1.50294596e-04, -1.86894948e-04, -2.42923688e-05,\n",
      "       -1.14958908e-04,  1.77333532e-05, -1.57045142e-04,  1.30864546e-05,\n",
      "       -1.73181310e-04, -2.83553469e-04, -1.29514941e-04, -9.95956943e-05,\n",
      "       -2.56529074e-05,  6.06496396e-05, -1.49455836e-05,  3.78145982e-04,\n",
      "       -9.73612405e-05,  4.44704983e-06, -1.01289785e-04,  1.61231335e-04,\n",
      "        3.15157354e-06,  9.02085276e-07, -8.65508482e-05,  1.83175420e-04,\n",
      "        0.00000000e+00, -3.91774165e-06,  7.40674368e-05, -7.69433173e-05,\n",
      "       -2.68967074e-06,  1.37076777e-05,  7.89366750e-05,  1.31821153e-05,\n",
      "       -3.10975565e-06,  9.06774003e-05,  8.65273307e-07,  7.21131244e-08,\n",
      "        4.12733607e-05, -7.20654862e-05,  4.13470025e-06,  1.96666937e-04,\n",
      "       -2.17712028e-07, -1.53899200e-05,  7.42645625e-07, -2.68642820e-04,\n",
      "       -2.25717922e-05,  0.00000000e+00, -3.76722746e-05,  1.10532477e-04,\n",
      "       -1.63573262e-04,  1.66053542e-06, -3.08789072e-06, -3.11978442e-06,\n",
      "        3.53772037e-07, -2.26311522e-06,  4.14964063e-07,  3.96063115e-06,\n",
      "        1.00876190e-04,  1.37300958e-05,  1.10300052e-05, -1.87138221e-06,\n",
      "        0.00000000e+00, -1.80886593e-04, -1.05330335e-04,  9.67998276e-05,\n",
      "        3.04192381e-05,  1.52981477e-06, -1.38448013e-05, -1.91609925e-05,\n",
      "        2.68226249e-05,  9.58003238e-06,  5.62426794e-06, -1.02244616e-04,\n",
      "       -7.92839783e-05, -3.39986968e-07, -4.80648850e-06,  2.34667634e-04,\n",
      "       -4.73240252e-05,  6.34190229e-08, -8.80785308e-07,  8.46137937e-07,\n",
      "        2.35813903e-04,  6.84978440e-05,  1.00809011e-05,  1.88719582e-06,\n",
      "        1.55967893e-04, -1.35704277e-05, -2.82593810e-05, -6.93501761e-07,\n",
      "       -9.80274990e-07,  3.09349649e-04,  1.62597717e-05,  1.85658541e-06,\n",
      "       -7.99459201e-07, -5.48419621e-06, -3.93832011e-07, -2.85939954e-04,\n",
      "       -2.28737423e-04,  6.81318706e-05, -2.15812674e-04, -1.73698500e-04],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(500, 200), dtype=float32, numpy=\n",
      "array([[ 3.3847172e-07,  0.0000000e+00,  3.5237193e-07, ...,\n",
      "         0.0000000e+00, -3.9255326e-07,  5.1101176e-07],\n",
      "       [-3.1132396e-07,  0.0000000e+00, -7.2403509e-07, ...,\n",
      "        -1.2932681e-07,  1.1047454e-06, -1.5938645e-06],\n",
      "       [ 2.7593785e-09,  0.0000000e+00, -6.5540644e-09, ...,\n",
      "         0.0000000e+00,  9.1549790e-09,  0.0000000e+00],\n",
      "       ...,\n",
      "       [-1.3452058e-05,  0.0000000e+00, -1.0452678e-05, ...,\n",
      "        -1.3773816e-07,  1.6103339e-05, -2.1229214e-05],\n",
      "       [-2.7453111e-06,  0.0000000e+00, -2.5328218e-06, ...,\n",
      "        -8.9119823e-11,  3.9575752e-06, -5.2060923e-06],\n",
      "       [-4.6220375e-06,  0.0000000e+00, -4.3388327e-06, ...,\n",
      "         0.0000000e+00,  6.2034978e-06, -8.4600015e-06]], dtype=float32)>, <tf.Tensor: shape=(200,), dtype=float32, numpy=\n",
      "array([-2.61300767e-04,  0.00000000e+00, -2.10060374e-04,  2.78354273e-04,\n",
      "        3.08206290e-05,  3.54969925e-05,  3.53976851e-04,  0.00000000e+00,\n",
      "        1.75397305e-04, -9.16265526e-06, -3.05303620e-05,  4.30228654e-04,\n",
      "        4.20570665e-04,  1.74616725e-04, -7.37815071e-06,  4.76680725e-06,\n",
      "       -4.18206881e-04,  2.84710404e-04, -2.17148074e-04, -2.03195232e-05,\n",
      "        2.92507611e-05,  1.23756854e-06, -3.05458962e-04, -4.43551562e-06,\n",
      "       -3.28803260e-04, -8.63298374e-06,  5.11235294e-06,  0.00000000e+00,\n",
      "        3.54962191e-04,  0.00000000e+00,  3.75091971e-04, -1.12052636e-04,\n",
      "       -3.22084059e-04,  0.00000000e+00, -2.44881958e-04,  3.66838212e-04,\n",
      "        1.41854471e-04,  1.84089586e-04, -2.23300831e-05,  2.23304654e-04,\n",
      "        3.02097877e-04,  1.64049634e-04, -7.01054705e-06,  3.67736240e-04,\n",
      "       -7.13022928e-07, -4.58231341e-04, -4.83177893e-04, -1.98914245e-06,\n",
      "       -2.48387238e-07,  2.34694511e-04,  3.00832762e-04, -1.50277601e-05,\n",
      "        1.05294464e-04, -1.61525350e-05, -7.39098541e-05,  3.36022757e-04,\n",
      "        2.12511630e-04,  0.00000000e+00,  2.31707483e-04,  6.95360650e-05,\n",
      "       -1.41303593e-04, -1.29492423e-06,  3.94005610e-06,  2.10259877e-05,\n",
      "        5.69936219e-06,  3.60479362e-06, -9.03024466e-06,  3.89126808e-06,\n",
      "       -1.39896883e-05,  2.64851860e-06,  0.00000000e+00, -2.51876895e-06,\n",
      "        0.00000000e+00,  2.34946739e-07, -1.83580869e-05, -1.22368701e-05,\n",
      "       -4.19827789e-04,  1.64626719e-04,  1.51982662e-04, -1.42547753e-04,\n",
      "        7.68773389e-05, -1.26140476e-05,  9.04306726e-05, -5.06984406e-06,\n",
      "       -1.01822943e-05, -2.49546792e-05,  0.00000000e+00,  9.98723499e-06,\n",
      "        1.81659329e-04, -1.29571490e-04, -4.13851725e-04,  1.97506711e-06,\n",
      "        3.57228593e-04, -1.62625201e-05,  4.62669013e-05,  4.15264774e-04,\n",
      "       -1.23027812e-05, -3.39540602e-05,  0.00000000e+00,  3.42187501e-04,\n",
      "        0.00000000e+00,  2.98885483e-04,  7.34086768e-07,  2.49838922e-06,\n",
      "       -4.84773882e-05, -3.45342210e-04,  3.03427340e-04, -3.84444313e-04,\n",
      "       -4.34110232e-04,  7.26021926e-06,  2.47984048e-04,  0.00000000e+00,\n",
      "       -9.68101813e-06, -2.59676062e-06,  3.24550674e-05, -3.09001131e-04,\n",
      "       -2.89491640e-04,  1.07572814e-05, -1.69108159e-06,  0.00000000e+00,\n",
      "        0.00000000e+00,  2.56641888e-05,  2.92476907e-04,  8.97207610e-06,\n",
      "       -2.44753710e-06,  2.32797363e-04, -3.30402656e-07, -2.35396496e-04,\n",
      "       -3.89661029e-04,  1.50219716e-06, -2.97069960e-07, -9.84585654e-07,\n",
      "        0.00000000e+00,  2.89782096e-04, -4.87516008e-05, -4.76615205e-05,\n",
      "        0.00000000e+00,  1.05763800e-04, -8.74123486e-07,  3.38374339e-05,\n",
      "       -2.73502974e-06,  1.39058284e-05,  2.49476871e-05,  3.51446826e-04,\n",
      "       -1.48970758e-09, -4.65878117e-07, -1.51104850e-05,  1.49630709e-04,\n",
      "        9.18887508e-06,  5.31201658e-04, -3.92486891e-06, -2.62328511e-04,\n",
      "        1.58464172e-05,  6.53492452e-06, -1.65350921e-05,  1.67865255e-05,\n",
      "        9.56058138e-05,  1.79960771e-05,  4.34596703e-04,  2.34357896e-04,\n",
      "        2.94372847e-04, -1.32795132e-04, -3.47218975e-06, -2.65574025e-04,\n",
      "       -1.06071593e-05,  4.57920896e-06, -6.34337339e-05, -1.10065730e-05,\n",
      "       -1.50188702e-04, -2.22987837e-05,  1.51588995e-06, -4.95302120e-05,\n",
      "        9.21570290e-07,  3.42682906e-04,  1.20554851e-05, -1.19003125e-05,\n",
      "        6.45063628e-05,  2.87952425e-04, -5.48328626e-06, -1.47632977e-06,\n",
      "        3.07436916e-04,  4.27736639e-04,  3.59568985e-05,  3.16971746e-05,\n",
      "        1.49934887e-04,  4.76084807e-07,  0.00000000e+00, -4.27652500e-04,\n",
      "        2.89495685e-04,  1.45103077e-05,  3.81638587e-04,  0.00000000e+00,\n",
      "        3.77170421e-04, -1.21622805e-04, -1.56078677e-05, -2.08085767e-05,\n",
      "        4.41964658e-05, -1.37902634e-06,  3.17368744e-04, -4.15700808e-04],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(200, 1), dtype=float32, numpy=\n",
      "array([[ 1.28262836e-04],\n",
      "       [ 0.00000000e+00],\n",
      "       [ 1.05732310e-04],\n",
      "       [ 1.17446420e-04],\n",
      "       [ 5.03515867e-06],\n",
      "       [ 1.24391754e-05],\n",
      "       [ 9.11211828e-05],\n",
      "       [ 0.00000000e+00],\n",
      "       [ 3.68601795e-05],\n",
      "       [ 5.62963987e-06],\n",
      "       [-7.19855143e-06],\n",
      "       [ 1.02314305e-04],\n",
      "       [ 3.96846735e-04],\n",
      "       [ 4.20435645e-05],\n",
      "       [-2.01977036e-06],\n",
      "       [ 4.94137976e-06],\n",
      "       [ 9.02467364e-05],\n",
      "       [ 1.75432768e-04],\n",
      "       [ 1.88433769e-04],\n",
      "       [-1.69103737e-06],\n",
      "       [ 4.72468855e-06],\n",
      "       [-1.87397552e-06],\n",
      "       [ 1.80450006e-04],\n",
      "       [ 7.83142605e-06],\n",
      "       [ 1.93631364e-04],\n",
      "       [ 2.33896935e-04],\n",
      "       [-1.58519470e-05],\n",
      "       [ 0.00000000e+00],\n",
      "       [ 2.70031072e-04],\n",
      "       [ 0.00000000e+00],\n",
      "       [ 1.06294297e-04],\n",
      "       [ 1.43430414e-04],\n",
      "       [ 6.90141997e-06],\n",
      "       [ 0.00000000e+00],\n",
      "       [ 2.40612528e-04],\n",
      "       [ 1.86084235e-05],\n",
      "       [ 9.00638915e-05],\n",
      "       [ 3.01810451e-05],\n",
      "       [ 1.11672625e-05],\n",
      "       [ 6.62687889e-05],\n",
      "       [ 1.58984491e-04],\n",
      "       [ 3.76629432e-05],\n",
      "       [ 4.71697376e-07],\n",
      "       [ 4.96164612e-05],\n",
      "       [ 5.26595954e-07],\n",
      "       [ 1.05510655e-04],\n",
      "       [ 2.35978208e-04],\n",
      "       [ 8.61504077e-07],\n",
      "       [ 1.37856631e-07],\n",
      "       [ 4.07799816e-05],\n",
      "       [ 5.62222995e-05],\n",
      "       [ 1.61149437e-04],\n",
      "       [ 4.47336060e-05],\n",
      "       [ 1.68179526e-04],\n",
      "       [ 1.83345492e-05],\n",
      "       [ 7.89787300e-05],\n",
      "       [ 1.39053023e-04],\n",
      "       [ 0.00000000e+00],\n",
      "       [ 1.06569038e-04],\n",
      "       [ 2.24670890e-04],\n",
      "       [ 3.96608048e-05],\n",
      "       [-3.59497164e-07],\n",
      "       [-5.19705600e-06],\n",
      "       [ 3.62230526e-06],\n",
      "       [-2.55153532e-06],\n",
      "       [ 2.03657748e-07],\n",
      "       [-1.33107790e-06],\n",
      "       [ 1.80244251e-06],\n",
      "       [-5.40988424e-07],\n",
      "       [ 1.54510303e-06],\n",
      "       [ 0.00000000e+00],\n",
      "       [-3.77793228e-08],\n",
      "       [ 0.00000000e+00],\n",
      "       [ 1.46364284e-04],\n",
      "       [-1.36751796e-05],\n",
      "       [ 1.54999492e-04],\n",
      "       [ 9.67219748e-05],\n",
      "       [ 1.67464445e-04],\n",
      "       [ 1.12769358e-05],\n",
      "       [ 2.14519794e-04],\n",
      "       [ 6.59418147e-05],\n",
      "       [ 7.53362974e-06],\n",
      "       [ 2.17785651e-04],\n",
      "       [-2.39048768e-06],\n",
      "       [-4.06061417e-06],\n",
      "       [ 7.54411030e-06],\n",
      "       [ 0.00000000e+00],\n",
      "       [-1.57896704e-06],\n",
      "       [ 1.59351359e-04],\n",
      "       [ 1.68339306e-04],\n",
      "       [ 2.79874421e-05],\n",
      "       [ 2.93285716e-08],\n",
      "       [ 9.96478630e-05],\n",
      "       [ 2.25569397e-06],\n",
      "       [ 2.06150304e-04],\n",
      "       [ 1.58421943e-04],\n",
      "       [ 1.51025904e-06],\n",
      "       [ 7.81453127e-06],\n",
      "       [ 0.00000000e+00],\n",
      "       [ 1.10002729e-04],\n",
      "       [ 0.00000000e+00],\n",
      "       [ 9.12018149e-05],\n",
      "       [-8.22334528e-07],\n",
      "       [ 4.12222653e-06],\n",
      "       [ 2.02311912e-05],\n",
      "       [ 1.22706668e-04],\n",
      "       [ 6.89506196e-05],\n",
      "       [ 9.20083621e-05],\n",
      "       [ 7.97788671e-05],\n",
      "       [ 3.36166920e-07],\n",
      "       [ 3.45207773e-05],\n",
      "       [ 0.00000000e+00],\n",
      "       [ 7.28174564e-06],\n",
      "       [ 2.11405077e-05],\n",
      "       [ 8.05804884e-05],\n",
      "       [ 1.18643678e-04],\n",
      "       [ 1.02012666e-04],\n",
      "       [-4.70524583e-06],\n",
      "       [ 2.82586757e-06],\n",
      "       [ 0.00000000e+00],\n",
      "       [ 0.00000000e+00],\n",
      "       [ 2.55938471e-06],\n",
      "       [ 2.52381778e-05],\n",
      "       [ 2.78991676e-04],\n",
      "       [ 1.75670702e-05],\n",
      "       [ 1.61639560e-04],\n",
      "       [-5.43924443e-07],\n",
      "       [ 1.31403955e-04],\n",
      "       [ 1.64545054e-04],\n",
      "       [-1.46954349e-07],\n",
      "       [-2.52617201e-06],\n",
      "       [ 8.49901880e-07],\n",
      "       [ 0.00000000e+00],\n",
      "       [ 3.12455813e-04],\n",
      "       [ 1.03908114e-05],\n",
      "       [ 1.06027101e-05],\n",
      "       [ 0.00000000e+00],\n",
      "       [ 1.79015740e-04],\n",
      "       [-8.73405156e-07],\n",
      "       [ 1.08519875e-04],\n",
      "       [-5.95508197e-07],\n",
      "       [ 1.76664835e-04],\n",
      "       [ 2.23824463e-06],\n",
      "       [ 1.68684128e-04],\n",
      "       [ 3.10361230e-07],\n",
      "       [ 5.61932900e-09],\n",
      "       [ 8.46434159e-06],\n",
      "       [ 2.85776769e-05],\n",
      "       [ 1.88053556e-04],\n",
      "       [ 2.79942578e-05],\n",
      "       [ 7.07629670e-06],\n",
      "       [ 4.74746397e-04],\n",
      "       [-5.44925433e-06],\n",
      "       [ 2.24009500e-06],\n",
      "       [ 2.35628590e-06],\n",
      "       [-4.48275523e-06],\n",
      "       [ 3.03837151e-05],\n",
      "       [ 2.56358453e-06],\n",
      "       [ 2.30561112e-04],\n",
      "       [ 6.56360644e-05],\n",
      "       [ 9.44790590e-05],\n",
      "       [ 2.12462237e-05],\n",
      "       [-5.75729200e-06],\n",
      "       [ 1.10888439e-04],\n",
      "       [-1.62874949e-06],\n",
      "       [ 1.42194523e-07],\n",
      "       [ 1.07402202e-05],\n",
      "       [ 3.97297772e-05],\n",
      "       [ 1.03010942e-04],\n",
      "       [-6.18570448e-06],\n",
      "       [ 1.41797005e-04],\n",
      "       [ 2.14428175e-04],\n",
      "       [-2.17993374e-06],\n",
      "       [ 6.21662475e-05],\n",
      "       [-1.30546380e-06],\n",
      "       [ 2.04149874e-05],\n",
      "       [ 1.85358425e-04],\n",
      "       [ 1.75044424e-05],\n",
      "       [-2.84437442e-06],\n",
      "       [ 1.05485100e-07],\n",
      "       [ 3.90185014e-05],\n",
      "       [ 2.55568593e-04],\n",
      "       [ 6.07577431e-06],\n",
      "       [ 1.49786138e-06],\n",
      "       [ 2.08432350e-04],\n",
      "       [-9.88524789e-06],\n",
      "       [ 0.00000000e+00],\n",
      "       [ 1.51294944e-05],\n",
      "       [ 1.17214266e-04],\n",
      "       [-7.71895793e-06],\n",
      "       [ 6.48157002e-05],\n",
      "       [ 0.00000000e+00],\n",
      "       [ 1.60530417e-05],\n",
      "       [ 3.45390115e-04],\n",
      "       [ 2.99866747e-06],\n",
      "       [ 2.06451350e-05],\n",
      "       [ 7.49811443e-05],\n",
      "       [-6.91861715e-08],\n",
      "       [ 8.57357518e-05],\n",
      "       [ 1.49046959e-04]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.00296171], dtype=float32)>]\n",
      ":: OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#bot_tweets: 92 #gen_tweets: 92\n"
     ]
    }
   ],
   "source": [
    "#libraries / dependencies\n",
    "import glob\n",
    "from preprocessor import api as tweet_preprocessor\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "from pyspark.sql.functions import col, udf, to_timestamp, lit, to_timestamp, when, rand\n",
    "from pyspark.sql.types import IntegerType, LongType, DoubleType, StringType, ArrayType\n",
    "from pyspark.ml.feature import Normalizer, StandardScaler, MinMaxScaler, VectorAssembler\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.accumulators import AccumulatorParam\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Dense, Input, concatenate, Dropout, Activation, BatchNormalization\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.losses import MeanSquaredError, CategoricalCrossentropy\n",
    "\n",
    "\n",
    "#thread depencency\n",
    "from pyspark import InheritableThread\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "\n",
    "\n",
    "# initialize the number of epochs to train for, batch size, and\n",
    "# initial learning rate\n",
    "EPOCHS = 25\n",
    "BS = 64\n",
    "INIT_LR = 1e-3\n",
    "\n",
    "mse = MeanSquaredError()\n",
    "cce = CategoricalCrossentropy()\n",
    "opt = Adam(learning_rate=INIT_LR)\n",
    "\n",
    "\n",
    "\n",
    "#Dataset location\n",
    "\n",
    "#Local\n",
    "bot_tweets_dataset_path = 'F://TwitterBotDataset//tweet_dataset_full//bot_tweets//'\n",
    "genuine_tweets_dataset_path = 'F://TwitterBotDataset//tweet_dataset_full//genuine_tweets//'\n",
    "\n",
    "#S3\n",
    "# bot_tweets_dataset_path = 's3://tweet-dataset/bot_tweets' #'F://TwitterBotDataset//tweet_dataset_small//bot_tweets//'\n",
    "# genuine_tweets_dataset_path = 's3://tweet-dataset/genuine_tweets' #'F://TwitterBotDataset//tweet_dataset_small//genuine_tweets//'\n",
    "\n",
    "#turn a line of text into d dimentional vector \n",
    "GLOVE_DIR = \"\"\n",
    "\n",
    "#all columns\n",
    "BOT_COLUMNS = ['id','text','source','user_id','truncated','in_reply_to_status_id', \n",
    "               'in_reply_to_user_id','in_reply_to_screen_name', 'retweeted_status_id',\n",
    "               'geo','place','contributors','retweet_count', 'reply_count','favorite_count',\n",
    "               'favorited', 'retweeted','possibly_sensitive','num_hashtags','num_urls',\n",
    "               'num_mentions','created_at','timestamp','crawled_at', 'updated']\n",
    "\n",
    "GENUINE_COLUMNS = ['id','text','source','user_id','truncated','in_reply_to_status_id', \n",
    "                   'in_reply_to_user_id','in_reply_to_screen_name', 'retweeted_status_id',\n",
    "                   'geo','place','contributors','retweet_count', 'reply_count','favorite_count',\n",
    "                   'favorited', 'retweeted','possibly_sensitive','num_hashtags','num_urls',\n",
    "                   'num_mentions','REMOVE_IT', 'created_at','timestamp','crawled_at', 'updated',]\n",
    "\n",
    "#feature used for bot detection\n",
    "COLUMN_NAMES = ['text', 'retweet_count', 'reply_count', 'favorite_count',\n",
    "                'num_hashtags', 'num_urls', 'num_mentions']\n",
    "\n",
    "\n",
    "# #configure spark\n",
    "# conf = SparkConf()\n",
    "# conf.setMaster(\"local[8]\").setAppName(\"ml_account_ base_session\")\n",
    "# conf.set(\"spark.executor.instances\", 4)\n",
    "# conf.set(\"spark.executor.cores\", 4)\n",
    "# conf.set(\"spark.driver.memory\", 4)\n",
    "# sc = SparkContext(conf=conf)\n",
    "\n",
    "# # for spark-submit\n",
    "# spark = SparkSession.builder.appName('ml_account_ base_session').getOrCreate()\n",
    "# spark\n",
    "\n",
    "# for local build\n",
    "# spark = SparkSession.builder.appName('ml_account_ base_session').getOrCreate()\n",
    "\n",
    "\n",
    "#for local multi thread\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[20]\").setAppName(\"data_parallel_processing_session\")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# read dataset from csv\n",
    "def read_dataset():\n",
    "    bot_tweets = spark.read.csv(bot_tweets_dataset_path, header = True, inferSchema = True).limit(1000)\n",
    "    genuine_tweets = spark.read.csv(genuine_tweets_dataset_path, header = True, inferSchema = True).limit(1000)\n",
    "    \n",
    "    #df = inputDF.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "#     bot_tweets = bot_tweets.persist(StorageLevel.MEMORY_ONLY)\n",
    "#     genuine_tweets = genuine_tweets.persist(StorageLevel.MEMORY_ONLY)\n",
    "    \n",
    "#     print(len(bot_tweets.collect()), len(genuine_tweets.collect()))\n",
    "    return bot_tweets, genuine_tweets\n",
    "\n",
    "def set_column_name(df, column_names):\n",
    "    df = df.toDF(*column_names)\n",
    "#     print(len(df.collect()))\n",
    "    return df\n",
    "\n",
    "def remove_column_miss_match(df):\n",
    "    ## dataset have diffrent number of columns\n",
    "    ## column name of dataframe\n",
    "    column_name = [cname for cname, tp in df.dtypes]\n",
    "#     len(df.collect()), len(df.dtypes)\n",
    "    #column_name\n",
    "\n",
    "    #Number of column is diffrent for bot and genuine tweets data\n",
    "\n",
    "    #genuine_tweets_df = genuine_tweets_df.toDF(*column_name)\n",
    "    df = set_column_name(df, GENUINE_COLUMNS)\n",
    "#     print(len(df.collect()))\n",
    "    \n",
    "    df = df.drop('REMOVE_IT') # remove 5th column from end\n",
    "    #update column name according to \n",
    "    df = set_column_name(df, BOT_COLUMNS)\n",
    "#     print(len(df.collect()))\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_type_miss_match(df):\n",
    "    # Same column has diffrent data type. So make data type same for every column\n",
    "    genuine_tweets_df = df.withColumn(\"id\",col(\"id\").cast(IntegerType())) \\\n",
    "                                    .withColumn(\"favorite_count\",col(\"favorite_count\").cast(LongType())) \\\n",
    "                                    .withColumn(\"favorited\",col(\"favorited\").cast(IntegerType()))\n",
    "    return df\n",
    "\n",
    "\n",
    "def resize_combine_data(bot_tweets_df, genuine_tweets_df):\n",
    "    ## only keep the required column from the dataframe\n",
    "    bot_tweets_df = bot_tweets_df.select(*COLUMN_NAMES)\n",
    "    genuine_tweets_df = genuine_tweets_df.select(*COLUMN_NAMES)\n",
    "    \n",
    "#     print(len(bot_tweets_df.collect()), len(genuine_tweets_df.collect()))\n",
    "\n",
    "    ## add BotOrNot column\n",
    "    bot_tweets_df = bot_tweets_df.withColumn('BotOrNot', lit(1))\n",
    "    genuine_tweets_df = genuine_tweets_df.withColumn('BotOrNot', lit(0))\n",
    "\n",
    "    #combine clean and bot accounts data togather\n",
    "    tweets_df = bot_tweets_df.union(genuine_tweets_df)\n",
    "\n",
    "    # shuffle dataset\n",
    "    tweets_df = tweets_df.orderBy(rand())\n",
    "\n",
    "#     print(len(tweets_df.collect()))\n",
    "    \n",
    "    return tweets_df\n",
    "\n",
    "text_process_udf = udf(lambda x : tweet_preprocessor.tokenize(x), StringType())\n",
    "def preprocess_data(df):\n",
    "    df = df.withColumn('text', text_process_udf(df.text))\n",
    "    df = df.withColumn(\"retweet_count\",col(\"retweet_count\").cast(DoubleType()))\n",
    "    df = df.withColumn(\"reply_count\",col(\"reply_count\").cast(DoubleType()))\n",
    "    df = df.withColumn(\"favorite_count\",col(\"favorite_count\").cast(DoubleType()))\n",
    "    df = df.withColumn(\"num_hashtags\",col(\"num_hashtags\").cast(DoubleType()))\n",
    "    df = df.withColumn(\"num_urls\",col(\"num_urls\").cast(DoubleType()))\n",
    "    df = df.withColumn(\"num_mentions\",col(\"num_mentions\").cast(DoubleType()))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def makeGloveWordEmbedder(glove_path):\n",
    "    embedding_dict = {}\n",
    "    with open(glove_path, 'r', encoding=\"utf-8\") as glove_file:\n",
    "        for line in glove_file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embedding_dict[word] = vector\n",
    "            \n",
    "    return embedding_dict  \n",
    "\n",
    "\n",
    "# # Test GLoVE result\n",
    "# glove_word2vec_embedder[\"google\"]\n",
    "\n",
    " ##create word embedding GLoVE model dictionary. Use pre trained model\n",
    "text_feature_dimention = 25\n",
    "# glove_word2vec_embedder = makeGloveWordEmbedder(GLOVE_DIR + \"glove.twitter.27B.25d.txt\")\n",
    "\n",
    "\n",
    "#Give a word and get that word representing feature vector of dimention 25 from embedding dictionary\n",
    "def word2vec(word_dict=None, word=None, dim=25):\n",
    "    default_vector = np.zeros(dim)\n",
    "    \n",
    "    if word_dict is None or word is None:\n",
    "        return default_vector\n",
    "    \n",
    "    word_vector = word_dict.get(word)\n",
    "    \n",
    "    if word_vector is None:\n",
    "        return default_vector\n",
    "    return word_vector\n",
    "\n",
    "# # test a word representing feature vector\n",
    "# word_vector = word2vec(glove_word2vec_embedder, \"tweet\", text_feature_dimention)\n",
    "# print(type(word_vector), word_vector)\n",
    "\n",
    "\n",
    "#----------------LSTM Model----------------------\n",
    "\n",
    "#create a 1 layer LSTM model\n",
    "#input dimention: 3d (1,7,25) input_sample = [[[1,...,25]],..,[1,...,25]]\n",
    "#output dimention: 1d (1,32) output_sample [[1,2,3,,,,32]]\n",
    "def lstm_model(output_dim):\n",
    "    model = LSTM(output_dim, return_sequences=False, return_state=False)\n",
    "    return model\n",
    "\n",
    "def reset_lstm_model(model):\n",
    "    model.resate_states() # stateful=True is required for reset states\n",
    "\n",
    "# create LSTM model of output dimention 32. Model output feature vector will be of 32 dimention vector\n",
    "lstm = lstm_model(32) \n",
    "\n",
    "# convver a sentence to a feature vector using LSTM(RNN) model\n",
    "def sent2vec(sent):\n",
    "    words = sent.split(' ')\n",
    "    word_vectors = np.array([])\n",
    "    count = 0;\n",
    "    for word in words:\n",
    "        word_vector = word2vec(broadcast_glove_dict.value, word)\n",
    "#         print(\"word dim: {}\".format(len(word_vector)))\n",
    "        if word_vectors.size == 0:\n",
    "            word_vectors = np.array([word_vector])\n",
    "        else:\n",
    "            word_vectors = np.vstack([word_vectors, word_vector])\n",
    "        count = count + 1\n",
    "    \n",
    "#     print(\"Input feature vector shape before reshape(2D): {}\".format(word_vectors.shape))\n",
    "        \n",
    "    input_feature_vectors = np.reshape(word_vectors, (1, count, text_feature_dimention))\n",
    "#     print(\"Input feature vector shape after reshape(3d): {}\".format(input_feature_vectors.shape))\n",
    "#     print(\"LSTM requirs 3d shape inputs [batch, timesteps, feature]\")\n",
    "    output_vector = lstm(input_feature_vectors)\n",
    "#     lstm.reset_states() # stateful = True is required for reset\n",
    "\n",
    "#     print(\"result vector shape: {}\".format(output_vector.shape))\n",
    "#     print(\"Last input was: {}\".format(input_feature_vectors[0][-1]))\n",
    "#     print(\"output result: {}\".format(output_vector))\n",
    "    \n",
    "    # (tensore --> numpy 0bject --> numpy.array --> array/list/ArrayType)\n",
    "    return output_vector.numpy()[0].tolist() \n",
    "    \n",
    "## For Testing sentence to vector convertion\n",
    "# sent = \"Twitter is a large social media network\"\n",
    "# res_vector = sent2vec(sent)\n",
    "# type(res_vector), res_vector\n",
    "\n",
    "\n",
    "# text string --> vector 32 dimention\n",
    "sent_to_vector_udf = udf(lambda x : sent2vec(x), ArrayType(DoubleType()))\n",
    "def processTextColumn(df, column_name, new_column_name):\n",
    "    df = df.withColumn(new_column_name, sent_to_vector_udf(col(column_name)))\n",
    "    return df\n",
    "\n",
    "def sentEmbeddingGLoVE_LSTM(df):\n",
    "    \n",
    "    text_updated_column = 'text_features'\n",
    "    updated_df = processTextColumn(df, \"text\", text_updated_column)\n",
    "\n",
    "#     print(len(updated_df.collect()), type(updated_df), updated_df.printSchema()) \n",
    "    \n",
    "    return updated_df\n",
    "\n",
    "\n",
    "\n",
    "def assembleColumns(tweets_df):\n",
    "    columns = ['retweet_count', 'reply_count', 'favorite_count',\n",
    "               'num_hashtags' ,'num_urls', 'num_mentions', 'BotOrNot']\n",
    "\n",
    "    tweets_df = tweets_df.select(*columns, \n",
    "                          tweets_df.text_features[0], tweets_df.text_features[1], tweets_df.text_features[2],tweets_df.text_features[3], tweets_df.text_features[4],\n",
    "                          tweets_df.text_features[5], tweets_df.text_features[6], tweets_df.text_features[7],tweets_df.text_features[8], tweets_df.text_features[9], \n",
    "                          tweets_df.text_features[10], tweets_df.text_features[11], tweets_df.text_features[12],tweets_df.text_features[13], tweets_df.text_features[14],\n",
    "                          tweets_df.text_features[15], tweets_df.text_features[16], tweets_df.text_features[17],tweets_df.text_features[18], tweets_df.text_features[19],\n",
    "                          tweets_df.text_features[20], tweets_df.text_features[21], tweets_df.text_features[22],tweets_df.text_features[23], tweets_df.text_features[24],\n",
    "                          tweets_df.text_features[25], tweets_df.text_features[26], tweets_df.text_features[27],tweets_df.text_features[28], tweets_df.text_features[29],\n",
    "                          tweets_df.text_features[30], tweets_df.text_features[31])\n",
    "\n",
    "\n",
    "#     print(tweets_df.columns, len(tweets_df.collect()), tweets_df.printSchema())\n",
    "\n",
    "    #remove \n",
    "\n",
    "    feature_columns = ['retweet_count','reply_count','favorite_count','num_hashtags','num_urls','num_mentions',\n",
    "                       'text_features[0]','text_features[1]', 'text_features[2]','text_features[3]','text_features[4]',\n",
    "                       'text_features[5]','text_features[6]','text_features[7]', 'text_features[8]','text_features[9]',\n",
    "                       'text_features[10]','text_features[11]','text_features[12]','text_features[13]','text_features[14]',\n",
    "                       'text_features[15]','text_features[16]','text_features[17]','text_features[18]','text_features[19]',\n",
    "                       'text_features[20]','text_features[21]','text_features[22]', 'text_features[23]', 'text_features[24]',\n",
    "                       'text_features[25]','text_features[26]','text_features[27]', 'text_features[28]', 'text_features[29]',\n",
    "                       'text_features[30]','text_features[31]']\n",
    "\n",
    "\n",
    "    tweets_df = tweets_df.na.fill(value=0.0 ,subset= feature_columns)\n",
    "    feature_assembler = VectorAssembler(inputCols = feature_columns, outputCol = 'independent_features')\n",
    "\n",
    "    tweets_updated_df = feature_assembler.transform(tweets_df)\n",
    "\n",
    "    #check\n",
    "#     num = len(tweets_updated_df.collect())\n",
    "#     print(num, type(tweets_updated_df), tweets_updated_df.printSchema())\n",
    "\n",
    "    #remove unnecessary columns\n",
    "    tweets_updated_df = tweets_updated_df.drop(*feature_columns)\n",
    "    \n",
    "    return tweets_updated_df\n",
    "\n",
    "\n",
    "def to_nparray_list(df, column_name):\n",
    "    rows = df.select(column_name).collect()\n",
    "    lists = [x[column_name] for x in rows]\n",
    "    nparr = np.array(lists)\n",
    "    \n",
    "    return nparr\n",
    "\n",
    "def to_nparray_dataset(df, feature_column, target_column):\n",
    "#     list(df.select('col_name').toPandas()['col_name']) \n",
    "#     feature = list(df.select(feature_column).toPandas()[feature_column])\n",
    "#     target = list(df.select(target_column).toPandas()[target_column])\n",
    "    feature = [row[0] for row in list(df.select(feature_column).toLocalIterator())]\n",
    "    target = [row[0] for row in list(df.select(target_column).toLocalIterator())]\n",
    "        \n",
    "    return np.array(feature), np.array(target)    \n",
    "\n",
    "def partition_dataset(df):\n",
    "    train_df, test_df = df.randomSplit([0.80, 0.20])\n",
    "#     print(len(train_df.collect()), len(test_df.collect()))\n",
    "\n",
    "    # features --> 'BotOrNot'\n",
    "#     X_train = train_df.drop('BotOrNot')\n",
    "#     y_train = train_df.select('BotOrNot')\n",
    "#     X_test = test_df.drop('BotOrNot')\n",
    "#     y_test = test_df.select('BotOrNot')\n",
    "    \n",
    "\n",
    "    #checkpoint\n",
    "#     print(len(X_train.collect()), len(y_train.collect()))\n",
    "#     print(len(X_test.collect()), len(y_test.collect()))\n",
    "\n",
    "#     X_train = to_nparray_list(X_train, 'independent_features')\n",
    "#     y_train = to_nparray_list(y_train, 'BotOrNot')\n",
    "#     X_test = to_nparray_list(X_test, 'independent_features')\n",
    "#     y_test = to_nparray_list(y_test, 'BotOrNot')\n",
    "    train_df = train_df.cache()\n",
    "    test_df = test_df.cache()\n",
    "\n",
    "    train_X,  train_Y = to_nparray_dataset(train_df, 'independent_features', 'BotOrNot')\n",
    "    test_X, test_Y = to_nparray_dataset(test_df, 'independent_features', 'BotOrNot')\n",
    "\n",
    "    \n",
    "    return train_X, train_Y, test_X, test_Y # return type: numpy.array\n",
    "\n",
    "'''\n",
    "def distributedTrainingGradients(df, feature_column, target_column, n_splits):\n",
    "    print(df.count())\n",
    "    each_len = df.count() // n_splits\n",
    "    gradients = []\n",
    "    ##split dataset into 'n_splits' part\n",
    "    copy_df = df\n",
    "    for i in range(n_splits):\n",
    "        print(i)\n",
    "        temp_df = copy_df.limit(each_len)\n",
    "        copy_df = copy_df.subtract(temp_df)\n",
    "        \n",
    "        X = temp_df.select(feature_column)\n",
    "        Y = temp_df.select(target_column)\n",
    "        X_np = to_nparray_list(X, feature_column)\n",
    "        Y_np = to_nparray_list(Y, target_column)\n",
    "        \n",
    "        grad = step(X_np, Y_np)\n",
    "        gradients.append(grad)\n",
    "        print(temp_df.count())\n",
    "        \n",
    "    return gradients\n",
    "'''\n",
    "\n",
    "def generateGradient(X, Y):\n",
    "    gd = step(X, Y)\n",
    "    return gd\n",
    "\n",
    "def step(X, Y):\n",
    "    print(\"Input count: {}, {}\".format(len(X), len(Y)))\n",
    "    #keep track of gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        curr_model = get2DenseLayeredModel(38)\n",
    "        #make a prediction using model\n",
    "        predict = curr_model(X)\n",
    "        #calculate loss\n",
    "        loss = mse(Y, predict)\n",
    "    #calculate the gradient\n",
    "    gd = tape.gradient(loss, curr_model.trainable_variables)\n",
    "    \n",
    "    # return the gradient to train final model\n",
    "    return gd\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "def stepEPOCH(X, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        curr_model = getDLModel()\n",
    "        for i in range(EPOCHS):\n",
    "            #make a prediction using model\n",
    "            predict = curr_model(X)\n",
    "            #calculate loss\n",
    "            loss = cce(y, predict)\n",
    "            print(\"{}: {}\".format(i, loss))\n",
    "            opt.apply_gradients(zip(grad, curr_model.trainable_variables))\n",
    "            \n",
    "    gradient = tape.gradient(loss, curr_model.trainable_variables)\n",
    "    \n",
    "    return gradient\n",
    "\n",
    "'''\n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "## create model\n",
    "def get2DenseLayeredModel(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(500, input_dim=input_dim))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(200))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def model_evaluation(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train,\n",
    "              batch_size=64,\n",
    "              epochs=10,\n",
    "              validation_data=(X_test, y_test))\n",
    "    score, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "\n",
    "gds = []\n",
    "def worker_task(bot_tweets_df, genuine_tweets_df):\n",
    "#     #cache df\n",
    "    bot_tweets_df = bot_tweets_df.cache()\n",
    "    genuine_tweets_df = genuine_tweets_df.cache()\n",
    "    \n",
    "    print(\"#bot_tweets: {} #gen_tweets: {}\".format(bot_tweets_df.count(), genuine_tweets_df.count()))\n",
    "    \n",
    "    ##clean data / remove unwanted column\n",
    "    if len(bot_tweets_df.columns) == 26:\n",
    "        bot_tweets_df = remove_column_miss_match(bot_tweets_df)\n",
    "    else:\n",
    "        bot_tweets_df = set_column_name(bot_tweets_df, BOT_COLUMNS)\n",
    "        \n",
    "    if len(genuine_tweets_df.columns) == 26:\n",
    "        genuine_tweets_df = remove_column_miss_match(genuine_tweets_df)\n",
    "    else:\n",
    "        genuine_tweets_df = set_column_name(genuine_tweets_df, BOT_COLUMNS)\n",
    "    \n",
    "    \n",
    "    bot_tweets_df = remove_type_miss_match(bot_tweets_df)\n",
    "    genuine_tweets_df = remove_type_miss_match(genuine_tweets_df)\n",
    "    \n",
    "#     print(len(bot_tweets_df.collect()), len(genuine_tweets_df.collect()))\n",
    "    \n",
    "    ##preprocess data\n",
    "    tweets_df = resize_combine_data(bot_tweets_df, genuine_tweets_df)\n",
    "    tweets_df = preprocess_data(tweets_df)\n",
    "    \n",
    "#     print(len(tweets_df.collect()))\n",
    "#     print(tweets_df.columns)\n",
    "    \n",
    "    ##text embedding using GLoVE & LSTM\n",
    "    ## Word Embedding\n",
    "    tweets_df = sentEmbeddingGLoVE_LSTM(tweets_df)\n",
    "    \n",
    "    ## Assable multiple colu,ms to create feature vector\n",
    "    tweets_updated_df = assembleColumns(tweets_df)\n",
    "#     print(len(tweets_updated_df.collect()), tweets_updated_df.columns)\n",
    "\n",
    "#     tweets_updated_df = tweets_updated_df.cache()\n",
    "    \n",
    "    X, Y = to_nparray_dataset(tweets_updated_df, 'independent_features', 'BotOrNot')\n",
    "    \n",
    "    curr_gd = generateGradient(X, Y)\n",
    "    \n",
    "    accu_count.add(1)\n",
    "    accu_gd.add(curr_gd)\n",
    "    \n",
    "#     gds.append(curr_gd)\n",
    "    print(\"Curr gd: {}\".format(curr_gd))\n",
    "    print(\":: OK\")\n",
    "    \n",
    "#     ##Create Dense Model\n",
    "    \n",
    "#     ##device data into test and train parts\n",
    "#     X_train, y_train, X_test, y_test = partition_dataset(tweets_updated_df)\n",
    "    \n",
    "#     ## ml model train and validation\n",
    "#     model = get2DenseLayeredModel(38)\n",
    "    \n",
    "#     ##evaluate model\n",
    "#     model_evaluation(model, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "#     # save gradient to hdfs/s3\n",
    "\n",
    "\n",
    "# broadcast glove word wmbedder to all task\n",
    "def broadcastGloveDict():\n",
    "    print(\"broadcast glove\")\n",
    "    glove_dict = makeGloveWordEmbedder(GLOVE_DIR + \"glove.twitter.27B.25d.txt\")\n",
    "    broadcast_glove_dict = sc.broadcast(glove_dict)\n",
    "\n",
    "glove_dict = makeGloveWordEmbedder(GLOVE_DIR + \"glove.twitter.27B.25d.txt\")\n",
    "broadcast_glove_dict = sc.broadcast(glove_dict)\n",
    "\n",
    "\n",
    "\n",
    "# accumulate partial train gradients\n",
    "class TensorAccumulator(AccumulatorParam):\n",
    "    def zero(self, value):\n",
    "        return None\n",
    "    \n",
    "    def addInPlace(self, val1, val2):\n",
    "        if val1 is None:\n",
    "            val1 = val2\n",
    "        else:\n",
    "            val1 += val2\n",
    "        return val1\n",
    "\n",
    "accu_gd = spark.sparkContext.accumulator(None, TensorAccumulator())\n",
    "accu_count = spark.sparkContext.accumulator(0)\n",
    "    \n",
    "def accumulateGradient():\n",
    "    print(\"accumulate gds\")\n",
    "\n",
    "def ApplicationJob():\n",
    "    bot_tweets_df, genuine_tweets_df = read_dataset()\n",
    "    \n",
    "#     broadcastGloveDict()\n",
    "    # split dataset for parallel data training\n",
    "    num_of_thread = 10\n",
    "    split_weight = 1.0 / num_of_thread\n",
    "    weights = [split_weight] * num_of_thread\n",
    "    bot_dfs = bot_tweets_df.randomSplit(weights, seed = 71)\n",
    "    gen_dfs = bot_tweets_df.randomSplit(weights, seed = 71)\n",
    "    \n",
    "\n",
    "    for idx in range(num_of_thread):\n",
    "        thread = InheritableThread(target = worker_task, kwargs={'bot_tweets_df': bot_dfs[idx], \n",
    "                                                                'genuine_tweets_df': gen_dfs[idx]})\n",
    "        thread.start()\n",
    "        thread.join()\n",
    "    \n",
    "    print(\"#gd: {}\".format(accu_count.value))\n",
    "    print(\"Sum of gd: {}\".format(accu_gd))\n",
    "#     ## single worker or multiple worker\n",
    "#     worker_task(bot_tweets_df, genuine_tweets_df)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ##load dara\n",
    "    ApplicationJob()\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b9bb55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4acbaf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaa\n"
     ]
    }
   ],
   "source": [
    "print(\"aaaa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395b04d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.accumulators import AccumulatorParam\n",
    "class TensorAccumulator(AccumulatorParam):\n",
    "    def zero(self, value):\n",
    "        return None\n",
    "    \n",
    "    def addInPlace(self, val1, val2):\n",
    "        if val1 is None:\n",
    "            val1 = val2\n",
    "        else:\n",
    "            val1 += val2\n",
    "        return val1\n",
    "    \n",
    "va = sc.accumulator(None, TensorAccumulator())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
