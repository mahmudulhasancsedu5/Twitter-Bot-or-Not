{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eef9e63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input count: 1, 1\n",
      "gradient: save successful\n",
      "#bot_tweets: 81 #gen_tweets: 81\n",
      "gradient: load successful\n",
      "Input count: 162, 162\n",
      "gradient: save successful\n",
      ":: OK\n",
      "#bot_tweets: 64 #gen_tweets: 64\n",
      "gradient: load successful\n",
      "Input count: 128, 128\n",
      "gradient: save successful\n",
      ":: OK\n",
      "#bot_tweets: 78 #gen_tweets: 78\n",
      "gradient: load successful\n",
      "Input count: 156, 156\n",
      "gradient: save successful\n",
      ":: OK\n",
      "#bot_tweets: 75 #gen_tweets: 75\n",
      "gradient: load successful\n",
      "Input count: 150, 150\n",
      "gradient: save successful\n",
      ":: OK\n",
      "#bot_tweets: 83 #gen_tweets: 83\n",
      "gradient: load successful\n",
      "Input count: 166, 166\n",
      "gradient: save successful\n",
      ":: OK\n",
      "#bot_tweets: 74 #gen_tweets: 74\n",
      "gradient: load successful\n",
      "Input count: 148, 148\n",
      "gradient: save successful\n",
      ":: OK\n",
      "#bot_tweets: 74 #gen_tweets: 74\n",
      "gradient: load successful\n",
      "Input count: 148, 148\n",
      "gradient: save successful\n",
      ":: OK\n",
      "#bot_tweets: 79 #gen_tweets: 79\n",
      "gradient: load successful\n",
      "Input count: 158, 158\n",
      "gradient: save successful\n",
      ":: OK\n",
      "#bot_tweets: 90 #gen_tweets: 90\n",
      "gradient: load successful\n",
      "Input count: 180, 180\n",
      "gradient: save successful\n",
      ":: OK\n",
      "#bot_tweets: 94 #gen_tweets: 94\n",
      "gradient: load successful\n",
      "Input count: 188, 188\n",
      "gradient: save successful\n",
      ":: OK\n",
      "gradient: load successful\n",
      "Final w0: \n",
      "GD: \n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - acc: 0.5649\n",
      "[INFO] test accuracy: 0.5649038553237915\n",
      "[INFO] test loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "#libraries / dependencies\n",
    "import glob\n",
    "from preprocessor import api as tweet_preprocessor\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "from pyspark.sql.functions import col, udf, to_timestamp, lit, to_timestamp, when, rand\n",
    "from pyspark.sql.types import IntegerType, LongType, DoubleType, StringType, ArrayType\n",
    "from pyspark.ml.feature import Normalizer, StandardScaler, MinMaxScaler, VectorAssembler\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.accumulators import AccumulatorParam\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Dense, Input, concatenate, Dropout, Activation, BatchNormalization\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.losses import MeanSquaredError, CategoricalCrossentropy\n",
    "\n",
    "\n",
    "#thread depencency\n",
    "from pyspark import InheritableThread\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "\n",
    "\n",
    "# initialize the number of epochs to train for, batch size, and\n",
    "# initial learning rate\n",
    "EPOCHS = 25\n",
    "BS = 64\n",
    "INIT_LR = 1e-3\n",
    "\n",
    "mse = MeanSquaredError()\n",
    "cce = CategoricalCrossentropy()\n",
    "opt = Adam(learning_rate=INIT_LR)\n",
    "\n",
    "\n",
    "\n",
    "#Dataset location\n",
    "\n",
    "#Local\n",
    "bot_tweets_dataset_path = 'F://TwitterBotDataset//tweet_dataset_full//bot_tweets//'\n",
    "genuine_tweets_dataset_path = 'F://TwitterBotDataset//tweet_dataset_full//genuine_tweets//'\n",
    "\n",
    "#S3\n",
    "# bot_tweets_dataset_path = 's3://tweet-dataset/bot_tweets' #'F://TwitterBotDataset//tweet_dataset_small//bot_tweets//'\n",
    "# genuine_tweets_dataset_path = 's3://tweet-dataset/genuine_tweets' #'F://TwitterBotDataset//tweet_dataset_small//genuine_tweets//'\n",
    "\n",
    "#turn a line of text into d dimentional vector \n",
    "GLOVE_DIR = \"\"\n",
    "grad_dir = \"C://Users//USER//projects//\"\n",
    "fnames = ['g1.npy', 'g2.npy', 'g3.npy', 'g4.npy', 'g5.npy', 'g6.npy']\n",
    "\n",
    "#all columns\n",
    "BOT_COLUMNS = ['id','text','source','user_id','truncated','in_reply_to_status_id', \n",
    "               'in_reply_to_user_id','in_reply_to_screen_name', 'retweeted_status_id',\n",
    "               'geo','place','contributors','retweet_count', 'reply_count','favorite_count',\n",
    "               'favorited', 'retweeted','possibly_sensitive','num_hashtags','num_urls',\n",
    "               'num_mentions','created_at','timestamp','crawled_at', 'updated']\n",
    "\n",
    "GENUINE_COLUMNS = ['id','text','source','user_id','truncated','in_reply_to_status_id', \n",
    "                   'in_reply_to_user_id','in_reply_to_screen_name', 'retweeted_status_id',\n",
    "                   'geo','place','contributors','retweet_count', 'reply_count','favorite_count',\n",
    "                   'favorited', 'retweeted','possibly_sensitive','num_hashtags','num_urls',\n",
    "                   'num_mentions','REMOVE_IT', 'created_at','timestamp','crawled_at', 'updated',]\n",
    "\n",
    "#feature used for bot detection\n",
    "COLUMN_NAMES = ['text', 'retweet_count', 'reply_count', 'favorite_count',\n",
    "                'num_hashtags', 'num_urls', 'num_mentions']\n",
    "\n",
    "\n",
    "# #configure spark\n",
    "# conf = SparkConf()\n",
    "# conf.setMaster(\"local[8]\").setAppName(\"ml_account_ base_session\")\n",
    "# conf.set(\"spark.executor.instances\", 4)\n",
    "# conf.set(\"spark.executor.cores\", 4)\n",
    "# conf.set(\"spark.driver.memory\", 4)\n",
    "# sc = SparkContext(conf=conf)\n",
    "\n",
    "# # for spark-submit\n",
    "# spark = SparkSession.builder.appName('ml_account_ base_session').getOrCreate()\n",
    "# spark\n",
    "\n",
    "# for local build\n",
    "# spark = SparkSession.builder.appName('ml_account_ base_session').getOrCreate()\n",
    "\n",
    "\n",
    "#for local multi thread\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[10]\").setAppName(\"distributed_training_session\")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# read dataset from csv\n",
    "def read_dataset():\n",
    "    bot_tweets = spark.read.csv(bot_tweets_dataset_path, header = True, inferSchema = True).limit(1000)\n",
    "    genuine_tweets = spark.read.csv(genuine_tweets_dataset_path, header = True, inferSchema = True).limit(1000)\n",
    "    \n",
    "#     bot_tweets = bot_tweets.persist(StorageLevel.MEMORY_ONLY)\n",
    "#     genuine_tweets = genuine_tweets.persist(StorageLevel.MEMORY_ONLY)\n",
    "    return bot_tweets, genuine_tweets\n",
    "\n",
    "def set_column_name(df, column_names):\n",
    "    df = df.toDF(*column_names)\n",
    "    return df\n",
    "\n",
    "def remove_column_miss_match(df):\n",
    "    ## dataset have diffrent number of columns\n",
    "    ## column name of dataframe\n",
    "    column_name = [cname for cname, tp in df.dtypes]\n",
    "#     len(df.collect()), len(df.dtypes)\n",
    "    #column_name\n",
    "\n",
    "    #Number of column is diffrent for bot and genuine tweets data\n",
    "\n",
    "    #genuine_tweets_df = genuine_tweets_df.toDF(*column_name)\n",
    "    df = set_column_name(df, GENUINE_COLUMNS)\n",
    "#     print(len(df.collect()))\n",
    "    \n",
    "    df = df.drop('REMOVE_IT') # remove 5th column from end\n",
    "    #update column name according to \n",
    "    df = set_column_name(df, BOT_COLUMNS)\n",
    "#     print(len(df.collect()))\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_type_miss_match(df):\n",
    "    # Same column has diffrent data type. So make data type same for every column\n",
    "    genuine_tweets_df = df.withColumn(\"id\",col(\"id\").cast(IntegerType())) \\\n",
    "                                    .withColumn(\"favorite_count\",col(\"favorite_count\").cast(LongType())) \\\n",
    "                                    .withColumn(\"favorited\",col(\"favorited\").cast(IntegerType()))\n",
    "    return df\n",
    "\n",
    "\n",
    "def resize_combine_data(bot_tweets_df, genuine_tweets_df):\n",
    "    ## only keep the required column from the dataframe\n",
    "    bot_tweets_df = bot_tweets_df.select(*COLUMN_NAMES)\n",
    "    genuine_tweets_df = genuine_tweets_df.select(*COLUMN_NAMES)\n",
    "    \n",
    "#     print(len(bot_tweets_df.collect()), len(genuine_tweets_df.collect()))\n",
    "\n",
    "    ## add BotOrNot column\n",
    "    bot_tweets_df = bot_tweets_df.withColumn('BotOrNot', lit(1))\n",
    "    genuine_tweets_df = genuine_tweets_df.withColumn('BotOrNot', lit(0))\n",
    "\n",
    "    #combine clean and bot accounts data togather\n",
    "    tweets_df = bot_tweets_df.union(genuine_tweets_df)\n",
    "\n",
    "    # shuffle dataset\n",
    "    tweets_df = tweets_df.orderBy(rand())\n",
    "\n",
    "#     print(len(tweets_df.collect()))\n",
    "    \n",
    "    return tweets_df\n",
    "\n",
    "text_process_udf = udf(lambda x : tweet_preprocessor.tokenize(x), StringType())\n",
    "def preprocess_data(df):\n",
    "    df = df.withColumn('text', text_process_udf(df.text))\n",
    "    df = df.withColumn(\"retweet_count\",col(\"retweet_count\").cast(DoubleType()))\n",
    "    df = df.withColumn(\"reply_count\",col(\"reply_count\").cast(DoubleType()))\n",
    "    df = df.withColumn(\"favorite_count\",col(\"favorite_count\").cast(DoubleType()))\n",
    "    df = df.withColumn(\"num_hashtags\",col(\"num_hashtags\").cast(DoubleType()))\n",
    "    df = df.withColumn(\"num_urls\",col(\"num_urls\").cast(DoubleType()))\n",
    "    df = df.withColumn(\"num_mentions\",col(\"num_mentions\").cast(DoubleType()))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def makeGloveWordEmbedder(glove_path):\n",
    "    embedding_dict = {}\n",
    "    with open(glove_path, 'r', encoding=\"utf-8\") as glove_file:\n",
    "        for line in glove_file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embedding_dict[word] = vector\n",
    "            \n",
    "    return embedding_dict  \n",
    "\n",
    "\n",
    "# # Test GLoVE result\n",
    "# glove_word2vec_embedder[\"google\"]\n",
    "\n",
    " ##create word embedding GLoVE model dictionary. Use pre trained model\n",
    "text_feature_dimention = 25\n",
    "# glove_word2vec_embedder = makeGloveWordEmbedder(GLOVE_DIR + \"glove.twitter.27B.25d.txt\")\n",
    "\n",
    "\n",
    "#Give a word and get that word representing feature vector of dimention 25 from embedding dictionary\n",
    "def word2vec(word_dict=None, word=None, dim=25):\n",
    "    default_vector = np.zeros(dim)\n",
    "    \n",
    "    if word_dict is None or word is None:\n",
    "        return default_vector\n",
    "    \n",
    "    word_vector = word_dict.get(word)\n",
    "    \n",
    "    if word_vector is None:\n",
    "        return default_vector\n",
    "    return word_vector\n",
    "\n",
    "# # test a word representing feature vector\n",
    "# word_vector = word2vec(glove_word2vec_embedder, \"tweet\", text_feature_dimention)\n",
    "# print(type(word_vector), word_vector)\n",
    "\n",
    "\n",
    "#----------------LSTM Model----------------------\n",
    "\n",
    "#create a 1 layer LSTM model\n",
    "#input dimention: 3d (1,7,25) input_sample = [[[1,...,25]],..,[1,...,25]]\n",
    "#output dimention: 1d (1,32) output_sample [[1,2,3,,,,32]]\n",
    "def lstm_model(output_dim):\n",
    "    model = LSTM(output_dim, return_sequences=False, return_state=False)\n",
    "    return model\n",
    "\n",
    "def reset_lstm_model(model):\n",
    "    model.resate_states() # stateful=True is required for reset states\n",
    "\n",
    "# create LSTM model of output dimention 32. Model output feature vector will be of 32 dimention vector\n",
    "lstm = lstm_model(32) \n",
    "\n",
    "# convver a sentence to a feature vector using LSTM(RNN) model\n",
    "def sent2vec(sent):\n",
    "    words = sent.split(' ')\n",
    "    word_vectors = np.array([])\n",
    "    count = 0;\n",
    "    for word in words:\n",
    "        word_vector = word2vec(broadcast_glove_dict.value, word)\n",
    "#         print(\"word dim: {}\".format(len(word_vector)))\n",
    "        if word_vectors.size == 0:\n",
    "            word_vectors = np.array([word_vector])\n",
    "        else:\n",
    "            word_vectors = np.vstack([word_vectors, word_vector])\n",
    "        count = count + 1\n",
    "    \n",
    "#     print(\"Input feature vector shape before reshape(2D): {}\".format(word_vectors.shape))\n",
    "        \n",
    "    input_feature_vectors = np.reshape(word_vectors, (1, count, text_feature_dimention))\n",
    "#     print(\"Input feature vector shape after reshape(3d): {}\".format(input_feature_vectors.shape))\n",
    "#     print(\"LSTM requirs 3d shape inputs [batch, timesteps, feature]\")\n",
    "    output_vector = lstm(input_feature_vectors)\n",
    "#     lstm.reset_states() # stateful = True is required for reset\n",
    "\n",
    "#     print(\"result vector shape: {}\".format(output_vector.shape))\n",
    "#     print(\"Last input was: {}\".format(input_feature_vectors[0][-1]))\n",
    "#     print(\"output result: {}\".format(output_vector))\n",
    "    \n",
    "    # (tensore --> numpy 0bject --> numpy.array --> array/list/ArrayType)\n",
    "    return output_vector.numpy()[0].tolist() \n",
    "    \n",
    "## For Testing sentence to vector convertion\n",
    "# sent = \"Twitter is a large social media network\"\n",
    "# res_vector = sent2vec(sent)\n",
    "# type(res_vector), res_vector\n",
    "\n",
    "\n",
    "# text string --> vector 32 dimention\n",
    "sent_to_vector_udf = udf(lambda x : sent2vec(x), ArrayType(DoubleType()))\n",
    "def processTextColumn(df, column_name, new_column_name):\n",
    "    df = df.withColumn(new_column_name, sent_to_vector_udf(col(column_name)))\n",
    "    return df\n",
    "\n",
    "def sentEmbeddingGLoVE_LSTM(df):\n",
    "    \n",
    "    text_updated_column = 'text_features'\n",
    "    updated_df = processTextColumn(df, \"text\", text_updated_column)\n",
    "\n",
    "#     print(len(updated_df.collect()), type(updated_df), updated_df.printSchema()) \n",
    "    \n",
    "    return updated_df\n",
    "\n",
    "\n",
    "\n",
    "def assembleColumns(tweets_df):\n",
    "    columns = ['retweet_count', 'reply_count', 'favorite_count',\n",
    "               'num_hashtags' ,'num_urls', 'num_mentions', 'BotOrNot']\n",
    "\n",
    "    tweets_df = tweets_df.select(*columns, \n",
    "                          tweets_df.text_features[0], tweets_df.text_features[1], tweets_df.text_features[2],tweets_df.text_features[3], tweets_df.text_features[4],\n",
    "                          tweets_df.text_features[5], tweets_df.text_features[6], tweets_df.text_features[7],tweets_df.text_features[8], tweets_df.text_features[9], \n",
    "                          tweets_df.text_features[10], tweets_df.text_features[11], tweets_df.text_features[12],tweets_df.text_features[13], tweets_df.text_features[14],\n",
    "                          tweets_df.text_features[15], tweets_df.text_features[16], tweets_df.text_features[17],tweets_df.text_features[18], tweets_df.text_features[19],\n",
    "                          tweets_df.text_features[20], tweets_df.text_features[21], tweets_df.text_features[22],tweets_df.text_features[23], tweets_df.text_features[24],\n",
    "                          tweets_df.text_features[25], tweets_df.text_features[26], tweets_df.text_features[27],tweets_df.text_features[28], tweets_df.text_features[29],\n",
    "                          tweets_df.text_features[30], tweets_df.text_features[31])\n",
    "\n",
    "\n",
    "#     print(tweets_df.columns, len(tweets_df.collect()), tweets_df.printSchema())\n",
    "\n",
    "    #remove \n",
    "\n",
    "    feature_columns = ['retweet_count','reply_count','favorite_count','num_hashtags','num_urls','num_mentions',\n",
    "                       'text_features[0]','text_features[1]', 'text_features[2]','text_features[3]','text_features[4]',\n",
    "                       'text_features[5]','text_features[6]','text_features[7]', 'text_features[8]','text_features[9]',\n",
    "                       'text_features[10]','text_features[11]','text_features[12]','text_features[13]','text_features[14]',\n",
    "                       'text_features[15]','text_features[16]','text_features[17]','text_features[18]','text_features[19]',\n",
    "                       'text_features[20]','text_features[21]','text_features[22]', 'text_features[23]', 'text_features[24]',\n",
    "                       'text_features[25]','text_features[26]','text_features[27]', 'text_features[28]', 'text_features[29]',\n",
    "                       'text_features[30]','text_features[31]']\n",
    "\n",
    "\n",
    "    tweets_df = tweets_df.na.fill(value=0.0 ,subset= feature_columns)\n",
    "    feature_assembler = VectorAssembler(inputCols = feature_columns, outputCol = 'independent_features')\n",
    "\n",
    "    tweets_updated_df = feature_assembler.transform(tweets_df)\n",
    "\n",
    "    #check\n",
    "#     num = len(tweets_updated_df.collect())\n",
    "#     print(num, type(tweets_updated_df), tweets_updated_df.printSchema())\n",
    "\n",
    "    #remove unnecessary columns\n",
    "    tweets_updated_df = tweets_updated_df.drop(*feature_columns)\n",
    "    \n",
    "    return tweets_updated_df\n",
    "\n",
    "\n",
    "def to_nparray_list(df, column_name):\n",
    "    rows = df.select(column_name).collect()\n",
    "    lists = [x[column_name] for x in rows]\n",
    "    nparr = np.array(lists)\n",
    "    \n",
    "    return nparr\n",
    "\n",
    "def to_nparray_dataset(df, feature_column, target_column):\n",
    "#     list(df.select('col_name').toPandas()['col_name']) \n",
    "#     feature = list(df.select(feature_column).toPandas()[feature_column])\n",
    "#     target = list(df.select(target_column).toPandas()[target_column])\n",
    "    feature = [row[0] for row in list(df.select(feature_column).toLocalIterator())]\n",
    "    target = [row[0] for row in list(df.select(target_column).toLocalIterator())]\n",
    "        \n",
    "    return np.array(feature), np.array(target)    \n",
    "\n",
    "def partition_dataset(df):\n",
    "    train_df, test_df = df.randomSplit([0.80, 0.20])\n",
    "#     print(len(train_df.collect()), len(test_df.collect()))\n",
    "\n",
    "    # features --> 'BotOrNot'\n",
    "#     X_train = train_df.drop('BotOrNot')\n",
    "#     y_train = train_df.select('BotOrNot')\n",
    "#     X_test = test_df.drop('BotOrNot')\n",
    "#     y_test = test_df.select('BotOrNot')\n",
    "    \n",
    "\n",
    "    #checkpoint\n",
    "#     print(len(X_train.collect()), len(y_train.collect()))\n",
    "#     print(len(X_test.collect()), len(y_test.collect()))\n",
    "\n",
    "#     X_train = to_nparray_list(X_train, 'independent_features')\n",
    "#     y_train = to_nparray_list(y_train, 'BotOrNot')\n",
    "#     X_test = to_nparray_list(X_test, 'independent_features')\n",
    "#     y_test = to_nparray_list(y_test, 'BotOrNot')\n",
    "    train_df = train_df.cache()\n",
    "    test_df = test_df.cache()\n",
    "\n",
    "    train_X,  train_Y = to_nparray_dataset(train_df, 'independent_features', 'BotOrNot')\n",
    "    test_X, test_Y = to_nparray_dataset(test_df, 'independent_features', 'BotOrNot')\n",
    "\n",
    "    \n",
    "    return train_X, train_Y, test_X, test_Y # return type: numpy.array\n",
    "\n",
    "\n",
    "def getTrainTestData(df, seed = 21):\n",
    "    train_X, test_X = df.randomSplit([0.7, 0.3], seed)\n",
    "    return train_X, test_X\n",
    "    \n",
    "\n",
    "'''\n",
    "def distributedTrainingGradients(df, feature_column, target_column, n_splits):\n",
    "    print(df.count())\n",
    "    each_len = df.count() // n_splits\n",
    "    gradients = []\n",
    "    ##split dataset into 'n_splits' part\n",
    "    copy_df = df\n",
    "    for i in range(n_splits):\n",
    "        print(i)\n",
    "        temp_df = copy_df.limit(each_len)\n",
    "        copy_df = copy_df.subtract(temp_df)\n",
    "        \n",
    "        X = temp_df.select(feature_column)\n",
    "        Y = temp_df.select(target_column)\n",
    "        X_np = to_nparray_list(X, feature_column)\n",
    "        Y_np = to_nparray_list(Y, target_column)\n",
    "        \n",
    "        grad = step(X_np, Y_np)\n",
    "        gradients.append(grad)\n",
    "        print(temp_df.count())\n",
    "        \n",
    "    return gradients\n",
    "'''\n",
    "\n",
    "def generateGradient(X, Y, bw0, grads):\n",
    "    gd = step(X, Y, bw0, grads)\n",
    "    return gd\n",
    "\n",
    "def step(X, Y, bw0, grads):\n",
    "    print(\"Input count: {}, {}\".format(len(X), len(Y)))\n",
    "    #keep track of gradients\n",
    "    \n",
    "    \n",
    "    \n",
    "    curr_model = get2DenseLayeredModel(38)\n",
    "    #apply previous training gradient\n",
    "    if bw0 is not None:\n",
    "        curr_model.set_weights(bw0)\n",
    "    \n",
    "    if grads is not None:\n",
    "        opt.apply_gradients(zip(grads, curr_model.trainable_variables))\n",
    "    \n",
    "    # gradienttape autometically watch trainable_variable\n",
    "    # curr_model.trainable_variables\n",
    "    # no need for tape.watch(curr_model.trainable_variables)\n",
    "    \n",
    "    with tf.GradientTape() as tape:    \n",
    "        #make a prediction using model\n",
    "        predict = curr_model(X)\n",
    "        #calculate loss\n",
    "        loss = mse(Y, predict)\n",
    "    #calculate the gradient\n",
    "    gd = tape.gradient(loss, curr_model.trainable_variables)\n",
    "    \n",
    "    # return the gradient to train final model\n",
    "    return gd\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "def stepEPOCH(X, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        curr_model = getDLModel()\n",
    "        for i in range(EPOCHS):\n",
    "            #make a prediction using model\n",
    "            predict = curr_model(X)\n",
    "            #calculate loss\n",
    "            loss = cce(y, predict)\n",
    "            print(\"{}: {}\".format(i, loss))\n",
    "            opt.apply_gradients(zip(grad, curr_model.trainable_variables))\n",
    "            \n",
    "    gradient = tape.gradient(loss, curr_model.trainable_variables)\n",
    "    \n",
    "    return gradient\n",
    "\n",
    "'''\n",
    "    \n",
    "\n",
    "## create model\n",
    "def get2DenseLayeredModel(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(500, input_dim=input_dim))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(200))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def model_evaluation(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train,\n",
    "              batch_size=64,\n",
    "              epochs=10,\n",
    "              validation_data=(X_test, y_test))\n",
    "    score, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "\n",
    "    \n",
    "def removeExtraColumn(df, column_names):\n",
    "    if len(df.columns) == 26:\n",
    "        df = remove_column_miss_match(df)\n",
    "    else:\n",
    "        df = set_column_name(df, column_names)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def save_grad(path, fnames, grad):\n",
    "    if fnames is None or grad is None:\n",
    "        return\n",
    "    lists = [gd.numpy() for gd in grad]\n",
    "    \n",
    "    n = len(fnames)\n",
    "    for idx in range(n):\n",
    "        np.save(path+fnames[idx], lists[idx])\n",
    "    \n",
    "    print(\"gradient: save successful\")\n",
    "    \n",
    "def load_grad(path, fnames):\n",
    "    if fnames is None:\n",
    "        return []\n",
    "    grad = []\n",
    "    for fname in fnames:\n",
    "        arr = np.load(path+fname)\n",
    "        tens = tf.constant(arr)\n",
    "        grad.append(tens)\n",
    "    \n",
    "    print(\"gradient: load successful\")\n",
    "    \n",
    "    return grad\n",
    "\n",
    "def add_grad(grad1, grad2):\n",
    "    if grad1 is None:\n",
    "        return grad2\n",
    "    if grad2 is None:\n",
    "        return grad1\n",
    "    \n",
    "    grad = []\n",
    "    n = min(len(grad1), len(grad2))\n",
    "    \n",
    "    for idx in range(n):\n",
    "        grad.append(grad1[idx]+grad2[idx])\n",
    "    \n",
    "    return grad\n",
    "        \n",
    "\n",
    "def worker_task_eval(bot_tweets_df, genuine_tweets_df):\n",
    "   #solve column number issue\n",
    "    bot_tweets_df = removeExtraColumn(bot_tweets_df, BOT_COLUMNS)\n",
    "    genuine_tweets_df = removeExtraColumn(genuine_tweets_df, BOT_COLUMNS)\n",
    "    \n",
    "    \n",
    "    bot_tweets_df = remove_type_miss_match(bot_tweets_df)\n",
    "    genuine_tweets_df = remove_type_miss_match(genuine_tweets_df)\n",
    "    \n",
    "#     print(len(bot_tweets_df.collect()), len(genuine_tweets_df.collect()))\n",
    "    \n",
    "    ##preprocess data\n",
    "    tweets_df = resize_combine_data(bot_tweets_df, genuine_tweets_df)\n",
    "    tweets_df = preprocess_data(tweets_df)\n",
    "    \n",
    "#     print(len(tweets_df.collect()))\n",
    "#     print(tweets_df.columns)\n",
    "    \n",
    "    ##text embedding using GLoVE & LSTM\n",
    "    ## Word Embedding\n",
    "    tweets_df = sentEmbeddingGLoVE_LSTM(tweets_df)\n",
    "    \n",
    "    ## Assable multiple colu,ms to create feature vector\n",
    "    tweets_updated_df = assembleColumns(tweets_df)\n",
    "#     print(len(tweets_updated_df.collect()), tweets_updated_df.columns)\n",
    "\n",
    "#     tweets_updated_df = tweets_updated_df.cache()\n",
    "    \n",
    "    X_test, Y_test = to_nparray_dataset(tweets_updated_df, 'independent_features', 'BotOrNot')\n",
    "    \n",
    "    model = get2DenseLayeredModel(38)\n",
    "    \n",
    "    bw0 = broadcast_w0.value\n",
    "    grads = load_grad(grad_dir, fnames)\n",
    "    \n",
    "    print(\"Final w0: \".format(bw0))\n",
    "    print(\"GD: \".format(grads))\n",
    "    \n",
    "    model.set_weights(bw0)\n",
    "    opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "        \n",
    "    # in order to calculate accuracy using Keras' functions we first need\n",
    "    # to compile the model\n",
    "    model.compile(optimizer= opt, loss=cce, metrics=[\"acc\"])\n",
    "    \n",
    "    \n",
    "    # now that the model is compiled we can compute the accuracy\n",
    "    (loss, acc) = model.evaluate(X_test, Y_test)\n",
    "    print(\"[INFO] test accuracy: {}\".format(acc))\n",
    "    print(\"[INFO] test loss: {}\".format(loss))\n",
    "    \n",
    "    \n",
    "\n",
    "def worker_task(bot_tweets_df, genuine_tweets_df):\n",
    "#     #cache df\n",
    "    bot_tweets_df = bot_tweets_df.cache()\n",
    "    genuine_tweets_df = genuine_tweets_df.cache()\n",
    "    \n",
    "    print(\"#bot_tweets: {} #gen_tweets: {}\".format(bot_tweets_df.count(), genuine_tweets_df.count()))\n",
    "\n",
    "    #solve column number issue\n",
    "    bot_tweets_df = removeExtraColumn(bot_tweets_df, BOT_COLUMNS)\n",
    "    genuine_tweets_df = removeExtraColumn(genuine_tweets_df, BOT_COLUMNS)\n",
    "    \n",
    "    \n",
    "    bot_tweets_df = remove_type_miss_match(bot_tweets_df)\n",
    "    genuine_tweets_df = remove_type_miss_match(genuine_tweets_df)\n",
    "    \n",
    "#     print(len(bot_tweets_df.collect()), len(genuine_tweets_df.collect()))\n",
    "    \n",
    "    ##preprocess data\n",
    "    tweets_df = resize_combine_data(bot_tweets_df, genuine_tweets_df)\n",
    "    tweets_df = preprocess_data(tweets_df)\n",
    "    \n",
    "#     print(len(tweets_df.collect()))\n",
    "#     print(tweets_df.columns)\n",
    "    \n",
    "    ##text embedding using GLoVE & LSTM\n",
    "    ## Word Embedding\n",
    "    tweets_df = sentEmbeddingGLoVE_LSTM(tweets_df)\n",
    "    \n",
    "    ## Assable multiple colu,ms to create feature vector\n",
    "    tweets_updated_df = assembleColumns(tweets_df)\n",
    "#     print(len(tweets_updated_df.collect()), tweets_updated_df.columns)\n",
    "\n",
    "#     tweets_updated_df = tweets_updated_df.cache()\n",
    "    \n",
    "    X, Y = to_nparray_dataset(tweets_updated_df, 'independent_features', 'BotOrNot')\n",
    "    \n",
    "    #load gradient sum in fs/hdfs/s3\n",
    "    grad_sum = load_grad(grad_dir, fnames)\n",
    "    \n",
    "    bw0 = broadcast_w0.value\n",
    "    curr_gd = generateGradient(X, Y, bw0, grad_sum)\n",
    "    \n",
    "    grad_sum = add_grad(curr_gd, grad_sum)\n",
    "    \n",
    "    #save gradient sum in fs/hdfs/s3\n",
    "    save_grad(grad_dir, fnames, grad_sum)\n",
    "    \n",
    "    print(\":: OK\")\n",
    "\n",
    "#\n",
    "def getInitGradient(input_dim = 38):\n",
    "    X = np.array([[0.0]*input_dim], dtype = 'float32')\n",
    "    Y = np.array([[0.0]], dtype = 'float32')\n",
    "    grad = generateGradient(X, Y, None, None)\n",
    "    return grad\n",
    "\n",
    "    \n",
    "# distributed training / adjustment of weights\n",
    "def getAdjustedWeights(weights = None, gradient = None):\n",
    "    if (weights is None):\n",
    "        model = get2DenseLayeredModel(38)\n",
    "        return model.get_weights()\n",
    "    elif (gradient is None):\n",
    "        return weights\n",
    "    else:\n",
    "        model = get2DenseLayeredModel(38)\n",
    "        model.set_weights(weights)\n",
    "        opt.apply_gradients(zip(gradient, model.trainable_variables))\n",
    "        \n",
    "        return model.get_weights()\n",
    "    \n",
    "\n",
    "glove_dict = makeGloveWordEmbedder(GLOVE_DIR + \"glove.twitter.27B.25d.txt\")\n",
    "broadcast_glove_dict = sc.broadcast(glove_dict)\n",
    "\n",
    "w0 = getAdjustedWeights(None, None)\n",
    "broadcast_w0 = sc.broadcast(w0)\n",
    "\n",
    "model_grad0 = getInitGradient(38)\n",
    "save_grad(grad_dir, fnames, model_grad0)\n",
    "\n",
    "\n",
    "\n",
    "# broadcast glove word wmbedder to all task\n",
    "def broadcastData():\n",
    "    print(\"broadcast glove\")\n",
    "    glove_dict = makeGloveWordEmbedder(GLOVE_DIR + \"glove.twitter.27B.25d.txt\")\n",
    "    broadcast_glove_dict = sc.broadcast(glove_dict)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def ApplicationJob():\n",
    "    \n",
    "#     broadcastData()\n",
    "#     accumulateData()\n",
    "    \n",
    "    bot_tweets_df, genuine_tweets_df = read_dataset()\n",
    "    \n",
    "    train_bot_tweet_df, test_bot_tweet_df = bot_tweets_df.randomSplit([0.8, 0.2], seed = 21)\n",
    "    train_genuine_tweets_df, test_genuine_tweets_df = genuine_tweets_df.randomSplit([0.8, 0.2], seed = 21)\n",
    "    \n",
    "#     broadcastGloveDict()\n",
    "    # split dataset for parallel data training\n",
    "    num_of_thread = 10\n",
    "    split_weight = 1.0 / num_of_thread\n",
    "    split_weights = [split_weight] * num_of_thread\n",
    "    bot_dfs = train_bot_tweet_df.randomSplit(split_weights, seed = 71)\n",
    "    gen_dfs = train_genuine_tweets_df.randomSplit(split_weights, seed = 71)\n",
    "    \n",
    "    \n",
    "   \n",
    "    ## run a task for each small model training\n",
    "    for idx in range(num_of_thread):\n",
    "        thread = InheritableThread(target = worker_task, kwargs={'bot_tweets_df': bot_dfs[idx],\n",
    "                                                                 'genuine_tweets_df': gen_dfs[idx]})\n",
    "        thread.start()\n",
    "        thread.join()\n",
    "        \n",
    "    \n",
    "    ## single worker or multiple worker\n",
    "    \n",
    "    # testing model\n",
    "#     worker_task_eval(bot_tweets_df, genuine_tweets_df, gradient)\n",
    "    thread = InheritableThread(target = worker_task_eval, kwargs={'bot_tweets_df': test_bot_tweet_df, \n",
    "                                                                  'genuine_tweets_df': test_genuine_tweets_df})\n",
    "    thread.start()\n",
    "    thread.join()\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ##load dara\n",
    "    ApplicationJob()\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b9bb55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3561e6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.empty([], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b576719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(6.e-45, dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1de02825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.isfile(os.getcwd()+\"\\\\data\\\\\"+\"grad_sum.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4bb5555c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grads.npy'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76156236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\USER\\\\projects\\\\Twitter-Bot-or-Not\\\\data\\\\grads.npy'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()+\"\\\\data\\\\\"+grad_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b0ec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(grad_fname):\n",
    "        grads = np.load(grad_fname) # load gds from s3/hdfs/file\n",
    "        grads = tf.convert_to_tensor(grads, dtype='float32')\n",
    "else:\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
