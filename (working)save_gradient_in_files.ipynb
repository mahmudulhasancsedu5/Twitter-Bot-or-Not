{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eef9e63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries / dependencies\n",
    "import glob\n",
    "from preprocessor import api as tweet_preprocessor\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "from pyspark.sql.functions import col, udf, to_timestamp, lit, to_timestamp, when, rand\n",
    "from pyspark.sql.types import IntegerType, LongType, DoubleType, StringType, ArrayType\n",
    "from pyspark.ml.feature import Normalizer, StandardScaler, MinMaxScaler, VectorAssembler\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.accumulators import AccumulatorParam\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Dense, Input, concatenate, Dropout, Activation, BatchNormalization\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.losses import MeanSquaredError, CategoricalCrossentropy\n",
    "\n",
    "\n",
    "#thread depencency\n",
    "from pyspark import InheritableThread\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "\n",
    "\n",
    "# initialize the number of epochs to train for, batch size, and\n",
    "# initial learning rate\n",
    "EPOCHS = 25\n",
    "BS = 64\n",
    "INIT_LR = 1e-3\n",
    "\n",
    "mse = MeanSquaredError()\n",
    "cce = CategoricalCrossentropy()\n",
    "opt = Adam(learning_rate=INIT_LR)\n",
    "\n",
    "\n",
    "\n",
    "#Dataset location\n",
    "\n",
    "#Local\n",
    "bot_tweets_dataset_path = 'F://TwitterBotDataset//tweet_dataset_full//bot_tweets//'\n",
    "genuine_tweets_dataset_path = 'F://TwitterBotDataset//tweet_dataset_full//genuine_tweets//'\n",
    "\n",
    "#S3\n",
    "# bot_tweets_dataset_path = 's3://tweet-dataset/bot_tweets' #'F://TwitterBotDataset//tweet_dataset_small//bot_tweets//'\n",
    "# genuine_tweets_dataset_path = 's3://tweet-dataset/genuine_tweets' #'F://TwitterBotDataset//tweet_dataset_small//genuine_tweets//'\n",
    "\n",
    "#turn a line of text into d dimentional vector \n",
    "GLOVE_DIR = \"\"\n",
    "grad_fname = 'grad_sum.txt'\n",
    "\n",
    "#all columns\n",
    "BOT_COLUMNS = ['id','text','source','user_id','truncated','in_reply_to_status_id', \n",
    "               'in_reply_to_user_id','in_reply_to_screen_name', 'retweeted_status_id',\n",
    "               'geo','place','contributors','retweet_count', 'reply_count','favorite_count',\n",
    "               'favorited', 'retweeted','possibly_sensitive','num_hashtags','num_urls',\n",
    "               'num_mentions','created_at','timestamp','crawled_at', 'updated']\n",
    "\n",
    "GENUINE_COLUMNS = ['id','text','source','user_id','truncated','in_reply_to_status_id', \n",
    "                   'in_reply_to_user_id','in_reply_to_screen_name', 'retweeted_status_id',\n",
    "                   'geo','place','contributors','retweet_count', 'reply_count','favorite_count',\n",
    "                   'favorited', 'retweeted','possibly_sensitive','num_hashtags','num_urls',\n",
    "                   'num_mentions','REMOVE_IT', 'created_at','timestamp','crawled_at', 'updated',]\n",
    "\n",
    "#feature used for bot detection\n",
    "COLUMN_NAMES = ['text', 'retweet_count', 'reply_count', 'favorite_count',\n",
    "                'num_hashtags', 'num_urls', 'num_mentions']\n",
    "\n",
    "\n",
    "# #configure spark\n",
    "# conf = SparkConf()\n",
    "# conf.setMaster(\"local[8]\").setAppName(\"ml_account_ base_session\")\n",
    "# conf.set(\"spark.executor.instances\", 4)\n",
    "# conf.set(\"spark.executor.cores\", 4)\n",
    "# conf.set(\"spark.driver.memory\", 4)\n",
    "# sc = SparkContext(conf=conf)\n",
    "\n",
    "# # for spark-submit\n",
    "# spark = SparkSession.builder.appName('ml_account_ base_session').getOrCreate()\n",
    "# spark\n",
    "\n",
    "# for local build\n",
    "# spark = SparkSession.builder.appName('ml_account_ base_session').getOrCreate()\n",
    "\n",
    "\n",
    "#for local multi thread\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[10]\").setAppName(\"distributed_training_session\")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# read dataset from csv\n",
    "def read_dataset():\n",
    "    bot_tweets = spark.read.csv(bot_tweets_dataset_path, header = True, inferSchema = True).limit(1000)\n",
    "    genuine_tweets = spark.read.csv(genuine_tweets_dataset_path, header = True, inferSchema = True).limit(1000)\n",
    "    \n",
    "    #df = inputDF.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "#     bot_tweets = bot_tweets.persist(StorageLevel.MEMORY_ONLY)\n",
    "#     genuine_tweets = genuine_tweets.persist(StorageLevel.MEMORY_ONLY)\n",
    "    \n",
    "#     print(len(bot_tweets.collect()), len(genuine_tweets.collect()))\n",
    "    return bot_tweets, genuine_tweets\n",
    "\n",
    "def set_column_name(df, column_names):\n",
    "    df = df.toDF(*column_names)\n",
    "#     print(len(df.collect()))\n",
    "    return df\n",
    "\n",
    "def remove_column_miss_match(df):\n",
    "    ## dataset have diffrent number of columns\n",
    "    ## column name of dataframe\n",
    "    column_name = [cname for cname, tp in df.dtypes]\n",
    "#     len(df.collect()), len(df.dtypes)\n",
    "    #column_name\n",
    "\n",
    "    #Number of column is diffrent for bot and genuine tweets data\n",
    "\n",
    "    #genuine_tweets_df = genuine_tweets_df.toDF(*column_name)\n",
    "    df = set_column_name(df, GENUINE_COLUMNS)\n",
    "#     print(len(df.collect()))\n",
    "    \n",
    "    df = df.drop('REMOVE_IT') # remove 5th column from end\n",
    "    #update column name according to \n",
    "    df = set_column_name(df, BOT_COLUMNS)\n",
    "#     print(len(df.collect()))\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_type_miss_match(df):\n",
    "    # Same column has diffrent data type. So make data type same for every column\n",
    "    genuine_tweets_df = df.withColumn(\"id\",col(\"id\").cast(IntegerType())) \\\n",
    "                                    .withColumn(\"favorite_count\",col(\"favorite_count\").cast(LongType())) \\\n",
    "                                    .withColumn(\"favorited\",col(\"favorited\").cast(IntegerType()))\n",
    "    return df\n",
    "\n",
    "\n",
    "def resize_combine_data(bot_tweets_df, genuine_tweets_df):\n",
    "    ## only keep the required column from the dataframe\n",
    "    bot_tweets_df = bot_tweets_df.select(*COLUMN_NAMES)\n",
    "    genuine_tweets_df = genuine_tweets_df.select(*COLUMN_NAMES)\n",
    "    \n",
    "#     print(len(bot_tweets_df.collect()), len(genuine_tweets_df.collect()))\n",
    "\n",
    "    ## add BotOrNot column\n",
    "    bot_tweets_df = bot_tweets_df.withColumn('BotOrNot', lit(1))\n",
    "    genuine_tweets_df = genuine_tweets_df.withColumn('BotOrNot', lit(0))\n",
    "\n",
    "    #combine clean and bot accounts data togather\n",
    "    tweets_df = bot_tweets_df.union(genuine_tweets_df)\n",
    "\n",
    "    # shuffle dataset\n",
    "    tweets_df = tweets_df.orderBy(rand())\n",
    "\n",
    "#     print(len(tweets_df.collect()))\n",
    "    \n",
    "    return tweets_df\n",
    "\n",
    "text_process_udf = udf(lambda x : tweet_preprocessor.tokenize(x), StringType())\n",
    "def preprocess_data(df):\n",
    "    df = df.withColumn('text', text_process_udf(df.text))\n",
    "    df = df.withColumn(\"retweet_count\",col(\"retweet_count\").cast(DoubleType()))\n",
    "    df = df.withColumn(\"reply_count\",col(\"reply_count\").cast(DoubleType()))\n",
    "    df = df.withColumn(\"favorite_count\",col(\"favorite_count\").cast(DoubleType()))\n",
    "    df = df.withColumn(\"num_hashtags\",col(\"num_hashtags\").cast(DoubleType()))\n",
    "    df = df.withColumn(\"num_urls\",col(\"num_urls\").cast(DoubleType()))\n",
    "    df = df.withColumn(\"num_mentions\",col(\"num_mentions\").cast(DoubleType()))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def makeGloveWordEmbedder(glove_path):\n",
    "    embedding_dict = {}\n",
    "    with open(glove_path, 'r', encoding=\"utf-8\") as glove_file:\n",
    "        for line in glove_file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embedding_dict[word] = vector\n",
    "            \n",
    "    return embedding_dict  \n",
    "\n",
    "\n",
    "# # Test GLoVE result\n",
    "# glove_word2vec_embedder[\"google\"]\n",
    "\n",
    " ##create word embedding GLoVE model dictionary. Use pre trained model\n",
    "text_feature_dimention = 25\n",
    "# glove_word2vec_embedder = makeGloveWordEmbedder(GLOVE_DIR + \"glove.twitter.27B.25d.txt\")\n",
    "\n",
    "\n",
    "#Give a word and get that word representing feature vector of dimention 25 from embedding dictionary\n",
    "def word2vec(word_dict=None, word=None, dim=25):\n",
    "    default_vector = np.zeros(dim)\n",
    "    \n",
    "    if word_dict is None or word is None:\n",
    "        return default_vector\n",
    "    \n",
    "    word_vector = word_dict.get(word)\n",
    "    \n",
    "    if word_vector is None:\n",
    "        return default_vector\n",
    "    return word_vector\n",
    "\n",
    "# # test a word representing feature vector\n",
    "# word_vector = word2vec(glove_word2vec_embedder, \"tweet\", text_feature_dimention)\n",
    "# print(type(word_vector), word_vector)\n",
    "\n",
    "\n",
    "#----------------LSTM Model----------------------\n",
    "\n",
    "#create a 1 layer LSTM model\n",
    "#input dimention: 3d (1,7,25) input_sample = [[[1,...,25]],..,[1,...,25]]\n",
    "#output dimention: 1d (1,32) output_sample [[1,2,3,,,,32]]\n",
    "def lstm_model(output_dim):\n",
    "    model = LSTM(output_dim, return_sequences=False, return_state=False)\n",
    "    return model\n",
    "\n",
    "def reset_lstm_model(model):\n",
    "    model.resate_states() # stateful=True is required for reset states\n",
    "\n",
    "# create LSTM model of output dimention 32. Model output feature vector will be of 32 dimention vector\n",
    "lstm = lstm_model(32) \n",
    "\n",
    "# convver a sentence to a feature vector using LSTM(RNN) model\n",
    "def sent2vec(sent):\n",
    "    words = sent.split(' ')\n",
    "    word_vectors = np.array([])\n",
    "    count = 0;\n",
    "    for word in words:\n",
    "        word_vector = word2vec(broadcast_glove_dict.value, word)\n",
    "#         print(\"word dim: {}\".format(len(word_vector)))\n",
    "        if word_vectors.size == 0:\n",
    "            word_vectors = np.array([word_vector])\n",
    "        else:\n",
    "            word_vectors = np.vstack([word_vectors, word_vector])\n",
    "        count = count + 1\n",
    "    \n",
    "#     print(\"Input feature vector shape before reshape(2D): {}\".format(word_vectors.shape))\n",
    "        \n",
    "    input_feature_vectors = np.reshape(word_vectors, (1, count, text_feature_dimention))\n",
    "#     print(\"Input feature vector shape after reshape(3d): {}\".format(input_feature_vectors.shape))\n",
    "#     print(\"LSTM requirs 3d shape inputs [batch, timesteps, feature]\")\n",
    "    output_vector = lstm(input_feature_vectors)\n",
    "#     lstm.reset_states() # stateful = True is required for reset\n",
    "\n",
    "#     print(\"result vector shape: {}\".format(output_vector.shape))\n",
    "#     print(\"Last input was: {}\".format(input_feature_vectors[0][-1]))\n",
    "#     print(\"output result: {}\".format(output_vector))\n",
    "    \n",
    "    # (tensore --> numpy 0bject --> numpy.array --> array/list/ArrayType)\n",
    "    return output_vector.numpy()[0].tolist() \n",
    "    \n",
    "## For Testing sentence to vector convertion\n",
    "# sent = \"Twitter is a large social media network\"\n",
    "# res_vector = sent2vec(sent)\n",
    "# type(res_vector), res_vector\n",
    "\n",
    "\n",
    "# text string --> vector 32 dimention\n",
    "sent_to_vector_udf = udf(lambda x : sent2vec(x), ArrayType(DoubleType()))\n",
    "def processTextColumn(df, column_name, new_column_name):\n",
    "    df = df.withColumn(new_column_name, sent_to_vector_udf(col(column_name)))\n",
    "    return df\n",
    "\n",
    "def sentEmbeddingGLoVE_LSTM(df):\n",
    "    \n",
    "    text_updated_column = 'text_features'\n",
    "    updated_df = processTextColumn(df, \"text\", text_updated_column)\n",
    "\n",
    "#     print(len(updated_df.collect()), type(updated_df), updated_df.printSchema()) \n",
    "    \n",
    "    return updated_df\n",
    "\n",
    "\n",
    "\n",
    "def assembleColumns(tweets_df):\n",
    "    columns = ['retweet_count', 'reply_count', 'favorite_count',\n",
    "               'num_hashtags' ,'num_urls', 'num_mentions', 'BotOrNot']\n",
    "\n",
    "    tweets_df = tweets_df.select(*columns, \n",
    "                          tweets_df.text_features[0], tweets_df.text_features[1], tweets_df.text_features[2],tweets_df.text_features[3], tweets_df.text_features[4],\n",
    "                          tweets_df.text_features[5], tweets_df.text_features[6], tweets_df.text_features[7],tweets_df.text_features[8], tweets_df.text_features[9], \n",
    "                          tweets_df.text_features[10], tweets_df.text_features[11], tweets_df.text_features[12],tweets_df.text_features[13], tweets_df.text_features[14],\n",
    "                          tweets_df.text_features[15], tweets_df.text_features[16], tweets_df.text_features[17],tweets_df.text_features[18], tweets_df.text_features[19],\n",
    "                          tweets_df.text_features[20], tweets_df.text_features[21], tweets_df.text_features[22],tweets_df.text_features[23], tweets_df.text_features[24],\n",
    "                          tweets_df.text_features[25], tweets_df.text_features[26], tweets_df.text_features[27],tweets_df.text_features[28], tweets_df.text_features[29],\n",
    "                          tweets_df.text_features[30], tweets_df.text_features[31])\n",
    "\n",
    "\n",
    "#     print(tweets_df.columns, len(tweets_df.collect()), tweets_df.printSchema())\n",
    "\n",
    "    #remove \n",
    "\n",
    "    feature_columns = ['retweet_count','reply_count','favorite_count','num_hashtags','num_urls','num_mentions',\n",
    "                       'text_features[0]','text_features[1]', 'text_features[2]','text_features[3]','text_features[4]',\n",
    "                       'text_features[5]','text_features[6]','text_features[7]', 'text_features[8]','text_features[9]',\n",
    "                       'text_features[10]','text_features[11]','text_features[12]','text_features[13]','text_features[14]',\n",
    "                       'text_features[15]','text_features[16]','text_features[17]','text_features[18]','text_features[19]',\n",
    "                       'text_features[20]','text_features[21]','text_features[22]', 'text_features[23]', 'text_features[24]',\n",
    "                       'text_features[25]','text_features[26]','text_features[27]', 'text_features[28]', 'text_features[29]',\n",
    "                       'text_features[30]','text_features[31]']\n",
    "\n",
    "\n",
    "    tweets_df = tweets_df.na.fill(value=0.0 ,subset= feature_columns)\n",
    "    feature_assembler = VectorAssembler(inputCols = feature_columns, outputCol = 'independent_features')\n",
    "\n",
    "    tweets_updated_df = feature_assembler.transform(tweets_df)\n",
    "\n",
    "    #check\n",
    "#     num = len(tweets_updated_df.collect())\n",
    "#     print(num, type(tweets_updated_df), tweets_updated_df.printSchema())\n",
    "\n",
    "    #remove unnecessary columns\n",
    "    tweets_updated_df = tweets_updated_df.drop(*feature_columns)\n",
    "    \n",
    "    return tweets_updated_df\n",
    "\n",
    "\n",
    "def to_nparray_list(df, column_name):\n",
    "    rows = df.select(column_name).collect()\n",
    "    lists = [x[column_name] for x in rows]\n",
    "    nparr = np.array(lists)\n",
    "    \n",
    "    return nparr\n",
    "\n",
    "def to_nparray_dataset(df, feature_column, target_column):\n",
    "#     list(df.select('col_name').toPandas()['col_name']) \n",
    "#     feature = list(df.select(feature_column).toPandas()[feature_column])\n",
    "#     target = list(df.select(target_column).toPandas()[target_column])\n",
    "    feature = [row[0] for row in list(df.select(feature_column).toLocalIterator())]\n",
    "    target = [row[0] for row in list(df.select(target_column).toLocalIterator())]\n",
    "        \n",
    "    return np.array(feature), np.array(target)    \n",
    "\n",
    "def partition_dataset(df):\n",
    "    train_df, test_df = df.randomSplit([0.80, 0.20])\n",
    "#     print(len(train_df.collect()), len(test_df.collect()))\n",
    "\n",
    "    # features --> 'BotOrNot'\n",
    "#     X_train = train_df.drop('BotOrNot')\n",
    "#     y_train = train_df.select('BotOrNot')\n",
    "#     X_test = test_df.drop('BotOrNot')\n",
    "#     y_test = test_df.select('BotOrNot')\n",
    "    \n",
    "\n",
    "    #checkpoint\n",
    "#     print(len(X_train.collect()), len(y_train.collect()))\n",
    "#     print(len(X_test.collect()), len(y_test.collect()))\n",
    "\n",
    "#     X_train = to_nparray_list(X_train, 'independent_features')\n",
    "#     y_train = to_nparray_list(y_train, 'BotOrNot')\n",
    "#     X_test = to_nparray_list(X_test, 'independent_features')\n",
    "#     y_test = to_nparray_list(y_test, 'BotOrNot')\n",
    "    train_df = train_df.cache()\n",
    "    test_df = test_df.cache()\n",
    "\n",
    "    train_X,  train_Y = to_nparray_dataset(train_df, 'independent_features', 'BotOrNot')\n",
    "    test_X, test_Y = to_nparray_dataset(test_df, 'independent_features', 'BotOrNot')\n",
    "\n",
    "    \n",
    "    return train_X, train_Y, test_X, test_Y # return type: numpy.array\n",
    "\n",
    "\n",
    "def getTrainTestData(df, seed = 21):\n",
    "    train_X, test_X = df.randomSplit([0.7, 0.3], seed)\n",
    "    return train_X, test_X\n",
    "    \n",
    "\n",
    "'''\n",
    "def distributedTrainingGradients(df, feature_column, target_column, n_splits):\n",
    "    print(df.count())\n",
    "    each_len = df.count() // n_splits\n",
    "    gradients = []\n",
    "    ##split dataset into 'n_splits' part\n",
    "    copy_df = df\n",
    "    for i in range(n_splits):\n",
    "        print(i)\n",
    "        temp_df = copy_df.limit(each_len)\n",
    "        copy_df = copy_df.subtract(temp_df)\n",
    "        \n",
    "        X = temp_df.select(feature_column)\n",
    "        Y = temp_df.select(target_column)\n",
    "        X_np = to_nparray_list(X, feature_column)\n",
    "        Y_np = to_nparray_list(Y, target_column)\n",
    "        \n",
    "        grad = step(X_np, Y_np)\n",
    "        gradients.append(grad)\n",
    "        print(temp_df.count())\n",
    "        \n",
    "    return gradients\n",
    "'''\n",
    "\n",
    "def generateGradient(X, Y, bw0, grads):\n",
    "    gd = step(X, Y, bw0, grads)\n",
    "    return gd\n",
    "\n",
    "def step(X, Y, bw0, grads):\n",
    "    print(\"Input count: {}, {}\".format(len(X), len(Y)))\n",
    "    #keep track of gradients\n",
    "    \n",
    "    \n",
    "    \n",
    "    curr_model = get2DenseLayeredModel(38)\n",
    "    #apply previous training gradient\n",
    "    if bw0 is not None:\n",
    "        curr_model.set_weights(bw0)\n",
    "    \n",
    "    if grads is not None:\n",
    "        opt.apply_gradients(zip(grads, curr_model.trainable_variables))\n",
    "    \n",
    "    # gradienttape autometically watch trainable_variable\n",
    "    # curr_model.trainable_variables\n",
    "    # no need for tape.watch(curr_model.trainable_variables)\n",
    "    \n",
    "    with tf.GradientTape() as tape:    \n",
    "        #make a prediction using model\n",
    "        predict = curr_model(X)\n",
    "        #calculate loss\n",
    "        loss = mse(Y, predict)\n",
    "    #calculate the gradient\n",
    "    gd = tape.gradient(loss, curr_model.trainable_variables)\n",
    "    \n",
    "    # return the gradient to train final model\n",
    "    return gd\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "def stepEPOCH(X, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        curr_model = getDLModel()\n",
    "        for i in range(EPOCHS):\n",
    "            #make a prediction using model\n",
    "            predict = curr_model(X)\n",
    "            #calculate loss\n",
    "            loss = cce(y, predict)\n",
    "            print(\"{}: {}\".format(i, loss))\n",
    "            opt.apply_gradients(zip(grad, curr_model.trainable_variables))\n",
    "            \n",
    "    gradient = tape.gradient(loss, curr_model.trainable_variables)\n",
    "    \n",
    "    return gradient\n",
    "\n",
    "'''\n",
    "    \n",
    "\n",
    "## create model\n",
    "def get2DenseLayeredModel(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(500, input_dim=input_dim))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(200))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def model_evaluation(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train,\n",
    "              batch_size=64,\n",
    "              epochs=10,\n",
    "              validation_data=(X_test, y_test))\n",
    "    score, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "\n",
    "    \n",
    "def removeExtraColumn(df, column_names):\n",
    "    if len(df.columns) == 26:\n",
    "        df = remove_column_miss_match(df)\n",
    "    else:\n",
    "        df = set_column_name(df, column_names)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def worker_task_eval(bot_tweets_df, genuine_tweets_df):\n",
    "   #solve column number issue\n",
    "    bot_tweets_df = removeExtraColumn(bot_tweets_df, BOT_COLUMNS)\n",
    "    genuine_tweets_df = removeExtraColumn(genuine_tweets_df, BOT_COLUMNS)\n",
    "    \n",
    "    \n",
    "    bot_tweets_df = remove_type_miss_match(bot_tweets_df)\n",
    "    genuine_tweets_df = remove_type_miss_match(genuine_tweets_df)\n",
    "    \n",
    "#     print(len(bot_tweets_df.collect()), len(genuine_tweets_df.collect()))\n",
    "    \n",
    "    ##preprocess data\n",
    "    tweets_df = resize_combine_data(bot_tweets_df, genuine_tweets_df)\n",
    "    tweets_df = preprocess_data(tweets_df)\n",
    "    \n",
    "#     print(len(tweets_df.collect()))\n",
    "#     print(tweets_df.columns)\n",
    "    \n",
    "    ##text embedding using GLoVE & LSTM\n",
    "    ## Word Embedding\n",
    "    tweets_df = sentEmbeddingGLoVE_LSTM(tweets_df)\n",
    "    \n",
    "    ## Assable multiple colu,ms to create feature vector\n",
    "    tweets_updated_df = assembleColumns(tweets_df)\n",
    "#     print(len(tweets_updated_df.collect()), tweets_updated_df.columns)\n",
    "\n",
    "#     tweets_updated_df = tweets_updated_df.cache()\n",
    "    \n",
    "    X_test, Y_test = to_nparray_dataset(tweets_updated_df, 'independent_features', 'BotOrNot')\n",
    "    \n",
    "    model = get2DenseLayeredModel(38)\n",
    "    \n",
    "    bw0 = broadcast_w0.value\n",
    "    grads = np.load(grad_fname)\n",
    "    grads = tf.convert_to_tensor(grads, dtype='float32')\n",
    "    \n",
    "    opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "        \n",
    "    # in order to calculate accuracy using Keras' functions we first need\n",
    "    # to compile the model\n",
    "    model.compile(optimizer= opt, loss=cce, metrics=[\"acc\"])\n",
    "    \n",
    "    \n",
    "    # now that the model is compiled we can compute the accuracy\n",
    "    (loss, acc) = model.evaluate(X_test, Y_test)\n",
    "    print(\"[INFO] test accuracy: {}\".format(acc))\n",
    "    print(\"[INFO] test loss: {}\".format(loss))\n",
    "    \n",
    "    \n",
    "\n",
    "def worker_task(bot_tweets_df, genuine_tweets_df):\n",
    "#     #cache df\n",
    "    bot_tweets_df = bot_tweets_df.cache()\n",
    "    genuine_tweets_df = genuine_tweets_df.cache()\n",
    "    \n",
    "    print(\"#bot_tweets: {} #gen_tweets: {}\".format(bot_tweets_df.count(), genuine_tweets_df.count()))\n",
    "    \n",
    "    ##clean data / remove unwanted column\n",
    "#     if len(bot_tweets_df.columns) == 26:\n",
    "#         bot_tweets_df = remove_column_miss_match(bot_tweets_df)\n",
    "#     else:\n",
    "#         bot_tweets_df = set_column_name(bot_tweets_df, BOT_COLUMNS)\n",
    "        \n",
    "#     if len(genuine_tweets_df.columns) == 26:\n",
    "#         genuine_tweets_df = remove_column_miss_match(genuine_tweets_df)\n",
    "#     else:\n",
    "#         genuine_tweets_df = set_column_name(genuine_tweets_df, BOT_COLUMNS)\n",
    "\n",
    "    #solve column number issue\n",
    "    bot_tweets_df = removeExtraColumn(bot_tweets_df, BOT_COLUMNS)\n",
    "    genuine_tweets_df = removeExtraColumn(genuine_tweets_df, BOT_COLUMNS)\n",
    "    \n",
    "    \n",
    "    bot_tweets_df = remove_type_miss_match(bot_tweets_df)\n",
    "    genuine_tweets_df = remove_type_miss_match(genuine_tweets_df)\n",
    "    \n",
    "#     print(len(bot_tweets_df.collect()), len(genuine_tweets_df.collect()))\n",
    "    \n",
    "    ##preprocess data\n",
    "    tweets_df = resize_combine_data(bot_tweets_df, genuine_tweets_df)\n",
    "    tweets_df = preprocess_data(tweets_df)\n",
    "    \n",
    "#     print(len(tweets_df.collect()))\n",
    "#     print(tweets_df.columns)\n",
    "    \n",
    "    ##text embedding using GLoVE & LSTM\n",
    "    ## Word Embedding\n",
    "    tweets_df = sentEmbeddingGLoVE_LSTM(tweets_df)\n",
    "    \n",
    "    ## Assable multiple colu,ms to create feature vector\n",
    "    tweets_updated_df = assembleColumns(tweets_df)\n",
    "#     print(len(tweets_updated_df.collect()), tweets_updated_df.columns)\n",
    "\n",
    "#     tweets_updated_df = tweets_updated_df.cache()\n",
    "    \n",
    "    X, Y = to_nparray_dataset(tweets_updated_df, 'independent_features', 'BotOrNot')\n",
    "    \n",
    "    grads = None\n",
    "    # need more testing\n",
    "    if os.path.isfile(grad_fname):\n",
    "        grads = np.load(grad_fname) # load gds from s3/hdfs/file\n",
    "        grads = tf.convert_to_tensor(grads, dtype='float32')\n",
    "    print(\"GD tensor: {}\".format(grads))\n",
    "    \n",
    "    bw0 = broadcast_w0.value\n",
    "    curr_gd = generateGradient(X, Y, bw0, grads)\n",
    "    print(\"curr_gd: {}\".format(curr_gd))\n",
    "    \n",
    "    if grads is not None:\n",
    "        grads = grads + curr_gd\n",
    "    else:\n",
    "        grads = curr_gd\n",
    "    \n",
    "    \n",
    "    # ------- Need to save gradient sum in hdfs/s3 files----------\n",
    "    \n",
    "    np.save(grad_fname, tf.make_ndarray(grads))\n",
    "#     print(\"Curr gd: {}\".format(curr_gd))\n",
    "    print(\":: OK\")\n",
    "\n",
    "\n",
    "# distributed training / adjustment of weights\n",
    "def getAdjustedWeights(weights = None, gradient = None):\n",
    "    if (weights is None):\n",
    "        model = get2DenseLayeredModel(38)\n",
    "        return model.get_weights()\n",
    "    elif (gradient is None):\n",
    "        return weights\n",
    "    else:\n",
    "        model = get2DenseLayeredModel(38)\n",
    "        model.set_weights(weights)\n",
    "        opt.apply_gradients(zip(gradient, model.trainable_variables))\n",
    "        \n",
    "        return model.get_weights()\n",
    "    \n",
    "\n",
    "glove_dict = makeGloveWordEmbedder(GLOVE_DIR + \"glove.twitter.27B.25d.txt\")\n",
    "broadcast_glove_dict = sc.broadcast(glove_dict)\n",
    "\n",
    "w0 = getAdjustedWeights(None, None)\n",
    "broadcast_w0 = sc.broadcast(w0)\n",
    "\n",
    "\n",
    "\n",
    "# broadcast glove word wmbedder to all task\n",
    "def broadcastData():\n",
    "    print(\"broadcast glove\")\n",
    "    glove_dict = makeGloveWordEmbedder(GLOVE_DIR + \"glove.twitter.27B.25d.txt\")\n",
    "    broadcast_glove_dict = sc.broadcast(glove_dict)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def ApplicationJob():\n",
    "    \n",
    "#     broadcastData()\n",
    "#     accumulateData()\n",
    "    \n",
    "    bot_tweets_df, genuine_tweets_df = read_dataset()\n",
    "    \n",
    "    train_bot_tweet_df, test_bot_tweet_df = bot_tweets_df.randomSplit([0.8, 0.2], seed = 21)\n",
    "    train_genuine_tweets_df, test_genuine_tweets_df = genuine_tweets_df.randomSplit([0.8, 0.2], seed = 21)\n",
    "    \n",
    "#     broadcastGloveDict()\n",
    "    # split dataset for parallel data training\n",
    "    num_of_thread = 10\n",
    "    split_weight = 1.0 / num_of_thread\n",
    "    split_weights = [split_weight] * num_of_thread\n",
    "    bot_dfs = train_bot_tweet_df.randomSplit(split_weights, seed = 71)\n",
    "    gen_dfs = train_genuine_tweets_df.randomSplit(split_weights, seed = 71)\n",
    "    \n",
    "    model_weights = getAdjustedWeights(None, None)\n",
    "    ## run a task for each small model training\n",
    "    for idx in range(num_of_thread):\n",
    "        thread = InheritableThread(target = worker_task, kwargs={'bot_tweets_df': bot_dfs[idx],\n",
    "                                                                 'genuine_tweets_df': gen_dfs[idx]})\n",
    "        thread.start()\n",
    "        thread.join()\n",
    "        \n",
    "    \n",
    "    ## single worker or multiple worker\n",
    "    \n",
    "    # testing model\n",
    "#     worker_task_eval(bot_tweets_df, genuine_tweets_df, gradient)\n",
    "    thread = InheritableThread(target = worker_task_eval, kwargs={'bot_tweets_df': test_bot_tweet_df, \n",
    "                                                                  'genuine_tweets_df': test_genuine_tweets_df})\n",
    "    thread.start()\n",
    "    thread.join()\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     ##load dara\n",
    "#     ApplicationJob()\n",
    "#     spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1506566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = tf.constant([1,2,3,4,5] , dtype = tf.int32)\n",
    "# b = tf.constant([53.44569] , dtype= tf.float32)\n",
    "# c = tf.constant(0)\n",
    "# # if u wish to write all these tensors on each line ,\n",
    "# # then create a single string out of these.\n",
    "# one_string = tf.strings.format(\"{}\\n{}\\n{}\\n\", (a,b,c))\n",
    "# # {} is a placeholder for each element ina string and thus you would need n PH for n tensors.\n",
    "# # send this string to write_file fn\n",
    "# tf.io.write_file(filename, one_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c891b681",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([[0]*500]*38, dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e427c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(38, 500), dtype=float32, numpy=\n",
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b5dc238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[0.0]*38], dtype = 'float32')\n",
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93567519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.array([[0.0]], dtype = 'float32')\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c380a16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInitGradient(input_dim = 38):\n",
    "    X = np.array([[0.0]*input_dim], dtype = 'float32')\n",
    "    Y = np.array([[0.0]], dtype = 'float32')\n",
    "    grad = generateGradient(X, Y, None, None)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279338f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b240049f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input count: 1, 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(38, 500), dtype=float32, numpy=\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(500,), dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.], dtype=float32)>,\n",
       " <tf.Tensor: shape=(500, 200), dtype=float32, numpy=\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(200,), dtype=float32, numpy=\n",
       " array([-0.,  0., -0., -0.,  0.,  0., -0., -0., -0.,  0., -0., -0.,  0.,\n",
       "        -0.,  0.,  0., -0., -0.,  0., -0., -0., -0.,  0., -0., -0.,  0.,\n",
       "         0., -0.,  0.,  0., -0., -0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n",
       "         0., -0.,  0., -0.,  0.,  0.,  0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0.,  0.,  0., -0., -0., -0.,  0.,  0., -0., -0.,  0., -0.,\n",
       "        -0.,  0.,  0., -0., -0., -0., -0.,  0., -0., -0.,  0., -0., -0.,\n",
       "         0.,  0., -0.,  0.,  0., -0., -0., -0.,  0., -0., -0.,  0.,  0.,\n",
       "         0., -0.,  0., -0.,  0.,  0.,  0.,  0.,  0., -0.,  0., -0.,  0.,\n",
       "         0.,  0., -0., -0.,  0.,  0., -0.,  0., -0.,  0.,  0.,  0.,  0.,\n",
       "        -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0., -0.,  0.,  0., -0.,\n",
       "         0., -0.,  0., -0., -0.,  0., -0.,  0., -0.,  0., -0.,  0., -0.,\n",
       "         0.,  0., -0.,  0.,  0.,  0., -0.,  0., -0.,  0., -0., -0., -0.,\n",
       "        -0., -0.,  0.,  0.,  0.,  0., -0.,  0., -0.,  0., -0., -0., -0.,\n",
       "         0.,  0., -0.,  0.,  0.,  0., -0., -0.,  0.,  0.,  0.,  0., -0.,\n",
       "        -0., -0., -0.,  0.,  0.,  0., -0.,  0., -0.,  0.,  0., -0., -0.,\n",
       "        -0., -0.,  0., -0.,  0.], dtype=float32)>,\n",
       " <tf.Tensor: shape=(200, 1), dtype=float32, numpy=\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.25], dtype=float32)>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad = getInitGradient(38)\n",
    "grad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ca53d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(38, 500), dtype=float32, numpy=\n",
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.add(grad[0],grad[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "298a7309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initWeight(input_dim = 38):\n",
    "    model = get2DenseLayeredModel(input_dim)\n",
    "    return model.get_weights()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60bafcca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.09880762, -0.07092009,  0.07138122, ...,  0.0017737 ,\n",
       "          0.0116813 ,  0.09298375],\n",
       "        [-0.03497412,  0.06734241,  0.07249802, ..., -0.05141531,\n",
       "         -0.04421047,  0.06866964],\n",
       "        [-0.00650023, -0.0164499 ,  0.04986086, ..., -0.04742328,\n",
       "         -0.09778465,  0.09048796],\n",
       "        ...,\n",
       "        [ 0.08723182,  0.05994258, -0.08577767, ..., -0.07094417,\n",
       "         -0.05482207,  0.02648393],\n",
       "        [-0.0790548 , -0.01575251, -0.05221164, ...,  0.0719097 ,\n",
       "          0.03366899,  0.0391947 ],\n",
       "        [ 0.05594756, -0.09232375, -0.02462843, ...,  0.06000019,\n",
       "         -0.02281345,  0.03888714]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[-0.05911882,  0.01969384,  0.02430113, ...,  0.05328954,\n",
       "         -0.06601989,  0.07930558],\n",
       "        [-0.08110108,  0.04859007, -0.0497349 , ..., -0.07098857,\n",
       "          0.05990077,  0.0722105 ],\n",
       "        [-0.04240765,  0.08095395, -0.0214738 , ...,  0.0094682 ,\n",
       "          0.05460788, -0.07059686],\n",
       "        ...,\n",
       "        [-0.03856368,  0.00500154, -0.05628068, ...,  0.00642464,\n",
       "         -0.03974551, -0.06675264],\n",
       "        [-0.01474788, -0.01827031,  0.02205327, ..., -0.08654051,\n",
       "          0.02514625, -0.00672807],\n",
       "        [ 0.00334498,  0.08290442, -0.00770973, ...,  0.07206406,\n",
       "         -0.08572539,  0.01139241]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[-0.10551588],\n",
       "        [ 0.1571948 ],\n",
       "        [ 0.15180138],\n",
       "        [-0.0711949 ],\n",
       "        [ 0.09516481],\n",
       "        [ 0.00045983],\n",
       "        [ 0.06560752],\n",
       "        [ 0.11989564],\n",
       "        [ 0.11061409],\n",
       "        [-0.16706166],\n",
       "        [-0.15650696],\n",
       "        [-0.04151632],\n",
       "        [ 0.07942095],\n",
       "        [ 0.01551604],\n",
       "        [-0.01638517],\n",
       "        [ 0.1610722 ],\n",
       "        [-0.01362132],\n",
       "        [ 0.13373095],\n",
       "        [-0.01836973],\n",
       "        [ 0.09791872],\n",
       "        [-0.13162132],\n",
       "        [-0.14498794],\n",
       "        [-0.02204245],\n",
       "        [ 0.05554712],\n",
       "        [-0.09744088],\n",
       "        [ 0.12489033],\n",
       "        [-0.10167884],\n",
       "        [-0.16866556],\n",
       "        [-0.03413381],\n",
       "        [ 0.06089568],\n",
       "        [ 0.07223201],\n",
       "        [-0.08770125],\n",
       "        [-0.02871943],\n",
       "        [ 0.16125599],\n",
       "        [ 0.02691461],\n",
       "        [ 0.01948337],\n",
       "        [ 0.07910967],\n",
       "        [ 0.11317968],\n",
       "        [ 0.10890415],\n",
       "        [ 0.04431926],\n",
       "        [-0.09853397],\n",
       "        [ 0.02308092],\n",
       "        [ 0.08287379],\n",
       "        [-0.09402294],\n",
       "        [-0.14130248],\n",
       "        [-0.05110238],\n",
       "        [-0.06859751],\n",
       "        [-0.16024949],\n",
       "        [ 0.10015973],\n",
       "        [ 0.15947652],\n",
       "        [-0.07271832],\n",
       "        [-0.01095423],\n",
       "        [ 0.10227972],\n",
       "        [ 0.07191692],\n",
       "        [-0.10597126],\n",
       "        [ 0.01554982],\n",
       "        [ 0.02383754],\n",
       "        [ 0.07573369],\n",
       "        [-0.15044186],\n",
       "        [ 0.04043888],\n",
       "        [ 0.14173561],\n",
       "        [-0.13845712],\n",
       "        [ 0.01851296],\n",
       "        [-0.14492528],\n",
       "        [ 0.13975349],\n",
       "        [ 0.12292075],\n",
       "        [-0.12064513],\n",
       "        [ 0.01116332],\n",
       "        [ 0.14738241],\n",
       "        [ 0.14315128],\n",
       "        [-0.15052478],\n",
       "        [ 0.04069658],\n",
       "        [-0.17163566],\n",
       "        [-0.16611843],\n",
       "        [-0.01417239],\n",
       "        [-0.00523289],\n",
       "        [-0.17259273],\n",
       "        [-0.05683196],\n",
       "        [-0.12212888],\n",
       "        [ 0.12823763],\n",
       "        [ 0.06010956],\n",
       "        [-0.15017703],\n",
       "        [ 0.16007316],\n",
       "        [-0.04471429],\n",
       "        [-0.04730617],\n",
       "        [ 0.08603567],\n",
       "        [ 0.00031525],\n",
       "        [ 0.0117062 ],\n",
       "        [-0.13504544],\n",
       "        [ 0.14874402],\n",
       "        [ 0.12190393],\n",
       "        [-0.09625038],\n",
       "        [-0.13612847],\n",
       "        [ 0.10375959],\n",
       "        [ 0.14308038],\n",
       "        [-0.0459978 ],\n",
       "        [ 0.10531092],\n",
       "        [ 0.04683949],\n",
       "        [-0.02559283],\n",
       "        [-0.1659308 ],\n",
       "        [ 0.16795325],\n",
       "        [-0.16970806],\n",
       "        [ 0.00206354],\n",
       "        [ 0.17277232],\n",
       "        [-0.01687573],\n",
       "        [-0.02819723],\n",
       "        [ 0.16687885],\n",
       "        [ 0.13915977],\n",
       "        [-0.13824114],\n",
       "        [-0.09051169],\n",
       "        [ 0.02253413],\n",
       "        [ 0.10571837],\n",
       "        [-0.12233007],\n",
       "        [ 0.09209406],\n",
       "        [-0.06500953],\n",
       "        [ 0.16664597],\n",
       "        [ 0.0718455 ],\n",
       "        [ 0.14673227],\n",
       "        [-0.15702738],\n",
       "        [ 0.02319209],\n",
       "        [ 0.16016135],\n",
       "        [ 0.08587146],\n",
       "        [-0.01325458],\n",
       "        [-0.02561945],\n",
       "        [ 0.10608596],\n",
       "        [ 0.16338152],\n",
       "        [-0.15873148],\n",
       "        [-0.05112289],\n",
       "        [ 0.09608218],\n",
       "        [ 0.06990705],\n",
       "        [-0.04616167],\n",
       "        [ 0.11277947],\n",
       "        [-0.01795764],\n",
       "        [-0.11008631],\n",
       "        [-0.03539108],\n",
       "        [ 0.01123486],\n",
       "        [ 0.08656475],\n",
       "        [-0.12868087],\n",
       "        [ 0.06068197],\n",
       "        [-0.1240363 ],\n",
       "        [-0.06324274],\n",
       "        [-0.087043  ],\n",
       "        [-0.11401796],\n",
       "        [-0.12566818],\n",
       "        [ 0.0652075 ],\n",
       "        [-0.10046976],\n",
       "        [ 0.01762263],\n",
       "        [-0.14366755],\n",
       "        [ 0.05254823],\n",
       "        [-0.08198313],\n",
       "        [-0.10119199],\n",
       "        [-0.01554483],\n",
       "        [ 0.13884342],\n",
       "        [ 0.06425154],\n",
       "        [-0.01707193],\n",
       "        [ 0.00087905],\n",
       "        [-0.08329527],\n",
       "        [ 0.15515187],\n",
       "        [ 0.06579873],\n",
       "        [-0.157625  ],\n",
       "        [ 0.10305884],\n",
       "        [-0.17183952],\n",
       "        [ 0.00944209],\n",
       "        [-0.0372915 ],\n",
       "        [ 0.00270264],\n",
       "        [-0.15215378],\n",
       "        [ 0.1369259 ],\n",
       "        [-0.05096928],\n",
       "        [-0.1191213 ],\n",
       "        [ 0.16403693],\n",
       "        [-0.16282229],\n",
       "        [ 0.13674286],\n",
       "        [-0.06442492],\n",
       "        [-0.14039382],\n",
       "        [-0.03347971],\n",
       "        [-0.09243542],\n",
       "        [-0.0621452 ],\n",
       "        [-0.00563298],\n",
       "        [-0.10287487],\n",
       "        [-0.00037177],\n",
       "        [ 0.07279354],\n",
       "        [ 0.12516415],\n",
       "        [ 0.00729272],\n",
       "        [ 0.06416145],\n",
       "        [-0.050979  ],\n",
       "        [ 0.04842412],\n",
       "        [ 0.03500754],\n",
       "        [ 0.02751875],\n",
       "        [ 0.05520184],\n",
       "        [ 0.08284253],\n",
       "        [-0.10606753],\n",
       "        [-0.00301401],\n",
       "        [-0.14062487],\n",
       "        [-0.0909031 ],\n",
       "        [ 0.11002371],\n",
       "        [ 0.04957409],\n",
       "        [ 0.10948116],\n",
       "        [-0.06578732],\n",
       "        [ 0.04169714],\n",
       "        [-0.10492028]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ww0 = initWeight(38)\n",
    "ww0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "59637a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "a = tf.constant([1,2,3,4,5] , dtype = tf.int32)\n",
    "b = tf.constant([53.44569] , dtype= tf.float32)\n",
    "c = tf.constant(0)\n",
    "# if u wish to write all these tensors on each line ,\n",
    "# then create a single string out of these.\n",
    "one_string = tf.strings.format(\"{}\\n{}\\n{}\\n\", (a,b,c))\n",
    "# {} is a placeholder for each element ina string and thus you would need n PH for n tensors.\n",
    "# send this string to write_file fn\n",
    "tf.io.write_file(filename, one_string)\n",
    "'''\n",
    "#using tf.saved_model\n",
    "grad_dir = \"C://Users//USER//projects//Twitter-Bot-or-Not//grad_sum//\"\n",
    "fnames = ['g1', 'g2', 'g3', 'g4', 'g5', 'g6']\n",
    "\n",
    "def save_grad(path, fnames, grad):\n",
    "    if fnames is None or grad is None:\n",
    "        return\n",
    "    lists = [gd.numpy() for gd in grad]\n",
    "    \n",
    "    n = len(fnames)\n",
    "    for idx in range(n):\n",
    "        np.save(path+fnames[idx], lists[idx])\n",
    "    \n",
    "    \n",
    "    print(\"save successful\")\n",
    "def load_grad(path, fnames):\n",
    "    if fnames is None:\n",
    "        return []\n",
    "    grad = []\n",
    "    for fname in fnames:\n",
    "        arr = np.load(path+fname+'.npy')\n",
    "        tens = tf.constant(arr)\n",
    "        grad.append(tens)\n",
    "    \n",
    "    print(\"load successful\")\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da68de18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save successful\n"
     ]
    }
   ],
   "source": [
    "save_grad(grad_dir, fnames, grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f3f0076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load successful\n"
     ]
    }
   ],
   "source": [
    "grad_load = load_grad(grad_dir, fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "988d27a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(38, 500), dtype=float32, numpy=\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(500,), dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.], dtype=float32)>,\n",
       " <tf.Tensor: shape=(500, 200), dtype=float32, numpy=\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(200,), dtype=float32, numpy=\n",
       " array([-0.,  0.,  0.,  0.,  0.,  0., -0., -0., -0., -0., -0.,  0., -0.,\n",
       "         0., -0.,  0., -0.,  0., -0.,  0.,  0., -0.,  0.,  0.,  0.,  0.,\n",
       "        -0., -0.,  0.,  0.,  0., -0.,  0.,  0., -0., -0.,  0.,  0.,  0.,\n",
       "        -0.,  0.,  0.,  0.,  0.,  0., -0., -0., -0.,  0.,  0., -0., -0.,\n",
       "         0.,  0., -0., -0.,  0.,  0.,  0., -0., -0.,  0., -0.,  0.,  0.,\n",
       "        -0., -0., -0.,  0., -0.,  0.,  0., -0., -0., -0., -0.,  0., -0.,\n",
       "         0.,  0., -0., -0., -0., -0., -0., -0., -0.,  0., -0., -0.,  0.,\n",
       "        -0., -0., -0., -0., -0., -0.,  0.,  0., -0., -0.,  0.,  0.,  0.,\n",
       "         0., -0., -0.,  0., -0., -0.,  0., -0., -0., -0., -0.,  0., -0.,\n",
       "         0.,  0.,  0.,  0., -0.,  0.,  0., -0.,  0.,  0., -0., -0., -0.,\n",
       "        -0.,  0.,  0.,  0., -0., -0., -0.,  0.,  0.,  0., -0., -0.,  0.,\n",
       "         0.,  0., -0.,  0., -0., -0., -0., -0., -0.,  0.,  0., -0., -0.,\n",
       "         0.,  0.,  0.,  0., -0., -0.,  0., -0.,  0.,  0., -0., -0., -0.,\n",
       "         0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.,  0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0.,  0., -0.,  0., -0.,  0., -0.,\n",
       "         0., -0., -0., -0.,  0.], dtype=float32)>,\n",
       " <tf.Tensor: shape=(200, 1), dtype=float32, numpy=\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.25], dtype=float32)>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54a6385b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#not working\n",
    "# model_dir = \"C://Users//USER//projects//Twitter-Bot-or-Not//grad_sum//\"\n",
    "# fnames = ['v1', 'v2', 'v3', 'v4', 'v5', 'v6']\n",
    "\n",
    "# def save_gd(dir_path, fnames, gds):\n",
    "#     n = len(fnames)\n",
    "#     for idx in range(n):\n",
    "#         tf.saved_model.save(gds[idx], dir_path + fnames[idx])\n",
    "                     \n",
    "# save_gd(model_dir, fnames, grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2ae6409c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'[[0 0 0 ... 0 0 0]\\n [0 0 0 ... 0 0 0]\\n [0 0 0 ... 0 0 0]\\n ...\\n [0 0 0 ... 0 0 0]\\n [0 0 0 ... 0 0 0]\\n [0 0 0 ... 0 0 0]]'>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_str = tf.strings.format(\"{}\", grad[0])\n",
    "grad_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6bc0b1cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'[[[0 0 0 ... 0 0 0]\\n [0 0 0 ... 0 0 0]\\n [0 0 0 ... 0 0 0]\\n ...\\n [0 0 0 ... 0 0 0]\\n [0 0 0 ... 0 0 0]\\n [0 0 0 ... 0 0 0]],[0 0 0 ... 0 0 0],[[0 0 0 ... 0 0 0]\\n [0 0 0 ... 0 0 0]\\n [0 0 0 ... 0 0 0]\\n ...\\n [0 0 0 ... 0 0 0]\\n [0 0 0 ... 0 0 0]\\n [0 0 0 ... 0 0 0]],[0 0 0 ... 0 -0 -0],[[0]\\n [0]\\n [0]\\n ...\\n [0]\\n [0]\\n [0]],[0.25]]'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = grad_str.numpy()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0b260132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(38, 500), dtype=string, numpy=\n",
       "array([[b'0.000000', b'0.000000', b'0.000000', ..., b'0.000000',\n",
       "        b'0.000000', b'0.000000'],\n",
       "       [b'0.000000', b'0.000000', b'0.000000', ..., b'0.000000',\n",
       "        b'0.000000', b'0.000000'],\n",
       "       [b'0.000000', b'0.000000', b'0.000000', ..., b'0.000000',\n",
       "        b'0.000000', b'0.000000'],\n",
       "       ...,\n",
       "       [b'0.000000', b'0.000000', b'0.000000', ..., b'0.000000',\n",
       "        b'0.000000', b'0.000000'],\n",
       "       [b'0.000000', b'0.000000', b'0.000000', ..., b'0.000000',\n",
       "        b'0.000000', b'0.000000'],\n",
       "       [b'0.000000', b'0.000000', b'0.000000', ..., b'0.000000',\n",
       "        b'0.000000', b'0.000000']], dtype=object)>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = tf.strings.as_string(grad[0])\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bd476ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnp = g.numpy()\n",
    "gnp\n",
    "ff = gnp.astype('float32')\n",
    "ff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "79deab82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(38, 500), dtype=float32, numpy=\n",
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "35514385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'[[0 0 0 ... 0 0 0]\\n [0 0 0 ... 0 0 0]\\n [0 0 0 ... 0 0 0]\\n ...\\n [0 0 0 ... 0 0 0]\\n [0 0 0 ... 0 0 0]\\n [0 0 0 ... 0 0 0]]\\n[0 0 0 ... 0 0 0]\\n[[0 0 0 ... 0 0 0]\\n [0 0 0 ... 0 0 0]\\n [0 0 0 ... 0 0 0]\\n ...\\n [0 0 0 ... 0 0 0]\\n [0 0 0 ... 0 0 0]\\n [0 0 0 ... 0 0 0]]\\n[0 0 0 ... 0 -0 -0]\\n[[0]\\n [0]\\n [0]\\n ...\\n [0]\\n [0]\\n [0]]\\n[0.25]\\n'>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_s = tf.convert_to_tensor(grad_str)\n",
    "g_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8b88314f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " array([ 0.,  0.,  0., -0., -0., -0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n",
       "        -0., -0.,  0., -0., -0.,  0., -0., -0.,  0., -0., -0., -0.,  0.,\n",
       "        -0.,  0., -0.,  0., -0.,  0., -0.,  0.,  0., -0.,  0., -0., -0.,\n",
       "         0.,  0.,  0., -0.,  0., -0., -0.,  0.,  0., -0.,  0., -0., -0.,\n",
       "        -0., -0., -0.,  0.,  0.,  0., -0.,  0., -0., -0., -0.,  0.,  0.,\n",
       "         0., -0., -0., -0., -0., -0.,  0., -0., -0.,  0.,  0., -0.,  0.,\n",
       "         0.,  0.,  0., -0., -0.,  0., -0.,  0.,  0., -0.,  0., -0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0.,  0., -0., -0.,  0., -0., -0.,\n",
       "         0., -0.,  0., -0.,  0., -0.,  0.,  0.,  0., -0., -0.,  0., -0.,\n",
       "         0.,  0., -0.,  0., -0., -0.,  0., -0., -0., -0.,  0., -0., -0.,\n",
       "        -0., -0.,  0.,  0., -0., -0.,  0.,  0., -0.,  0., -0.,  0., -0.,\n",
       "         0., -0.,  0., -0., -0.,  0.,  0.,  0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0.,  0., -0., -0.,  0., -0.,  0.,  0., -0.,  0.,  0.,\n",
       "        -0.,  0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.,\n",
       "        -0., -0., -0., -0.,  0.,  0., -0.,  0.,  0., -0., -0.,  0., -0.,\n",
       "         0.,  0.,  0., -0., -0.], dtype=float32),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]], dtype=float32),\n",
       " array([0.25], dtype=float32)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd_list = [gd.numpy() for gd in grad]\n",
    "gd_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "61b0411e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(38, 500), dtype=float32, numpy=\n",
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(gd_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e4a83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9e7a83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3b9bb55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3561e6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.empty([], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b576719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(6.e-45, dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1de02825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.isfile(os.getcwd()+\"\\\\data\\\\\"+\"grad_sum.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4bb5555c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grads.npy'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76156236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\USER\\\\projects\\\\Twitter-Bot-or-Not\\\\data\\\\grads.npy'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()+\"\\\\data\\\\\"+grad_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b0ec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(grad_fname):\n",
    "        grads = np.load(grad_fname) # load gds from s3/hdfs/file\n",
    "        grads = tf.convert_to_tensor(grads, dtype='float32')\n",
    "else:\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
